{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n",
        "\n",
        "Sections:\n",
        "- Google Translate Models\n",
        "- M2M-100 Models\n",
        "- ChatGPT Models\n",
        "- ChatGPT Reworded Models\n",
        "- mBART Models"
      ],
      "metadata": {
        "id": "G1kVEOutUAJ6"
      },
      "id": "G1kVEOutUAJ6"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "e46626f4-0974-41e0-8707-89498788d1e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca712f71-912b-496e-8173-7f240863d0e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install transformers --quiet\n",
        "!pip install datasets --quiet\n",
        "!pip install sentencepiece --quiet\n",
        "!pip install torch --quiet"
      ],
      "id": "e46626f4-0974-41e0-8707-89498788d1e5"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kn3gArHqRX6x",
        "outputId": "b07ceac1-9fd2-424d-f3e3-2e1f05b8a1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Kn3gArHqRX6x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "0LjnvJI2zzhi"
      },
      "id": "0LjnvJI2zzhi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4yT15PuZWW3",
        "outputId": "860b719a-e24f-41bf-fbd1-574a6ed03e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Mar 26 06:26:01 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "id": "v4yT15PuZWW3"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "\n",
        "print(\"Before moving to GPU:\", tensor)\n",
        "\n",
        "tensor = tensor.to(device)\n",
        "\n",
        "print(\"After moving to GPU:\", tensor)"
      ],
      "metadata": {
        "id": "AIyZzb83NAx_",
        "outputId": "b6601472-0b13-4674-b616-e0f0d869a73d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "AIyZzb83NAx_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before moving to GPU: tensor([1, 2, 3])\n",
            "After moving to GPU: tensor([1, 2, 3], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall torch\n",
        "!pip install torch torchvision -f https://download.pytorch.org/whl/cu118/torch_stable.html\n",
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "m0gwYghtSX4e"
      },
      "id": "m0gwYghtSX4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2fa2ea5-8786-4290-a068-87adf228c3e6"
      },
      "source": [
        "## Imports"
      ],
      "id": "d2fa2ea5-8786-4290-a068-87adf228c3e6"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "d5a85591-1716-436c-9e38-3494d461832c"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import MBartForConditionalGeneration, MBartTokenizer, MBart50TokenizerFast\n",
        "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import copy"
      ],
      "id": "d5a85591-1716-436c-9e38-3494d461832c"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8a69aec6-4af8-4cf8-86b3-f72f2a75098f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# swahili to english tokenizer\n",
        "train_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
      ],
      "id": "8a69aec6-4af8-4cf8-86b3-f72f2a75098f"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", src_lang=\"sw_KE\", tgt_lang=\"en_XX\")"
      ],
      "metadata": {
        "id": "wFSxWF1UWavO"
      },
      "id": "wFSxWF1UWavO",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "sw_test_flores = load_dataset(\"facebook/flores\", 'swh_Latn')\n",
        "en_test_flores = load_dataset(\"facebook/flores\", 'eng_Latn')\n",
        "\n",
        "src_eval_texts = sw_test_flores['dev']['sentence'] + sw_test_flores['devtest']['sentence']\n",
        "tgt_eval_texts = en_test_flores['dev']['sentence'] + en_test_flores['devtest']['sentence']\n",
        "\n",
        "src_lang, tgt_lang = 'sw_KE', 'en_XX'\n",
        "\n",
        "val_src_texts, val_tgt_texts = src_eval_texts[:256], tgt_eval_texts[:256]"
      ],
      "metadata": {
        "id": "ksmk7cZpOM9E"
      },
      "id": "ksmk7cZpOM9E",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "3pefbjInKMyu"
      },
      "id": "3pefbjInKMyu"
    },
    {
      "cell_type": "code",
      "source": [
        "! nvcc --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFNvGgWODWqW",
        "outputId": "b66fa59c-6e8c-4d50-c3f4-52d64c376945"
      },
      "id": "AFNvGgWODWqW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2022 NVIDIA Corporation\n",
            "Built on Wed_Sep_21_10:33:58_PDT_2022\n",
            "Cuda compilation tools, release 11.8, V11.8.89\n",
            "Build cuda_11.8.r11.8/compiler.31833905_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DThg6RKhEMfT",
        "outputId": "07ebd445-84dc-4240-adfc-ac8ee35f6ac6"
      },
      "id": "DThg6RKhEMfT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device('cuda')\n",
        "\n",
        "tensor = torch.tensor([1, 2, 3])\n",
        "\n",
        "print(\"Before moving to GPU:\", tensor)\n",
        "\n",
        "tensor = tensor.to(device)\n",
        "\n",
        "print(\"After moving to GPU:\", tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXk7OAUeDob3",
        "outputId": "2633b8a8-84b2-423c-e6ee-a45ce15d7482"
      },
      "id": "kXk7OAUeDob3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before moving to GPU: tensor([1, 2, 3])\n",
            "After moving to GPU: tensor([1, 2, 3], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
      ],
      "metadata": {
        "id": "BmJJmIj6WKVn"
      },
      "id": "BmJJmIj6WKVn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cpu')\n",
        "\n",
        "swahili_phrase = \"MKUTANO WA BIASHARA Je Ungependa Kupata Mualiko Maalum Kuhudhuria Kwenye Mafunzo Makubwa Ya Ujasiriamali Kama Haya Bureee Yatakayo Kukutanisha Na Wafanyabiashara Na Wamiliki Wa Biashara Kubwa Na Wadogo\"\n",
        "english_phrase = \"BUSINESS MEETING Would you like to get a special invitation to attend large entrepreneurship trainings like this one that will bring you together with businessmen and owners of large and small businesses\"\n",
        "\n",
        "inputs = tokenizer(swahili_phrase, text_target=english_phrase, return_tensors=\"pt\")\n",
        "# print(inputs)\n",
        "# print('----')\n",
        "# print(inputs.to(device))\n",
        "# inputs = {key: tensor.to(device) for key, tensor in inputs.items()}\n",
        "# print('-----')\n",
        "# print(inputs)\n",
        "# train_model = train_model.to(device)\n",
        "outputs = train_model(**inputs)\n",
        "loss = outputs.loss\n",
        "print(outputs, loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS9gZIzV-01D",
        "outputId": "c15391e0-51d1-44f7-f820-a146cca3ac3a"
      },
      "id": "hS9gZIzV-01D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seq2SeqLMOutput(loss=tensor(3.9605, grad_fn=<NllLossBackward0>), logits=tensor([[[ 1.8658e-01,  1.5054e-01,  4.3912e+00,  ...,  2.5980e-01,\n",
            "           6.7222e-01,  1.6126e-01],\n",
            "         [-4.4747e+00, -4.6242e+00, -3.9612e+00,  ..., -1.2108e+01,\n",
            "          -1.1325e+01, -4.6192e+00],\n",
            "         [-1.1511e+01, -1.1941e+01, -1.2384e+01,  ..., -1.9471e+01,\n",
            "          -1.6065e+01, -1.1571e+01],\n",
            "         ...,\n",
            "         [-1.3091e+00, -1.4542e+00,  1.8708e+00,  ..., -5.4042e+00,\n",
            "          -3.6051e+00, -1.4427e+00],\n",
            "         [-6.0748e-01, -7.7335e-01,  2.0534e+00,  ..., -9.0371e-01,\n",
            "          -2.8325e+00, -4.6288e-01],\n",
            "         [-8.8003e-02,  1.4763e-02,  6.0667e+00,  ..., -2.5823e+00,\n",
            "          -2.8287e+00,  2.8594e-02]]], grad_fn=<AddBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[ 0.5459,  0.6820,  0.6011,  ..., -0.6271,  0.4598, -0.7743],\n",
            "         [ 0.7910,  0.8533,  0.1472,  ..., -0.0631, -0.5529, -1.0668],\n",
            "         [ 0.0616,  0.7862,  0.3144,  ..., -0.1773,  0.0810, -0.7672],\n",
            "         ...,\n",
            "         [ 0.2954,  0.4984, -0.0798,  ..., -0.3887,  0.1484, -0.4589],\n",
            "         [ 0.5090,  0.7120, -0.0444,  ..., -0.1503, -0.4748, -0.1308],\n",
            "         [ 0.0181, -0.0059,  0.0167,  ..., -0.0192,  0.0049, -0.0058]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), encoder_hidden_states=None, encoder_attentions=None) tensor(3.9605, grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsVG_GDA_4y3",
        "outputId": "dc6cd342-f3ef-4650-dcdd-8ec966b325b4"
      },
      "id": "FsVG_GDA_4y3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[250043,  52262,  70201,   8575,   4336,    335,  83081, 136499,    845,\n",
              "            345, 213001, 104472,    102,  47218,  41578,  71467,    316,   1461,\n",
              "         139518,   1651, 176614, 220491, 149722,    275,    634,   2369,    345,\n",
              "         236375,  34041,   9627,     11,    667,    107,   7039,   2369,   6758,\n",
              "           1410,   1461, 113437,   7230,    353,   3316,  18357,   9166,      7,\n",
              "          11416,    353,   3316,  97961,   3316, 177728,  96305,    634,    353,\n",
              "           3316, 105934,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[250004,  22013,  88868,  64397,    276,  24821,  68151, 154559,    398,\n",
              "           1884,     47,   2046,     10,   5361, 194134,     47,  29966,  21334,\n",
              "              6,  98659,  16070,  23189,      7,   1884,    903,   1632,    450,\n",
              "           1221,  19095,    398,  25842,    678,   8063,   1055,    136, 144044,\n",
              "            111,  21334,    136,  19336, 108880,      2]])}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BxgA5SYQ_M8g",
        "outputId": "b3b1a80a-f550-446f-85c7-c425ebebdd8b"
      },
      "id": "BxgA5SYQ_M8g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.1751, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f018ed6d-81ea-403e-bd8a-8f9e776d79b7"
      },
      "source": [
        "## Model Training"
      ],
      "id": "f018ed6d-81ea-403e-bd8a-8f9e776d79b7"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Z0sxiS1-myWM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, src_texts, tgt_texts, tokenizer):\n",
        "        self.src_texts = src_texts\n",
        "        self.tgt_texts = tgt_texts\n",
        "        self.tokenizer = tokenizer\n",
        "        # self.device = device\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text = self.src_texts[idx] \n",
        "        tgt_text = self.tgt_texts[idx]\n",
        "\n",
        "        return src_text, tgt_text\n",
        "\n",
        "def train(src_texts, tgt_texts, val_src_texts, val_tgt_texts, to_train_model, tokenizer, batch_size=64, epochs=4, learning_rate=5e-4, device='cuda'):\n",
        "    model_copy = copy.deepcopy(to_train_model)\n",
        "    if torch.cuda.is_available() and device == 'cuda':\n",
        "        device = torch.device('cuda')\n",
        "        model_copy.to(device)\n",
        "        print('on gpu')\n",
        "    else:\n",
        "        print('on cpu')\n",
        "        device = torch.device('cpu')\n",
        "\n",
        "    optimizer = Adam(model_copy.parameters(), lr=learning_rate)\n",
        "\n",
        "    train_dataset = CustomDataset(src_texts, tgt_texts, tokenizer)\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    val_dataset = CustomDataset(val_src_texts, val_tgt_texts, tokenizer)\n",
        "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    def compute_validation_loss(model, dataloader, device):\n",
        "      model.eval()\n",
        "      total_loss = 0\n",
        "      with torch.no_grad():\n",
        "          for src_text, tgt_text in dataloader:\n",
        "              model_inputs = tokenizer(src_text, text_target=tgt_text, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
        "              model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "              outputs = model(**model_inputs)\n",
        "              loss = outputs.loss\n",
        "              total_loss += loss.item()\n",
        "      return total_loss / len(dataloader) # avg loss per epoch\n",
        "\n",
        "    batch_losses = []\n",
        "    train_epoch_losses, val_epoch_losses = [], []\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        model_copy.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for i, (src_text, tgt_text) in enumerate(train_dataloader):\n",
        "\n",
        "            model_inputs = tokenizer(src_text, text_target=tgt_text, padding=\"max_length\", max_length=32, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "            model_inputs = {k: v.to(device) for k, v in model_inputs.items()}\n",
        "\n",
        "            optimizer.zero_grad()  # reset gradients\n",
        "            print('here')\n",
        "            outputs = model_copy(**model_inputs)\n",
        "            print('after forward pass')\n",
        "            loss = outputs.loss\n",
        "            loss.backward()  # backward pass\n",
        "            print('after backward pass')\n",
        "            optimizer.step()  # update model parameters\n",
        "            print('after step update')\n",
        "            batch_losses.append(loss.item())\n",
        "            print('loss: ', loss.item())\n",
        "            total_loss += loss.item()\n",
        "            print(f\"  Batch {i+1}, Loss: {total_loss/(i+1):.4f}\")\n",
        "\n",
        "        print('length of train_dataloader: ', len(train_dataloader))\n",
        "        train_loss = total_loss/len(train_dataloader)\n",
        "        train_epoch_losses.append(train_loss)\n",
        "        print(f\"Epoch {epoch+1} Loss: {train_loss:.4f}\")\n",
        "\n",
        "        val_loss = compute_validation_loss(model_copy, val_dataloader, device)\n",
        "        val_epoch_losses.append(val_loss)\n",
        "        print(f\"Validation Loss after Epoch {epoch+1}: {val_loss:.4f}\")\n",
        "      \n",
        "    return model_copy, batch_losses, train_epoch_losses, val_epoch_losses"
      ],
      "id": "Z0sxiS1-myWM"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d7e9d07c-5826-4bbd-bdcd-49a031f7ee99"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data):\n",
        "    if 'gpt' in data.columns[-1]:\n",
        "      data.columns = ['index', 'swahili', 'english', 'swahili_2']\n",
        "    \n",
        "    if 'reworded' in data.columns[-1]:\n",
        "      data = data[['swahili', 'reworded_english', 'reworded_swahili']]\n",
        "      data.columns = ['swahili', 'english', 'swahili_2']\n",
        "\n",
        "    to_concat = data[['english', 'swahili_2']]\n",
        "    to_concat.columns = ['english', 'swahili']\n",
        "    base = data[['english', 'swahili']]\n",
        "    \n",
        "    concated = pd.concat([base, to_concat])\n",
        "    return concated"
      ],
      "id": "d7e9d07c-5826-4bbd-bdcd-49a031f7ee99"
    },
    {
      "cell_type": "code",
      "source": [
        "def train_pipeline(data_path, \n",
        "             to_train_model, tokenizer, model_type):\n",
        "    \n",
        "    datasize = data_path.split('_')[-2]\n",
        "    print(datasize)\n",
        "    data = pd.read_csv(data_path)\n",
        "    data = preprocess_data(data)\n",
        "    print('data loaded')\n",
        "    \n",
        "    src_texts = data['swahili'].values\n",
        "    tgt_texts = data['english'].values\n",
        "    \n",
        "    trained_model, batch_losses, train_epoch_losses, val_epoch_losses = train(src_texts, tgt_texts, val_src_texts, val_tgt_texts, to_train_model, tokenizer)\n",
        "    print('model finished training')\n",
        "\n",
        "    trained_model.save_pretrained(f'/content/drive/MyDrive/W266 Final Project/{model_type}_mbart_sw_en_{datasize}_model')\n",
        "    print('model saved')\n",
        "\n",
        "    loss_df = pd.DataFrame([batch_losses]).T\n",
        "    loss_df.to_csv(f'/content/drive/MyDrive/W266 Final Project/{model_type}_{datasize}_batch_losses.csv')\n",
        "\n",
        "    train_val_loss_df = pd.DataFrame([train_epoch_losses, val_epoch_losses]).T\n",
        "    train_val_loss_df.to_csv(f'/content/drive/MyDrive/W266 Final Project/{model_type}_{datasize}_train_val_losses.csv')\n",
        "    print('lossed saved')\n",
        "        \n",
        "    return trained_model, batch_losses, train_epoch_losses, val_epoch_losses"
      ],
      "metadata": {
        "id": "_GYqZg-byhq2"
      },
      "id": "_GYqZg-byhq2",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Google Translate Models"
      ],
      "metadata": {
        "id": "fEVRe2Nom2eF"
      },
      "id": "fEVRe2Nom2eF"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10K Model"
      ],
      "metadata": {
        "id": "x4BurouayiHy"
      },
      "id": "x4BurouayiHy"
    },
    {
      "cell_type": "code",
      "source": [
        "data_path_10k = '/content/drive/MyDrive/W266 Final Project/google_translated_data/gt_10k_dataset.csv'\n",
        "\n",
        "trained_model_10k, losses_10k = train_pipeline(data_path_10k, train_model, tokenizer)"
      ],
      "metadata": {
        "id": "ftqCvjMmyhbW"
      },
      "id": "ftqCvjMmyhbW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(losses_10k).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "HXlkrtuClO-9",
        "outputId": "84ddd7ff-528c-4a83-9785-a88748202cd3"
      },
      "id": "HXlkrtuClO-9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiEAAAGdCAYAAADE96MUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABXx0lEQVR4nO3dd3hUVf4/8PednjLpvYfea+hYEBRdRER3FUVF3bViQfdnYXdR113Fsuuirgu2tawF9SvYFelFqQk9EAKE9N4mfdr9/XFnbjIpkMBMZjLzfj1Pnm+Yubn3nDz7zbw953POEURRFEFERETUyxTubgARERH5JoYQIiIicguGECIiInILhhAiIiJyC4YQIiIicguGECIiInILhhAiIiJyC4YQIiIicguVuxvQntVqRVFREfR6PQRBcHdziIiIqBtEUURdXR3i4uKgUHRvjMPjQkhRURESExPd3QwiIiI6D/n5+UhISOjWtR4XQvR6PQCpE0FBQW5uDREREXWHwWBAYmKi/DneHR4XQuxTMEFBQQwhREREfUxPSilYmEpERERuwRBCREREbsEQQkRERG7hcTUhREREfZXFYoHJZHJ3M1xGrVZDqVQ67X4MIURERE5QX1+PgoICiKLo7qa4jCAISEhIQGBgoFPuxxBCRER0gSwWCwoKCuDv74/IyEiv3GxTFEWUl5ejoKAAAwcOdMqICEMIERHRBTKZTBBFEZGRkfDz83N3c1wmMjISZ86cgclkckoIYWEqERGRk3jjCEhbzu4fQwgRERG5BUMIERERuQVDCBEREbkFQwgREZGPe+ONN5CSkgKdTodJkyZhz549vfJchpA2ahtNWLX1FIpqmtzdFCIiol7x2Wef4dFHH8XTTz+NjIwMjB49GrNnz0ZZWZnLn80lum18kZ6PF348jpLaZjxzzXB3N4eIiPooURTRZLK45dl+amWPVrG88soruOuuu3DHHXcAAFatWoXvv/8e//3vf/Hkk0+6qpkAGEIcVDcaAQC1Td675S4REblek8mCYU+tc8uzM5+dDX9N9z7ejUYj0tPTsXTpUvk1hUKBWbNmYefOna5qYuuzXP6EPqTJaAUANLspvRIREfWmiooKWCwWREdHO7weHR2NkpISlz+fIyFt2IfOWsxWN7eEiIj6Mj+1EpnPznbbs/sKhpA2WmwhhCMhRER0IQRB6PaUiDtFRERAqVSitLTU4fXS0lLExMS4/PmcjmmDIyFERORLNBoNxo8fj40bN8qvWa1WbNy4EVOmTHH58z0/pvWi1hDCkRAiIvINjz76KBYtWoS0tDRMnDgRK1asQENDg7xaxpUYQtpoMtqnYzgSQkREvuHGG29EeXk5nnrqKZSUlGDMmDH46aefOhSrugJDSBvNHAkhIiIf9MADD+CBBx7o9eeyJqQN+whIC0dCiIiIXI4hpI0mro4hIiLqNQwhbXB1DBERUe9hCGmj2dgaQkRRdHNriIiIvBtDSBvNbQpSORpCREQ95e3/Aevs/jGE2JgsVpgsrb9chhAiIuoupVLaKt1oNLq5Ja5l75+9vxeKS3Rt2hejSst01e5pDBER9SkqlQr+/v4oLy+HWq2GQuF9/41vtVpRXl4Of39/qFTOiQ8MITZN7UMIl+kSEVE3CYKA2NhY5OTkIDc3193NcRmFQoGkpCQIguCU+zGE2DQbHUMHNywjIqKe0Gg0GDhwoFdPyWg0GqeO8jCE2DS3Cx3cup2IiHpKoVBAp9O5uxl9hvdNWp0n+7kxdhwJISIici2GEBvWhBAREfUuhhCbDiGES3SJiIhciiHEptnYviaE0zFERESuxBBi074wlSMhRERErsUQYtPEJbpERES9iiHEpn1NCJfoEhERuRZDiE3n27YTERGRqzCE2LTfJ4QjIURERK7FEGLDkRAiIqLexRBiw83KiIiIehdDiI09hOjU0q+k/ZJdIiIici6GEBv7dEyovwYAR0KIiIhcjSHExl6YGuynBsDNyoiIiFyNIcTGvhomxF9t+zenY4iIiFyJIcTGXhMS4mebjuFICBERkUsxhNjYRz7sIyFcoktERORaDCE29pGQYHk6hiMhRERErsQQYtNhdQynY4iIiFyKIcTGvjomxI/TMURERL2BIcSm/eoY7hNCRETkWgwhAMwWK4wWKXQEy6tjOBJCRETkSgwhAJrb1H9wJISIiKh3MITAcWMy+46pPDuGiIjItRhC0FqU6qdWQqdWAgBMFhEWq+jOZhEREXk1hhC0joTo1ApoVa2/EiOX6RIREbkMQwhaNyrzUysdQgjPjyEiInIdhhC0TsfoNEqolAqoFAIAblhGRETkSgwhaF0d42erB7GPhnCZLhERkev0OIRs27YNc+fORVxcHARBwFdffeXwviiKeOqppxAbGws/Pz/MmjUL2dnZzmqvS7QtTAUgF6fy/BgiIiLX6XEIaWhowOjRo/HGG290+v5LL72E1157DatWrcLu3bsREBCA2bNno7m5+YIb6yqthakcCSEiIuotqp7+wFVXXYWrrrqq0/dEUcSKFSvwl7/8BfPmzQMAfPjhh4iOjsZXX32FBQsWXFhrXaSpfQjhSAgREZHLObUmJCcnByUlJZg1a5b8WnBwMCZNmoSdO3d2+jMtLS0wGAwOX71Nno7RcCSEiIiotzg1hJSUlAAAoqOjHV6Pjo6W32tv+fLlCA4Olr8SExOd2aRuse+O6qeWfh32kRBu3U5EROQ6bl8ds3TpUtTW1spf+fn5vd6G5naFqfaREG7dTkRE5DpODSExMTEAgNLSUofXS0tL5ffa02q1CAoKcvjqbe1rQnQcCSEiInI5p4aQ1NRUxMTEYOPGjfJrBoMBu3fvxpQpU5z5KKfqUJgq14QwhBAREblKj1fH1NfX4+TJk/K/c3JycODAAYSFhSEpKQlLlizB3//+dwwcOBCpqalYtmwZ4uLicO211zqz3U5lXwXTvjCV27YTERG5To9DyL59+zBjxgz5348++igAYNGiRXj//ffx+OOPo6GhAXfffTdqamowffp0/PTTT9DpdM5rtZO1PTsGaDMdw5EQIiIil+lxCLn00kshil0fcS8IAp599lk8++yzF9Sw3mQvTNXZV8dwiS4REZHLuX11jCfoWBPCzcqIiIhcjSEEnU3HcCSEiIjI1Xw2hOw7U4Uyg3SeTcfCVNaEEBERuZpPhpDjJQb8dtVO3P9xBoDWVTDtR0K4OoaIiMh1fDKElBpaAAAZedVoNJrls2O4TwgREVHv8ckQYrKFC6sIHC6o7fIUXe6YSkRE5Do+GULM1tZwsT+/prUwVcPCVCIiot7ikyHEaGnd5yQjtxpG28iIX7sluhwJISIich2fDCFmS2u42J1TJX/f/hRdjoQQERG5jk+GEFObEFLbZJK/t4cPe20INysjIiJyHR8NIR23ndeqFFAoBPl7gCMhREREruSjIaTjCIe9KBXgZmVERES9wSdDiNk2EqJWCvJr9noQgJuVERER9QafDCFG20jIqIQQ+bW2IYQjIURERK7nkyHEPhIyOEYPvU4FoHWDMul77phKRETkaj4ZQuw1IRqlAmMSQwAAfurWX4XONhJisYqd1o8QERHRhfPNEGLbMVWtFDDWHkI0HUdCAI6GEBERuYpvhhCzvTBVgatGxiJAo8TU/hHy+xpl66+FxalERESuoXJ3A9zBfnaMSqnA0NggHHpmNpSK1pUyCoUAjUoBo9nKkRAiIiIX8c2RELkmRAoebQOInbxhGUdCiIiIXMInQ4jRNh2jUnbdffsyXW7dTkRE5Bo+GULMcmFq193Xqbl1OxERkSv5ZAixT8e03TG1vdbzYzgSQkRE5Ao+GkJaV8d0pXU6hiMhREREruCjIcS2OqaTglQ7HXdNJSIicimfDCH2bds1qnOPhDCEEBERuYZPhhCjPBJy7sJUTscQERG5hk+GEHO3ClM5EkJERORKPhlCulWYquZmZURERK7koyGkG/uE2EZCmowMIURERK7g0yFEdZbpmKRwfwDA2gOF8vVERETkPD4ZQszWc0/H3DolGeEBGpwub8AHv57ppZYRERH5Dp8MISbzuQtTg3RqPDZ7MADg1Y3ZqKhv6ZW2ERER+QrfDCHdGAkBgN+lJWJEfBDqms34588neqNpREREPsM3Q0g3lugCgFIh4Om5wwEAq/fmYd+ZKpe3jYiIyFf4ZAgxd2OJrt2ElDBcOyYOogjc9eE+nCqvd3XziIiIfIJPhhB5x9RuhBAAeG7+SIxOCEZ1owm3vbsHZYZmVzaPiIjIJ/hkCOnOjqltBWhV+O/tE5AS7o/CmiYsfGc33t52GntyqriPCBER0XlSubsBvc1iFWGrS4X6LGfHtBceqMWHd07CdSt/QXZZPZ774Zj0eoAGPz9yMcIDta5oLhERkdfyuZGQthuPqc9yim5nksL98fUD0/HY7MG4Ylg09DoVKhuM+O5QsbObSURE5PV8OoSoFN2bjmkrPsQPi2cMwFu3pWHJrEEAgK8PFDqtfURERL7C50KIfWUM0L3VMWczd1QsFAKQkVeDvMrGC20aERGRT/G5EGIfCVEI0j4gFyIqSIep/SMAAN8c5GgIERFRT/heCOnmbqnddc2YOADAVweKIIriOa4mIiIiO98LIfK5Mc7p+pUjYqBRKXCyrB7Hiuucck8iIiJf4HMhxGzt2R4h5xKkU2PmkCgALFAlIiLqCZ8LIUazNGXS3d1Su2OebUrmm4NFsFo5JUNERNQdPhdC7CMhGieGkEsHRyFQq0JxbTMOF9Y67b5ERETezOdCiEk+N8Y50zEAoFMrcfEgaZXMxmOlTrsvERGRN/PBEOLc1TF2M4dEAwA2HCtz6n2JiIi8lQ+GENtIyAXuEdLepYMjIQhAZrEBRTVNTr03ERGRN/K5EGLfMVXTw3NjziU8UItxSaEAgE3HORpCRER0Lj4XQowuGgkBgJlDpaW6rAshIiI6N58LIWYX1YQArXUhv5yqRKPR7PT7ExEReROfCyH2mhBXhJBB0YFICPWD0WzFjuwKp9+fiIjIm/hwCHH+dIwgCJg1VBoNYV0IERHR2flgCHH+jqlt2etCvj5QhI925XIHVSIioi74XAhxxY6pbU3pF46LBkagyWTBX746gpve3oX8qkaXPIuIiKgv87kQYjQ7f8fUtlRKBT64YyKemTsMfmoldudU4dHPD7jkWURERH2Z00OIxWLBsmXLkJqaCj8/P/Tv3x9/+9vfIIqeMS1htrpudYydQiHg9mmp+PK+qQCA/Xk1cvghIiIiicrZN3zxxRexcuVKfPDBBxg+fDj27duHO+64A8HBwXjooYec/bgeM5ldV5ja3tBYPfQ6FeqazThVXo+hsUEufyYREVFf4fQQ8uuvv2LevHmYM2cOACAlJQWffvop9uzZ4+xHnRdTL4yE2AmCgCExeuw9U43jJQaGECIiojac/kk8depUbNy4ESdOnAAAHDx4EDt27MBVV13V6fUtLS0wGAwOX67UenZM75TDDImRgsfxkrpeeR4REVFf4fSRkCeffBIGgwFDhgyBUqmExWLBc889h4ULF3Z6/fLly/HXv/7V2c3oktm+T4jK9dMxADA4Rg8AOF7MEEJERNSW04cDPv/8c3z88cf45JNPkJGRgQ8++AD/+Mc/8MEHH3R6/dKlS1FbWyt/5efnO7tJDuz7hKh7aSRkaKwthJS4doSHiIior3H6SMhjjz2GJ598EgsWLAAAjBw5Erm5uVi+fDkWLVrU4XqtVgutVuvsZnTJldu2d2ZQtBRCSg0tqG4wIjRA0yvPJSIi8nRO/yRubGyEot0og1KphNXqGUtU5ZqQXlgdAwB6nRoJoX4AWBdCRETUltNDyNy5c/Hcc8/h+++/x5kzZ7B27Vq88sormD9/vrMfdV7sp+i6asfUzrQWp3JKhoiIyM7p0zGvv/46li1bhvvvvx9lZWWIi4vDPffcg6eeesrZjzovxl4eCQGkupANx0qRxZEQIiIimdNDiF6vx4oVK7BixQpn39op7CMhvVUTArSukDnGEEJERCTzubNjWgtTe28kxD4dc6KkDhaeqktERATAF0NIL+6YapcS7g+tSoEmkwV5PFGXiIgIgC+GEPkU3d7rukqpwMDoQABAFotTiYiIAPhgCDHblgprenE6BmidkjnGnVOJiIgA+GAIMdoKU3vr7Bi7Ibbi1HVHS1DbZOrVZxMREXkinwshrWfH9G7XZw+PgV6rwvGSOtz45k6UGZp79flERESexudCiLw6RtG70zGJYf747J4piNRrcbykDtet/BX5LFIlIiIf5nMhRN4npJdHQgBgWFwQvrx3KpLD/VFQ3YTXN2X3ehuIiIg8hc+FEHnH1F4eCbFLCvfHn34zFABwtIgrZYiIyHf5XAhxx46p7dmLVLPL6rl5GRER+SyfCyGtO6a6r+uJof7wUythNFtxprLBbe0gIiJyJx8OIe6ZjgEAhULAINvmZSd4ngwREfkoHwwh7p+OAYBB0dKUTFYpQwgREfkmnwsh9h1T3R1C7CfrZnEkhIiIfJRPhRBRFOWREJUbp2OANiGEIyFEROSjfCqEmNusRHH7SIhtOuZMRQOaTRa3toWIiMgdfCqE2ItSAfcWpgJApF6LEH81rCJwsqzerW0hIiJyB98KIWbPGQkRBEEeDTnBKRkiIvJBvhVCrK0jIe7aMbUtFqcSEZEv860Q0maPEEHwoBDCkRAiIvJBPhVC7Fu2qxSe0W15OoYjIURE5IM849O4lxg9YLfUtgbaQkhRbTNqm0xubg0REVHv8qkQ4gmH17UV7KdGXLAOAJDNKRkiIvIxnvFp3Es84fC69gaxLoSIiHyU53wa9wJ7CHH3bqlt2c+QyS7lXiFERORbfCyESNMxGg8aCekXEQAAOF3R4OaWEBER9S7P+TTuBWYPHAnpFxkIADhdzpEQIiLyLT4VQoweWBPSL1IaCSmsaeIZMkRE5FM859O4F8j7hHhQCAkP0CBIp4IoAjmckiEiIh/iOZ/GvcBemKrxoOkYQRDaTMkwhBARke/wrRBi9awdU+3sUzJt60L25FThlfUnYLGKXf0YERFRn6ZydwN6k8lsqwlReVYI6W8fCWkzHfPEl4eQU9GAEXFBuGJ4jLuaRkRE5DKe9WnsYmbbKbpqDzhBty15ma5tJKTU0CzXh2SXcdUMERF5J58KIUYP27bdrm1NiCiK2HW6Un6PdSJEROStPOvT2MU8cZ8QAEgO94cgAHUtZpTXt2B3TpX83ukKjoQQEZF38qkQ0ro6xrO6rVMrkRDqB0Aa+djdbiREFFmcSkRE3sezPo1dzOSh0zEA0C9CmpLZm1OFU22mYGqbTKhqMLqrWURERC7jeZ/GLuSJB9jZ2ZfpfrYvHwAwJEaP+BDb6Ag3MSMiIi/kUyHE7MkjIbbi1ILqJgDApNSwTvcPISIi8hae92nsQib57BjPGwnpb1umazepX3ibpbscCSEiIu/jW5uV9YGRELuJqWEor2sBAIcaESIiIm/heZ/GLtRaE+J53Y4O0iJAowQADIgKRESgtnU6hst0iYjIC3nep7EL2XdM9aQD7OwEQUCqLXRMTA0D0Do6klfZKAeotppNFp4tQ0REfZZPhRCj2XaAnQeOhADAJYMiIQjA1aNiAQCxQTro1AqYrSLyqxodrq1uMGLGP7bgt6t+dUdTiYiILphnfhq7iHx2jIeGkEdmDcK+P8/C1P4RAACFQkCqbf+QnHbLdD/fl4/i2mbsz6tBTSP3ESEior7HMz+NXcSTV8cA0ghNeKDW4bXWZbqtIcRqFfHx7jz536e4hJeIiPogHwshnrs6piv2pbtti1O3ZZcjr830zKkyrp4hIqK+p+98GjuBvDpG4ZkjIZ2xF6e2Xab70S5pFMTeD46EEBFRX+RTIcS+Y6pG1Xe63X46prCmCZuOlwIAbpmcDIAhhIiI+qa+82nsBEZ5JKTvdDvVNh1TUd+Cd3fk4NUNJ2AVgSn9wnHF8GgAwMkyhhAiIup7fGrHVLOHF6Z2Rq9TIzncH7mVjfjbd5ny67dOScYA+z4iVY1oMVugVSnd1UwiIqIe86kQ0hcLUwHg7dvS8N3BImQWG5BZZEBKRAAuHxYNlUKAXqtCXYsZuZWNGBStd3dTiYiIus3HQohn7xPSlUHRejx6xeBO3+sXFYiD+TU4VVbPEEJERH1K3/o0vkCtZ8f0nemYc7FPybAuhIiI+hqfCiFma9+cjjmb/lFS4SpXyBARUV/jPZ/G3WAy973C1HPp38k+IkRERH2Bb4UQbxwJkUNIPUSRJ+oSEVHf4T2fxt3g6WfHnI/kcH+oFAIajRYU1za7uzlERETd5lMhxNxHl+iejVqpQHK4P4DzrwvhCAoREbmD93wad4O8Y6oXhRCgzZRMuxUyJ0rr8NTXR3CitK7TnxNFEUvXHMa4v63HmQrWlBARUe9yyadxYWEhbrnlFoSHh8PPzw8jR47Evn37XPGoHumLO6Z2R/+ojsWpa/cXYN6/f8GHO3PxyGcHOh3tWLu/EJ/uyUN1owlr9hf2WnuJiIgAF2xWVl1djWnTpmHGjBn48ccfERkZiezsbISGhjr7UT1isYqw1aVC3YfOjukO+0jIz5klMFtF1DWb8N2hYvn9o0UG/HqqEtMGRMiv5Vc14umvj8r/3pBZikcvH9Tp/QtrmqBRKhCp17qoB0RE5IucHkJefPFFJCYm4r333pNfS01NdfZjesxelAoA6j50im53jEkMBgCUGlrw6Z48AIAgAA9eNhBVDS34aFce3tx2Wg4hFquIP35+EHUtZoyID0JmkQGZxQYU1jQhPsTP4d41jUZctWIbwgI02Pz/LoUgeNcoEhERuY/TP42/+eYbpKWl4Xe/+x2ioqIwduxYvP32211e39LSAoPB4PDlCm1DiErhXR+kA6L0+GrxNDw/fyQenjkQi6Yk4+PfT8Kjlw/C3Rf1h0IAtp0ox7FiA0RRxEvrjmPPmSoEaJT4z83jMT5ZGqXaeKy0w73Tc6thaDbjTGUjKuqNvd01IiLyYk4fCTl9+jRWrlyJRx99FH/605+wd+9ePPTQQ9BoNFi0aFGH65cvX46//vWvzm5GB/aVMYB3rY6xG5MYgjGJIR1eTwr3x1UjY/H9oWL8Z8sp+KkV+HxfAQDgmWuGIyncH7OGRmPvmWqszyzFbVNSHH5+f16N/H1eVQOnZIiIyGmc/mlstVoxbtw4PP/88xg7dizuvvtu3HXXXVi1alWn1y9duhS1tbXyV35+vrObBKB1JEQhAEovGwk5l3su7gcA+PZgET7fVwCFADw3fwR+l5YIAJg5NBoAsOt0JeqaTQ4/uz+/Wv4+r6qxl1pMRES+wOkhJDY2FsOGDXN4bejQocjLy+v0eq1Wi6CgIIcvV/DG3VK7a1RCCCb3CwMA+KmVePu2NCyclCy/3z8yAKkRATBZRGzPrpBft1hFHMyvlf+dW8kQQkREzuP0T+Rp06YhKyvL4bUTJ04gOTm5i5/oHa3nxvheCAGAF64bhZsnJeGzeybLIx92giBg1tAoANIqGbuTZfWobzHL/85jCCEiIidy+ifyI488gl27duH555/HyZMn8cknn+Ctt97C4sWLnf2oHjFbvXOPkO5KiQjA8/NHYlRCSKfvz7IFk01ZZfJ+KvvzpKkY++wVp2OIiMiZnF6YOmHCBKxduxZLly7Fs88+i9TUVKxYsQILFy509qN6JDk8ADuemAGr9dzX+qLxyaEI8VejptGErSfKMXNotFyUOm1ABLZnVyD3AkJIXbMJ+/NqoFII0KoViA/xR0ywzkmtJyKivsjpIQQArr76alx99dWuuPV5UysVSAj1d3czPJZKqcANaYl4a9tpvLM9RwohtqLUeWPisT27AuV1LWg0muGv6fn/bP74+UH83GaqRyEAi2cMwIOXDYTGy/ZtISKi7uFff5ItmpoCpULAztOV2HW6Etm2s2guGRSJIJ0UPPKrmnp8X5PFKhe89o8MQGKYH6wi8Pqmk5j/n1+6PNuGiIi8G0MIyeJD/DBnZCwA4PH/OwRRBBLD/BCp1yI5PAAAkFvZej7N1hPlKKtrPud9jxTWoslkQYi/GusfuQTbH78Mb9w8DiH+ahwtMuD6lb+ior7F4Wcq6lscNpgjIiLvwxBCDv5wkbTFvr0IdWyitJtqUri/w+sbMkux6L978MhnB855zz05VQCACSlhUNiqXOeMisXPSy7GoOhA1DWb8cnu1iXcx0sMuOSlzZjxjy0OoYeIiLwLQwg5GJUQgompYfK/xyaFAACSwhxDyHpbfcfOU5WobDeK0d7eM1IImZgS5vB6VJAOi2cMAAB8uDMXLWYLAOCfP59Ag9GCguom3PDmTpwqr7/AXhERkSdiCKEO/jC99cDBsUnSSEiyLYTkVjZCFEVsyy4HAFhFYNPxMvl6URSRXyVdAwBWq4i9Z6QC17bhxu43I2MRE6RDRX0LvjtYjAP5NVifWQqFAKRGBKDU0IIb39yJrBLWjRAReRuGEOpg1tBoXDo4EhNTwzA8TtrB1j4dk1/ViJNl9Siuba0F2dDm4Lu3tp3GRS9txtvbTwMATpTVobbJBH+NUr5XW2qlArdNlTaye3dHDv75s7TR3fyxCfjyvqkYFhuEinojln11pMv2fplegOkvbsKhgpoL6zgREfUqhhDqQKEQ8P4dE/H5PVPkHWbt0zH51Y3YkiWNgsTZ9vnYdqICzSYLGo1mrNp6CgDwxuZTqG8xY6+tHmR8cihUXexWe/PEJOjUCmQWG7A9uwIqhYAlswYiLECDlbeMAwBk5FWjoc3urXYNLWb8/ftMFFQ34d+bTjrxt0BERK7GEELdEhvsB7VSgMki4vN90iGDt09LQWywDk0mC3aeqsQX+wpQ3SgdgFfbZMJHu3Kxu01RaldC/DW4flyC/O8bJyQi0RZ6ksMDEB/iB7NVRHpudYef/WhXrvzMTcfLurVah4iIPANDCHWLUiEg0bbZW+v+IVHydu8/HSmRp2Am2Wo/3tl+ulshBADumJYKhQBoVQo8cNkAh/cm2Q7f23W60uH1JqNFfqa/RgmzVcSajMLz7iMREfUuhhDqNvvoBABEB2kxKDoQs4ZJIeSL9HwUVDchPECDdxalISHUDxX1RpTXtUCtFORVNl0ZEBWI1XdPwRf3TkFssJ/De5P7hQOAHGjsPtmTh4p6IxLD/PDnOUMBAJ/tzZeLYomIyLMxhFC3JYe3hpCLBkZCEARM7heGAI0SVtvn/qKpKdDr1Ljv0v7ytaMSQqBTK895/4mpYZ0esDfFFkIO5teg0SjVhTSbLHjTVn+y+NIBuHZMPAI0SuRUNMj7khARkWdjCKFuS2ozEnLxoEgAgFalxCWDpe/91ErcOlla6fLb8QmIDtIC6Hxpbk8khPp1qAtZvScPZXUtiA/xw3XjEhCgVeGaMXEApNEQIiLyfAwh1G32ECIIwEUDIuTXb5yQBAC46+J+CA3QAJDCyQvXjcLkfmG4eWLSBT1XEAS5LmT36SoYmk14zbYS5r5L+8sH4Nnb8f3hYtQ2mS7omURE5HouOUWXvNP45FCE+KsxrX+EHDYA6YC7w89cgUCt4/+cZgyJwowhUU559uTUcKzJKMSu05WwiiKqGozoFxmAGyckyteMTgjG4Gg9skrrsPFYKa5rs+KGiIg8D0MIdVt4oBYZf7kcnZV96nVqlz7bXpx6IL8GhwtrAQBLrxoq72MCSCMmFw+KQFZpHfblVncZQswWKwRBgNJ2jg0REbkHp2OoRxQK93x4J4b5IS5YB7NVRIvZiompYZg1tOMoy/hkaZv5jE72FAGkzc1mvbIVc1/fATNP6SUiciuGEOoTpJU44fK///yboRCEjmFonC2EZJXWdVoX8s3BIpypbERmsQE72+07QkREvYshhPqM2SNiAADXj0vA6MSQTq+J0uuQFOYPUZSmbtr7ZHee/P3XB4pc0UwiIuomhhDqM2YPj8GGRy/Gi9ePPOt1abbRkPbbvB8uqJXrSQBpl9dmk8X5DSUiom5hCKE+ZUCUvsuD8OzGySGk/Q6ruQCAq0fFIj7ED/UtZmw8VuaahhIR0TkxhJDXSUuRQsiBvBq5+LSu2SRPvyyclCxvbPb1AZ41Q0TkLgwh5HUGRumh16rQYLTgeEkdAKn+o9FoQb/IAEzuF4Zrx8QDALZklaO2kRubERG5A0MIeR2lQsBY+1LdvGo0GS34cOcZAMDNE5MgCAIGx+gxJEYPo8WKH48Uu7G1RES+iyGEvNL4JCmE7DxViXs+SseJ0nrodSpc32YDs3m20ZC1+zklQ0TkDgwh5JXsdSE/HinBthPl8FMr8d/bJzhsN3/NmDgIArA7pwq5lQ3uaioRkc9iCCGvNCYxBPaNXTUqBd6+LQ0TUhxP840P8cNFA6UTgD/fx5N3iYh6G0MIeaUArQoXDYyERqXAyoXjMH1gRKfXLbAdgPfFvgJu405E1Mt4gB15rXcWpaGhxYwQf02X18waGo2wAA3K6lqwJascs4ZF92ILiYh8G0dCyGuplYqzBhBAmqq5fpxUoLp6rzQlI4oiTpTWwcSRESIil2IIIZ93o21KZnNWGbaeKMeCt3bhin9tw3PfH3Nzy4iIvBunY8jnDYjSIy05FPtyq7Hov3vk1zOLDW5sFRGR9+NICBGABROT5O9HJwQDAMrrWtzVHCIin8CRECIA142Nh9UqIjncH9FBOlz6jy0oNTRDFEUIguDu5hEReSWGECIACoWAG2y1IY1Gs+3/WlDfYoZep3Zn04iIvBanY4ja8deooNdK+bzUwCkZIiJXYQgh6kRUkBYAUFbXfM5rK+tbcKK0ztVNIiLyOgwhRJ2IDtIBAMq6MRJyz//ScdWr23GMq2mIiHqEIYSoE/YQUmo4+0hIVYMR+3KrYbGK2HistDeaRkTkNRhCiDoRpZemY85VE7Inp1L+fsfJCpe2iYjI2zCEEHUiyj4Sco6akF2nq+TvM3Jr5JU1RER0bgwhRJ2IthWmlp9jJGR3TmsIMVqs2Hum2qXtIiLyJgwhRJ2I7sZISG2jCcdLpGLUGYMjAQC/cEqGiKjbGEKIOhGtby1MFUURAFDdYMS3B4vk03X3nqmCKAL9IgJw7VjpJN4d2QwhRETdxRBC1An7PiHNJisMzVKdx8s/Z+HBT/fLp+vuthWlTuoXhqn9IwBIh95V1regrtmEhe/swv0fp8shhoiIHDGEEHVCp1YiSCftmlpum5LZd0aq//jfrlwcLzHI9SCTUsMRqddiSIwegLRK5qFP9+OXk5X44XAJcisb3dADIiLPxxBC1IXWvUJa0GS04GRZPQDAYhWxdM1hHCmsBSCNhADA9AHSaMhTXx/F5qxy+T57z1SBiIg6Yggh6kLbDcuOlxhgFQG9TgWdWoH9eTWwikBSmD9ig/0AANMGSiGktskEABgRHwQASM/lihkios4whBB1oe2GZUeLpFUw45JCcf+lA+RrJqaGtX6fEgaNUvp/qQdmDMDDMwcBAPYxhBARdUrl7gYQeaqoNiMheVUWAMDwuCDcfXE/fJGej/yqJkwbEC5fH6BV4V83jkFhTSP+ML0famwjIifL6lHdYERogKb3O0FE5MEYQoi6IG9YVteCvCqpuHREfDB0aiX+d+ckbM8uxzWj4x1+Zs6oWPn7sAAN+kcG4FR5A9JzqzFrWDREUcRPR0owLC4IyeEBvdcZIiIPxOkYoi7Ya0IKapqQVVIHABgRFwwASIkIwK1TUqBUCGe9x4QUabpmb65UnPrVgULc93EG7nhvLyxWLt0lIt/GEELUBftIyJHCWhgtVuh1KiSG+fXoHuOTQwEA6Wekk3b/vekkAOB0RQPWZ5Y4t8FERH0MQwhRF6Jsu6baRyyGxwVBEM4+8tGefSTkUEEtvj5QiFPlDfJ7q7ae5kZmROTTGEKIuhBpWx1jZ5+K6YnkcH9EBGpgtFjx9DdHAQC3TE6CRqXAgfwaHnhHRD6NIYSoCzq1EiH+avnfw237fvSEIAhIS5ZGQ+qazQjUqvDYFUNw/bgEAMBb2045p7FERH0QQwjRWdgPsgPObyQEANJSQuXvb5uSjGB/Ne66KBWCAGw4VoaTZXUX3E4ior6IIYToLOwH2enUCvSLDDyve0xKDZfv8fvpqQCAfpGBuHxoNADg5XVZrA0hIp/EEEJ0Fvbi1KGxQedcjtuVkQnBePm3o/De7RMRHthaZ/LQzIFQKQSsO1qK//5yxhnNJSLqUxhCiM4iKcwfADAmMeSC7vO7tERM6R/u8NqI+GD8Zc5QAMDzPxzDnhwedEdEvoUhhOgsFk1NxrKrh2HxjAHnvvi87p+CeWPiYLGKWPxJBsoMzS55DhGRJ3J5CHnhhRcgCAKWLFni6kcROV2Ivwa/n56KiEDtuS8+D4IgYPl1IzE4Wo/yuha8tinbJc8hIvJELg0he/fuxZtvvolRo0a58jFEfZq/RoWlvxkCAFifWcoiVSLyGS4LIfX19Vi4cCHefvtthIaGnvsHiHzYlP7hCNAoUWpowZFCg7ubQ0TUK1wWQhYvXow5c+Zg1qxZZ72upaUFBoPB4YvI12hVSlw0MBIAsP5YqZtbQ0TUO1wSQlavXo2MjAwsX778nNcuX74cwcHB8ldiYqIrmkTk8WYNk/YN2cgQQkQ+wukhJD8/Hw8//DA+/vhj6HS6c16/dOlS1NbWyl/5+fnObhJRnzBjcCQEAThaZEBRTZO7m9NBs8mCZpPF3c0gIi/i9BCSnp6OsrIyjBs3DiqVCiqVClu3bsVrr70GlUoFi8Xxj5hWq0VQUJDDF5EvCg/UYnySVD+18XiZm1vjyGSxYs5r2/GbV7fDaLa6uzlE5CWcHkJmzpyJw4cP48CBA/JXWloaFi5ciAMHDkCpVDr7kUReY6ZtK/cNmRc2JdNssmD5j8ewJcs5YeZUeT1OlTfgdEUDMotZt0VEzuH0EKLX6zFixAiHr4CAAISHh2PEiBHOfhyRV7l8WBQAYOepSjS0mM/7Pm9vO403t57Go58fdMrIRWZRa/BIz62+4PsREQHcMZXIo/SPDERyuD+MFitufXc3nv76CL7Ylw+L1XHvEJPFCrOl83BR1WDEW9tOy99vOn7hha6OIYTbyxORc6h64yFbtmzpjccQ9XmCIODaMfF4dWM2MvJqkJFXAwBoMllw25QUAIDZYsUNb+5EYXUTNvzxEgTp1A73WLnlJOrajKJ8sa8AV46IvaB2tZ2CSc+thiiKEITzO9CPiMiOIyFEHmbJrIH4evE0vPTbUZg7Og4AsHLLKbSYpaLutfsLsT+vBmV1LchoNzVSWNOED3bmAgCenjsMALA5qwylF3AmjSiKDiGk1NCCQg9cvUNEfQ9DCJGHEQQBoxNDcENaIl7+7ShEB2lRXNuM/0svgNFsxasbW8+XOVxQ6/Czr244AaPZisn9wnD71BSMTw6FVQTWZBR2+qzubBFfXNuMmkYTVAoBQ2L0AFgXQkTOwRBC5MF0aiXuvaQ/AOA/m0/h4925KKhuHYU4VNgaQoprm/B/6QUAgMevHAJBEHBDWgIA4It9+XLgsFhFrDtaghvf3IlBf/kRPx4uPmsb7PUgA6ICMaV/OAB0GIEhIjofvVITQkTn76aJSXhj8ykU1jTh798fAwBcPSoW3x0qdhgJ2Z5dAasIjEkMwTjbfiNzRsXhmW8ycbqiAf/Zcgoltc3YnFXmEGSe+uYoLhoUiUBt538O7FMxw2KDMD45FO/9cgbpeR1DiCiK2HCsDLtOV+JYsQGlhmYsu3oYLh0c5bTfBRF5F46EEHk4nVqJey7uB0AaxYgP8cPf5o2AIAAlhmaU2eo9dp2qBABMGxAu/2ygVoU5o6Si1JfXZeF/u6SRlBB/Ne6/tD9Swv1RXteC1zdloyv2kZBhcVIIAYBjxXUdlhB/uicfd324D+/uyMGvpypxqrwBX+wrcNJvgYi8EUMIUR+wcHISwgM0AICHZg5AaIAGAyIDAQCHC2shiiJ2npZCyJR+EQ4/e9dF/RAf4ofhcUG4Y1oKVi4ch51PzsTjVw7BX+ZIxav/3ZGDnIqGTp8tj4TEBSE22A/xIX6wWEUczK+Rr2kxW+QgM2dkLG6bkgwAyK9udNJvgIi8EadjiPoAf40K/719Ao4WGfC78dIhjyMTgpFdVo9DBbXoHxmI4tpmqJWCPFphNzhGj1+evKzT+84cGoWLB0Vi24lyPPd9Jt5ZNMHhfUOzCXlVUpAYFisdqTAuORSFNU1Iz63G1AFS4PliXwGKa5sRHaTFP28YjTOVDfhwZ678s53Jr2rE8ZI6XG47uI+IfA9HQoj6iNGJIbh5UhIUCml/jlHxwQCkkRD7KMjYxFD4abp/NIIgCHjq6mFQKQRsOFaGH9oVqR4vrgMAxIf4IcRfGokZnxQCANhnK041mq1YueUUAOC+S/pDp1YiMdQfAFDTaIKh2dThuQ0tZix4axfu+nAfTw0m8mEMIUR91MiEEADAoYJa7LTVg0zuH36Wn+jcgKhA3GWrOfnj5wcddkc9WiQVvg6NbT1YMi0lDACw9UQ5lq45jPd/zUFhTROi9FosmJgEAAjQqhARKIWW/E5GQ/61/oS818hne3lyNpGvYggh6qOGxQZBqRBQUd8ijyZMPY8QAgB/vHwQpg+IQJPJgrs+3IfK+hYAjkWpdsPjgvCH6akAgE/35OH5H44DAO61jYLYJdhGQ9qHkCOFtXjv1zPyvzcdL5Of5yzrM0tx67u7caSw9twXE5HbMIQQ9VF+GiUGRknFqQ1GC7QqBcbapkp6SqVU4N83j0VKuD8Ka5qw8J3dWPxxBtbbws2wNiMhgiDgL1cPw+q7JyMlXAoakXotbp6U5HDPpDDpvbZ1IRariD+vPQyLVcTVo2IxKiEYZquIrw8UnVe7OyOKIpb/cAzbsytw01u7sO8Mz7oh8lQMIUR92KiEYPn78cmh0Kq6Xw/SXoi/Bu8sSkOgVoXjJXX4/nAxahpNUCuFTsPN5H7h+GnJxXjx+pH46PeTHEZBgNYQkl/VuifJJ3vycLCgFnqdCk9dPQy/HS9tpmbfZM0ZDhbU4rRtpU9dixm3vrsH27PLnXZ/InIero4h6sNGJoTgc9teHFP6nd9UTFsDovT45K5J2JpVDr1OhWB/NYbGBiE6SNfp9Tq1EjdOSOr0vcQwPwCOIyFrM6S2Lpk1CFFBOswdFYe/f3cMmcUGZBYZHKZ9zpf9GbOHR6PZZMXWE+X4/fv7sHbxVAyPCz7HTxNRb+JICFEfZl8hA0DeUv2C75kQggdnDsTt01Ixf2wChsScXzBIDHOsCTFZrDhqqzG5bIi0i2pogAYzh0rff5lxfqMhzSaL/L3JYsW3h6QVPgsmJuHt29JwyaBIGC1WvPDj8fO6PxG5DkMIUR82JFaPpDB/pEYEYJRttYynsE/HFFQ3wWoVkVVShxazFUE6lVxLAkCekvlqfyFMFmuPnvHhzjMY+cw6PPPNUYiiiG0nylHVYEREoBYXDYiARqXA3+aNgFopYHt2BX45WeG8DrZzqKAGq7aegrmHfeipLVllWLrmEJqMlnNfTOThOB1D1IdpVUr8tOQiAIBG5Vn/TREb7AeVQoDRYkVpXTMOFtQAkEZaBEGQr7t4UCQiArXyKp8rR8R26/6bjpfimW+OwioC7/96BpF6rby76zWj46BSSr+PpHB/LJyUjPd/PYMXfzqOrxdPc3i+M1isIu77KAOFNU0I9lPjpomdT1E5wzPfHMWZykaMiA/GwknJPfrZU+X1iA7SdXlOEFFv86y/WkTUY/4aFfw1nveholQIiA+11YVUNuJQvrRcdnSiY12GWqmQT/v9eHdet+59vMSABz/ZD6sIjLRNSb28LgvrjpQAAOaPjXe4/oHLBiBAo8Shglr8cLiky/uW1TXjnz9nYduJnhWybskqk/c9WXOe00rdkV/ViDOV0vTW3pyerfpJz63GrFe24okvD7miaUTnhSGEiFym7TLdtiMh7d00MQmCIJ0EnFvZ+Rk2dkeLavH79/ehwWjB1P7hWHP/VNxt22zNbBUxICoQI+Id61giArXyhmwvrzveYdpHFEV8vjcfs/65Fa9vOomHV++HxSrK7zcazfjPlpMoqmlCZz7alSt/v/dM9Tn7cL7aTift6WEI+elIMUQR2JFdAVEUz/0DRL2AIYSIXMZenHqitA4nSqUt4MckhnR63cUDIwFIy3gBwGoV8f4vOXh1QzY2ZJYiu7QOS9ccxtWv70BhTRP6RQRg5cLxUCsVePLKIbjadlrwzROTOp1u+cNF/RAWoMGZykb8atthFpCmUu76cB8e//IQDM3SycDVjSY5NAHAqi2n8NJPWVj21ZEO982vasQW28jJ4Gg9AGBNRmGPfk/dtb1NCCmqbUZBDw4I3HFS6nNtkwlFtc1ObxvR+WAIISKXsY+E/HikBFYRiA7Sdrncd6Fts7Mv9hWg2WTBM98exTPfZuJfG07gDx/uw+X/2oZP9+RBFIG5o+Pw6d2TEeyvBgAoFAJeWzAWPy25CLdPTen0/oFaFWYPlw7La3tezc5TldhwrAwalQJ/+s0QXDk8BgCw5XiZfM0PtmmebdnlqG10PAvnE1ubpg+IwP0z+gMA1uwvgNXq3NEGq1XEr7YQYq/p6O5oSEV9C44Vt27Hf6zN1vze4mRZfY8Lm8n9GEKIyGXsB9kVVEvTGGdbwXPZkCjEBOlQ1WDEwnd248OduRAEab+PwdF6qBQCRicE44t7p+D1m8Z2CDMKhYAhMUHyAX+dmTnEHkLK5CmJ722H9l0/Lh53X9xfXjK8OUsa3ThZVoeTZfUAAJNFxM+ZrTUlLWYLPredfXPL5CRcMSwGgVoV8quasPc8dmq1WEVsySpDXSeH/mUWG1DdaEKgVoUb0qSTlLsbQtqvCmobSLzB2v0FmPXKVtzzv3RONfUxDCFE5DL2kRC7zqZi7FRKBW6cIH24pttO6H3u2pF489Y0rHvkYpz4+1X4+oHpmGA7QO98TBsQAa1KgcKaJhwvqYPZYsW6o1Ko+M1IaTrnksHStNDhwlqU17XgJ9soiD3bfHeo9aThn46UoLLBiOggLWYNjYafRonfjJRGUs5nSuav3x7F7e/txZ/Wdpz22Z4tBYnJ/cLkM4L2dDPo2EOI3jaCktkmhIiiiJLa5j774V3fYpbPL9p0vAwftDmXiDwfQwgRuUz7ENJ2m/nOLJiYCJXt0/6pq4c5nEdzthGO7vLTKDF9QAQAaUpmT04VqhqMCPVXY7Jtx9kovU4ubN16ohw/2kKIvbD1l5MVqG4wosVswWsbs6V2T0iSlwRfP05a6fP94eJORzTs9udVOxzu99X+Qny4Uypw/eFwcYciWHuQmDYgAhNSwiAIwOnyBpTXnf3wP1EUscMWYG6y/T7bjoSs3puPycs34u3tp896H0+1cstJlNe1wM92bMDzPx5HVkmdm1tF3cUQQkQuE+yvhl7Xunx4VHzIWa+PDfbDe3dMwNu3peFO20m9zjZzqDQls+FYmTwVc8WwGKiVrX8OZwyWpmQ+3p2Lo0UGKATg7ov6YVhsEMxWaUrmjc2ncKq8ARGBWtw5rbWtE1LCkBoRgPoWMx74ZH+nm5d9d6gI8//zK2b8YwuW/3AMGXnVWLrmMADAX6OExSri492tK26aTRZ51OOigREI9lfLRbDnmvY5U9mIotpmaJQK3DYlWX6tvkUqwv18nzSd9J8tp9Bgew2QTjt+Y/PJDnUWy388hj98sE8+YdmdCqob8fb2HADAigVjMGNwJIxmKx5evd9hJ13yXAwhRORS9tGQ1IgAuZD0bC4aGInLh0W7rD32LeMPFtTIUyu/GeW4QdqltimZ/Xk1AIBJqeEID9Riju26d3fkYOWWkwCAv14z3KFfCoWAVxeMgZ9aia0nyvG0bTdXu7zKRiz9UgocZquIN7edxnX/+RVNJgsuGhiBf/xuNADg0z358gfpvjPVMJqtiA7Son+kdHLyxFRpWupcdSE7bCMo45JDkBDqj+ggLQAgq8SAsrpmHMiX+ljTaMJqW31LbaMJt7+3Fy+vy8Kne1r3bjlZVo83t57GhmOlmPvvHXj+h2NoNJrhLi/8eBxGsxVT+oXjimHReOm3oxEeoMHxkjq89FOW29pF3ccQQkQuZQ8h55qK6S0xwdJ0iyhKy1WD/dRyjYXdmMRQhLQJFleOkOo85tjqRk6U1sNkETFzSJRcA9LWqIQQvLpgDARB2oDt1Y3ZaDJaYDRb8cCnGahrMSMtORRv35Ym/37ignV4dcFYXDEsGnHBUoHutweLYLGK8mjF9AGR8vLj7oaQX2xTMfZpqGGx0lRTZnEdNh8vgygCaqV0z3e3n4bRbMULPx1DRb00zdP2hOOvD0h1LnqdChariLe2nca8f/8ij6p0pr7F7PSVQoC0X8x3h4ohCMCyq4dBEARE6rV46bejAAD//SWnx5vOUe9jCCEil5o2IAKCIE15eAr7KhkAuGJYtMNUDCDt9mrftwQAZtuW7aZEBMj1IgEaJf527Ygut4C/YngM/jJnGABgxYZsjP/7ely38hccKqhFsJ8ar940FpcPi8bPj1yM124ai/+7byrCAjRQKRW4xTZt8u6OHPzhg7345mARAGDu6NYRm4m2At1jJQZc8+8duHLFNtz+3h5szmpd+VPdYMSvp1prSQBgqD2EFBmwPlNahnz3xf0QqdeiqLYZT39zBJ/uyZd/D4cKapFVUgerVcTa/VIIeW7+SLy7KA2Rei2yy+rxzDdHO/0d/HqyAqOeWYcVttoZZ7IXDM8eFuNw+vLModG4dbL0+/vjFwdR1WB0+rPJeRhCiMilbpmcjCPPzJanMjzBrKGtIaT9VIydfUpoYkoYYoJblwPfOS0VKoWAp+cOR1yI31mfc+e0FCy7ehjiQ/zQaLTgSKFUR/Hyb0ch3vazOrUS14yOc7jXgglJ0KoUOF5Sh81Z5dCqFHh1wRhcaqtVAYCoIB2GxUojOocKanG8pA5bsspxx3t7ceWK7bjprV1Ie24DDM1mBOlU8vJoewjZn1eNHSelkYI5I+Pwe1sNjj2A3DQxUZ66+jKjAOl51SiobkKgVoXLh0Zj5tBovHHzOCgEabTEHpTa+nBnLqyiVHTrbBuPSQHqiuEdp+7+9JuhGBAViPK6Fjzx5aE+u/LHF3jegRNE5HUCPOzAtBHxQZgxOBINRgum9Y/o9Br7Dqzjk0MdXr9uXILDAXlnIwgCfj89FXdOS8GB/BqsO1qKfhEBuGL42UeFwgI0mD82Hqv35iMuWIc3b03DyE6ms96/YwIy8mqgUQlQKxXYmlWOT/fkIau0dXXI0NggLJ7RH0rb6iL7qMFx2wqS+BA/DI3VIzHMD29sPom6ZjMiArV48sqh2JVTifWZpViTUShv0nbliBj4aaSVKBNTw/DAjAF4bdNJ/HntYYxNDJF3ya1rNmFTlhQU8qoaUVTTdM7Q1l3FtU3ILDZAEOAQzOz8NEq8umAMrn3jF6zPLMU3B4swb0x8J3cid/OsvwxERL1AEAS8d8fEc14zd3Rcp+91J4C0v9fYpFCMTQo998U2z1wzHBNTw3DJoEiEB2o7vSYqSCfXqwBSUe+DMwfiq/2FsFhFzBoajaRwx2XSKeEB0KkVaDZJq15mDY2CIAjQ69R46LKBePnnLCy/biSC/dWYMTgKYQEaVNS34PN0aYSk/eGAD80ciB0nK5CRV4M/fnEQn909GYIgYMOxUhjNrStrdudUYv7YhLP2WRTFbp1wvMm2m+24pFCEBWg6vWZ4XDDuu6Q/Xtt0Ep/szmMI8VCcjiEi8kA6tRLXjUvoMoB0JdhPjUVTU3Dn9NQOAQSQ6jyGxLTWUMxqsxLprov74fizV8pTURqVAvPGSEFMFIGYIJ28n4qdSqnAqwvGwk+txJ6cKvycKW2J/+1BaeWRfYv5XafOXkAriiL++PlBTHth0zn3Ptlkm4qxTxd15UbbwYi7c6oc9mQhz8EQQkTkY+x1IYFaFSalOoaK9pvC2TdfA4B5Y+LkaZ22EsP85ZqSl346jor6FnllysMzBwKQRkLOZk1GIdbsL0RhTRM2HS/t8romo0VedmzfYr8r8SF+mGILTWtdUJdCF44hhIjIx0yyLe+9Yng0NKqzfwwMjwvCxJQwaFUK/M52Zk1n7rmkH0L91ThV3oD7P8qA2SpiaGwQbpyYCIUgbZBWXNvU6c+WGZrx129bV9jsyanu8jk7T1egxWxFfIifvGHb2dhD1JqMAhaoeiCGECIiHzNvTBw+uHMinp034pzXSvUzE7Dt8RkYEBXY5XV6nRoPXiaNeth3d507OhZBOjWGx0lFtbtPt07JWGx7h4iiiL98dURexQOcfRfYjW2mYrpTP3LliBj4a5Q4U9mIjLyuww25B0MIEZGPEQQBlwyKlOs1ziVAq+pwanFnFk5OQmJY6wqYq0dK9SST+0kjL/YpmRUbTqD/n37AxOc24Nr//IqfM0uhUgh49/YJUAjSappSQ3OH+4uiKBelXnaOqZi2bbcX7355HocKkmsxhBARkVNoVUo8ceUQANL+KvbCWHsx667TVfhqfyFWbJA2Lyura8FB27bx988YgAkpYXK9Smc7wX6RXoDi2mbo1Aq51qM7fmubkvnuYBHPlPEwXKJLREROc/WoOMQE6ZASESC/lmY79TenogGP/98hANIurXNGxiKnogFGixXX2Zb+TkgJw9EiA/aeqXJYIr0/rxp/WXsEAHD/pQOgs52a2x2T+4UjLliHotpm3PjmTjxx5RBMHdD5/jDUuzgSQkRETpWWEoaINkuLg/3UGG7bJM1osWLmkCg8ceUQjE4MwbVj43FDWqK890pnZ+KUGppxz//SYbRYccWwaDwwY0CP2qNQCHhq7jD4a5Q4WFCLm9/ZjT98sJejIh6AIYSIiFzOvjPtoOhArFgwptOlvgCQliJt6JZVWofaJhOMZivu/SgdZXUtGBgViFduHNNhGXF3XDkiFlsfm4FFU5KhVgrYcKzM4XA+cg9OxxARkcvdd2l/hAVocO3YeOh16i6vi9LrkBLujzOVjUjPrcLu01XYn1eDIJ0Kb9+W1u1i2s5E6rX467wRSAzzx9+/P4ZP9+ThFtthd+1tO1GOiECtw+F45HwcCSEiIpcL8dfgnkv6d2uVzQTbCcFvbD6FN7edBgC89NvRDnUmF+L6cQnQKBU4WmTA4YLaDu8fKqjBbf/dgxvf2onaJpNTnkmdYwghIiKPMsFWF5KeK+3rcevkZIczci5UaIAGV42U7vfJnrwO73+0KxcAUNdsxvu/nHHac6kjhhAiIvIoE20jIQAwJEaPP88Z6vRn3DQxCQDwzYFC1LeY5dcNzSb53BsAeHfHaRiaO46G7D1ThYtf2owxz/6M4U/9hLHP/oxfT1U4vZ3ejiGEiIg8SnK4P4bFBiFIp8K/bx7bo+W43TUpNQz9IgLQYLTg24NF8utf7S9Ek8mCQdGBGBAVCEOzGR/+esbhZy1WEX9ZewR5VY2oaTShwWhBdaMJb9mmjqj7GEKIiMijCIKAtYunYseTl2FA1LnPhznfZyyYKJ2F8+mePIiiCFEU8cluaXrm5olJePAyaSnwOztyHEZLvkwvQFZpHYJ0Knz34HSsvnsyAGB7dsU5TwBuy2IV8cKPx/Hx7twet7+2yYRdpyv7/Hk4DCFERORxtColgs6yisYZrh+XALVSwKGCWtz/cQa2nCjH8ZI66NQKzB+XgKtHxaFfRABqGk142zbK0Wg045/rswAAD142ECPigzG5XzhGJ4bAYhUdRlVazBb8fLQET355CBOf24BLX96M2sbWqZ1tJ8qxauspPPX1UZTVtW5TL4oitmeXn7Uo9vnvj2HBW7vwTZvn9UUMIURE5JPCA7X4+7UjoFYK+PFICe58fy8AYO6oOAT7qaFUCFhs2xjt1Y3ZWLJ6P1ZsyEapoQUJoX64bWrr8l77jq9r90vn05gtVtz6zh7c/b90rN6bj7K6FpypbMQX6fnyz3yZIe1TYrGK+OZAa5hYk1GIW9/dg1ve2Q2zxdpp2w/YtrtvW7/SFzGEEBGRz7pxQhL+796pSAj1g31m4+ZJSfL788fG44EZA6AQgK8OFMl1H4/NHgytqrVWZe7oOKgUAg4X1uJkWR3+s+UU9pypQoBGidunpuCOaSkApNU4oiiitsmEnzNL5Z+3hxdRFOVnHC6slZcot2W1ijhT2QAA2HGyHE3GvrvzK0MIERH5tNGJIfj+oYtw+9QUPHjZAIxJDJHfUygE/L/Zg7Hm/mnoHyntUzIqIRhzR8U53CMsQINLB0cCAJb/cByvbZQO6fv7/BF45prhePTyQfDXKHG6vAG7c6rw4+FiGM1WJIX5Q6UQcLTIgBOldfjlZCWySutg3xT21Q3ZyC6tc3hWsaEZLWZphKTZZMWOk313VQ5DCBER+bxgPzWeuWY4/njFYAhCx23hx9iCyhs3j8P7d0zsdOv4a21TMhuPl8FsFTFnZCyuHSO9ptepMc/2/ce787AmQxr5uHlSEmYMiQIgTcO8u0Ma+bh1cjIuGxIFo8WKx/7vECzW1gLUnPIGh+duaDOi0tcwhBAREXWDTq3EnFGxCAvQdPr+rKHR0Nu2lY/SS/UmbQPNQts0z4+Hi7HnTBUEAbh2TLxcT7J6bx42Z5VDEIA7pqXiufkjoNeqcCC/xmEFTY5tKibEXyrc3Xi8FFZr91fJVDUYe9Br12IIISIicgKdWok7pqXAT63EP28YjdB2YWVEfDBGJQTDbAsM0wdEICZYh8uGRiFIp0KNbeXMzCHRSIkIQGywHx6cKRXGrm8z2nGmQgoh80bHQa9VoaLeiAMFNd1qY5mhGZOf34hb393tEbUkDCFERERO8ugVg3Hkr7Nx0cDITt9f2Kbo9bpx0giIVqXEnDY1JndOT5G/n5QaDgA4Ulgr7wmSYwshA6P1uMRWh9LdKZkv0gtgtFjR0GKGn8b5m8D1FEMIERGREyk7qRexmzs6DnHBOsQG6zB7eOt5ODdNTIRSIWBsUgim9AuXXx8co4dKIaC60YTCmiYArSMh/SICcPmwaADAhmPnDiFWq4jP9ubbnpd0jqt7x/mfiUxEREQ94q9RYd0jF0O0fW83KiEE6x+5GOGBWoc6Ep1aiUHRemQWG3CksBYxQTrkVTUCAFIiAjA8LhhKhYATpfU4XmLAkJigLp/966lK5FU1Qq9T4ep2q3vchSMhREREvUivU3e6G2y/yEAE+3V8fVRCMABp35CC6iaYrSK0KgVignQI9ldjhm1K5r6PMlB9lqLTT20nBs8fG+8RUzEAQwgREZFHGxEvhZBDBbXyypjUiAB5mfDy60YhPsQPORUNuOejdLSYOxaclte1YN3REgDAggmeMRUDMIQQERF5tJG2EHKksFbeIyQlPEB+P1KvxXt3TIBeq8KenCo89Ol+bMkqQ2V962F6X2YUwGwVMToxBMPiup6y6W2sCSEiIvJgQ2L1UCul4tRfbLujpkYGOFwzKFqP/9wyDre/txfrjpZi3VGpUDUiUIPoIJ1c1Hqz7eRgT+H0kZDly5djwoQJ0Ov1iIqKwrXXXousrCxnP4aIiMgnaFVScSoAbMsuBwCkhgd0uO6igZF4/44JuGa0dPovAFTUG3G0yICaRpNHFaTaOX0kZOvWrVi8eDEmTJgAs9mMP/3pT7jiiiuQmZmJgICOvzQiIiI6u5HxwThaZIDJIu0VkhLR+efpRQMj5T1K6ppNyKtqRKmhGWWGFoxMCEaA1rMmQJzemp9++snh3++//z6ioqKQnp6Oiy++2NmPIyIi8nojE4Kx2rbHByAVpp6LXqfG8LhgDI8LdmXTLojLC1Nra2sBAGFhYa5+FBERkVeyF6cCQKBWhYjAzs+v6WtcOi5jtVqxZMkSTJs2DSNGjOj0mpaWFrS0tFbwGgwGVzaJiIiozxkcIxWnmiwiUiMCOj3pty9y6UjI4sWLceTIEaxevbrLa5YvX47g4GD5KzHRsyp3iYiI3K1tcWpX9SB9kctCyAMPPIDvvvsOmzdvRkJCQpfXLV26FLW1tfJXfn5+l9cSERH5qrTkUADA0Fi9m1viPE6fjhFFEQ8++CDWrl2LLVu2IDU19azXa7VaaLVaZzeDiIjIqzx6+WAMjNbj+nFd/4d9X+P0ELJ48WJ88skn+Prrr6HX61FSIm0TGxwcDD8/P2c/joiIyCcE+6txy+RkdzfDqQRRFEWn3rCLYpn33nsPt99++zl/3mAwIDg4GLW1tQgK8pytZYmIiKhr5/P57ZLpGCIiIqJz4QF2RERE5BYMIUREROQWDCFERETkFgwhRERE5BYMIUREROQWDCFERETkFgwhRERE5BYMIUREROQWDCFERETkFgwhRERE5BYMIUREROQWTj875kLZz54xGAxubgkRERF1l/1zuydnyHlcCKmrqwMAJCYmurklRERE1FN1dXUIDg7u1rWC6GHH3lqtVhQVFUGv10MQBKfe22AwIDExEfn5+d0+ZrgvYj+9hy/0EWA/vQ376T160kdRFFFXV4e4uDgoFN2r9vC4kRCFQoGEhASXPiMoKMhr/wfTFvvpPXyhjwD76W3YT+/R3T52dwTEjoWpRERE5BYMIUREROQWPhVCtFotnn76aWi1Wnc3xaXYT+/hC30E2E9vw356D1f30eMKU4mIiMg3+NRICBEREXkOhhAiIiJyC4YQIiIicguGECIiInILnwkhb7zxBlJSUqDT6TBp0iTs2bPH3U26IMuXL8eECROg1+sRFRWFa6+9FllZWQ7XNDc3Y/HixQgPD0dgYCCuv/56lJaWuqnFF+6FF16AIAhYsmSJ/Jo39bGwsBC33HILwsPD4efnh5EjR2Lfvn3y+6Io4qmnnkJsbCz8/Pwwa9YsZGdnu7HFPWOxWLBs2TKkpqbCz88P/fv3x9/+9jeHcyb6Yh+3bduGuXPnIi4uDoIg4KuvvnJ4vzt9qqqqwsKFCxEUFISQkBD8/ve/R319fS/24tzO1k+TyYQnnngCI0eOREBAAOLi4nDbbbehqKjI4R59vZ/t3XvvvRAEAStWrHB43Vv6eezYMVxzzTUIDg5GQEAAJkyYgLy8PPl9Z/z99YkQ8tlnn+HRRx/F008/jYyMDIwePRqzZ89GWVmZu5t23rZu3YrFixdj165dWL9+PUwmE6644go0NDTI1zzyyCP49ttv8cUXX2Dr1q0oKirCdddd58ZWn7+9e/fizTffxKhRoxxe95Y+VldXY9q0aVCr1fjxxx+RmZmJf/7znwgNDZWveemll/Daa69h1apV2L17NwICAjB79mw0Nze7seXd9+KLL2LlypX497//jWPHjuHFF1/ESy+9hNdff12+pi/2saGhAaNHj8Ybb7zR6fvd6dPChQtx9OhRrF+/Ht999x22bduGu+++u7e60C1n62djYyMyMjKwbNkyZGRkYM2aNcjKysI111zjcF1f72dba9euxa5duxAXF9fhPW/o56lTpzB9+nQMGTIEW7ZswaFDh7Bs2TLodDr5Gqf8/RV9wMSJE8XFixfL/7ZYLGJcXJy4fPlyN7bKucrKykQA4tatW0VRFMWamhpRrVaLX3zxhXzNsWPHRADizp073dXM81JXVycOHDhQXL9+vXjJJZeIDz/8sCiK3tXHJ554Qpw+fXqX71utVjEmJkZ8+eWX5ddqampErVYrfvrpp73RxAs2Z84c8c4773R47brrrhMXLlwoiqJ39BGAuHbtWvnf3elTZmamCEDcu3evfM2PP/4oCoIgFhYW9lrbe6J9PzuzZ88eEYCYm5sriqJ39bOgoECMj48Xjxw5IiYnJ4v/+te/5Pe8pZ833nijeMstt3T5M876++v1IyFGoxHp6emYNWuW/JpCocCsWbOwc+dON7bMuWprawEAYWFhAID09HSYTCaHfg8ZMgRJSUl9rt+LFy/GnDlzHPoCeFcfv/nmG6SlpeF3v/sdoqKiMHbsWLz99tvy+zk5OSgpKXHoa3BwMCZNmtRn+jp16lRs3LgRJ06cAAAcPHgQO3bswFVXXQXAO/rYXnf6tHPnToSEhCAtLU2+ZtasWVAoFNi9e3evt9lZamtrIQgCQkJCAHhPP61WK2699VY89thjGD58eIf3vaGfVqsV33//PQYNGoTZs2cjKioKkyZNcpiycdbfX68PIRUVFbBYLIiOjnZ4PTo6GiUlJW5qlXNZrVYsWbIE06ZNw4gRIwAAJSUl0Gg08h8Au77W79WrVyMjIwPLly/v8J639BEATp8+jZUrV2LgwIFYt24d7rvvPjz00EP44IMPAEDuT1/+3/GTTz6JBQsWYMiQIVCr1Rg7diyWLFmChQsXAvCOPrbXnT6VlJQgKirK4X2VSoWwsLA+2+/m5mY88cQTuOmmm+RDz7ylny+++CJUKhUeeuihTt/3hn6WlZWhvr4eL7zwAq688kr8/PPPmD9/Pq677jps3boVgPP+/nrcKbrUc4sXL8aRI0ewY8cOdzfFqfLz8/Hwww9j/fr1DvOQ3shqtSItLQ3PP/88AGDs2LE4cuQIVq1ahUWLFrm5dc7x+eef4+OPP8Ynn3yC4cOH48CBA1iyZAni4uK8po8kFanecMMNEEURK1eudHdznCo9PR2vvvoqMjIyIAiCu5vjMlarFQAwb948PPLIIwCAMWPG4Ndff8WqVatwySWXOO1ZXj8SEhERAaVS2aFit7S0FDExMW5qlfM88MAD+O6777B582YkJCTIr8fExMBoNKKmpsbh+r7U7/T0dJSVlWHcuHFQqVRQqVTYunUrXnvtNahUKkRHR/f5PtrFxsZi2LBhDq8NHTpUrkS396cv/+/4sccek0dDRo4ciVtvvRWPPPKIPMrlDX1srzt9iomJ6VAkbzabUVVV1ef6bQ8gubm5WL9+vcPR797Qz+3bt6OsrAxJSUny36Tc3Fz88Y9/REpKCgDv6GdERARUKtU5/yY54++v14cQjUaD8ePHY+PGjfJrVqsVGzduxJQpU9zYsgsjiiIeeOABrF27Fps2bUJqaqrD++PHj4darXbod1ZWFvLy8vpMv2fOnInDhw/jwIED8ldaWhoWLlwof9/X+2g3bdq0DkusT5w4geTkZABAamoqYmJiHPpqMBiwe/fuPtPXxsZGKBSOf3KUSqX8X13e0Mf2utOnKVOmoKamBunp6fI1mzZtgtVqxaRJk3q9zefLHkCys7OxYcMGhIeHO7zvDf289dZbcejQIYe/SXFxcXjsscewbt06AN7RT41GgwkTJpz1b5LTPmN6WETbJ61evVrUarXi+++/L2ZmZop33323GBISIpaUlLi7aeftvvvuE4ODg8UtW7aIxcXF8ldjY6N8zb333ismJSWJmzZtEvft2ydOmTJFnDJlihtbfeHaro4RRe/p4549e0SVSiU+99xzYnZ2tvjxxx+L/v7+4kcffSRf88ILL4ghISHi119/LR46dEicN2+emJqaKjY1Nbmx5d23aNEiMT4+Xvzuu+/EnJwccc2aNWJERIT4+OOPy9f0xT7W1dWJ+/fvF/fv3y8CEF955RVx//798qqQ7vTpyiuvFMeOHSvu3r1b3LFjhzhw4EDxpptucleXOnW2fhqNRvGaa64RExISxAMHDjj8TWppaZHv0df72Zn2q2NE0Tv6uWbNGlGtVotvvfWWmJ2dLb7++uuiUqkUt2/fLt/DGX9/fSKEiKIovv7662JSUpKo0WjEiRMnirt27XJ3ky4IgE6/3nvvPfmapqYm8f777xdDQ0NFf39/cf78+WJxcbH7Gu0E7UOIN/Xx22+/FUeMGCFqtVpxyJAh4ltvveXwvtVqFZctWyZGR0eLWq1WnDlzppiVleWm1vacwWAQH374YTEpKUnU6XRiv379xD//+c8OH1J9sY+bN2/u9P8XFy1aJIpi9/pUWVkp3nTTTWJgYKAYFBQk3nHHHWJdXZ0betO1s/UzJyeny79Jmzdvlu/R1/vZmc5CiLf089133xUHDBgg6nQ6cfTo0eJXX33lcA9n/P0VRLHNdoVEREREvcTra0KIiIjIMzGEEBERkVswhBAREZFbMIQQERGRWzCEEBERkVswhBAREZFbMIQQERGRWzCEEBERkVswhBAREZFbMIQQERGRWzCEEBERkVswhBAREZFb/H8gaOgwWeIzpQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(losses_10k).plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "21q5mFOghkk2",
        "outputId": "de9e8b49-1617-4a8e-8ba0-86fc21a39ddf"
      },
      "id": "21q5mFOghkk2",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20K Model"
      ],
      "metadata": {
        "id": "qpi5bEBQ1Phg"
      },
      "id": "qpi5bEBQ1Phg"
    },
    {
      "cell_type": "code",
      "source": [
        "data_path_20k = '/content/drive/MyDrive/W266 Final Project/google_translated_data/gt_20k_dataset.csv'\n",
        "trained_model_20k, losses_20k = train_pipeline(data_path_20k, train_model, tokenizer)"
      ],
      "metadata": {
        "id": "DUKG3UHB1RvV"
      },
      "id": "DUKG3UHB1RvV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5gyIykFRy7K"
      },
      "source": [
        "## 50K Model"
      ],
      "id": "w5gyIykFRy7K"
    },
    {
      "cell_type": "code",
      "source": [
        "# 50k model took 6 minutes to train\n",
        "data_path_50k = '/content/drive/MyDrive/W266 Final Project/df_25k.csv'\n",
        "trained_model_50k, losses_50k = train_pipeline(data_path_50k, train_model, tokenizer)"
      ],
      "metadata": {
        "id": "4JYzNxxw5RE0"
      },
      "id": "4JYzNxxw5RE0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XQ-b9xixNm0m"
      },
      "id": "XQ-b9xixNm0m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 100k model"
      ],
      "metadata": {
        "id": "FZCs7g-hTLwR"
      },
      "id": "FZCs7g-hTLwR"
    },
    {
      "cell_type": "code",
      "source": [
        "# 100k model took 14 minutes\n",
        "data_path_100k = '/content/drive/MyDrive/W266 Final Project/df_50k.csv'\n",
        "trained_model_100k, losses_100k = train_pipeline(data_path_100k, train_model, tokenizer)"
      ],
      "metadata": {
        "id": "EWCDPSNLTNVU"
      },
      "id": "EWCDPSNLTNVU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 200K Model"
      ],
      "metadata": {
        "id": "OuodMQcDXhkp"
      },
      "id": "OuodMQcDXhkp"
    },
    {
      "cell_type": "code",
      "source": [
        "losses_200k_df = pd.DataFrame(losses_200k)\n",
        "losses_200k_df.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "dq6UAsd-eb3F",
        "outputId": "d1875042-406f-4eba-e2a8-6adf2855bc0f"
      },
      "id": "dq6UAsd-eb3F",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: >"
            ]
          },
          "metadata": {},
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 200k model took 20 minutes\n",
        "data_path_200k = '/content/drive/MyDrive/W266 Final Project/df_100k.csv'\n",
        "trained_model_200k, losses_200k = train_pipeline(data_path_200k, train_model, tokenizer)"
      ],
      "metadata": {
        "id": "N81rfNuKXi91"
      },
      "id": "N81rfNuKXi91",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WMtyaOAtm6Zq"
      },
      "id": "WMtyaOAtm6Zq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# M2M-100 Models"
      ],
      "metadata": {
        "id": "QkPfs2CFHGxx"
      },
      "id": "QkPfs2CFHGxx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10K Model"
      ],
      "metadata": {
        "id": "dUU3SDCFIrJD"
      },
      "id": "dUU3SDCFIrJD"
    },
    {
      "cell_type": "code",
      "source": [
        "m2m_data_path_10k = '/content/drive/MyDrive/W266 Final Project/m2m_translated_data/m2m_10k_dataset.csv'\n",
        "\n",
        "m2m_trained_model_10k, m2m_losses_10k = train_pipeline(m2m_data_path_10k, train_model, tokenizer, 'm2m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Mv0bv7OHIYE",
        "outputId": "94a21d93-8d5a-42c5-dac4-6e6e7c73d6f0"
      },
      "id": "7Mv0bv7OHIYE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10k\n",
            "data loaded\n",
            "on gpu\n",
            "Epoch 1/1\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  6.543159008026123\n",
            "  Batch 1, Loss: 6.5432\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  9.588566780090332\n",
            "  Batch 2, Loss: 8.0659\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  7.606108665466309\n",
            "  Batch 3, Loss: 7.9126\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  7.2772297859191895\n",
            "  Batch 4, Loss: 7.7538\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.691890239715576\n",
            "  Batch 5, Loss: 7.3414\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.816404819488525\n",
            "  Batch 6, Loss: 6.9206\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.339329719543457\n",
            "  Batch 7, Loss: 6.5518\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.890199899673462\n",
            "  Batch 8, Loss: 6.2191\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.816929340362549\n",
            "  Batch 9, Loss: 5.9522\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.451101541519165\n",
            "  Batch 10, Loss: 5.7021\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.397191047668457\n",
            "  Batch 11, Loss: 5.4926\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.6934099197387695\n",
            "  Batch 12, Loss: 5.3426\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.3886923789978027\n",
            "  Batch 13, Loss: 5.1923\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.66099214553833\n",
            "  Batch 14, Loss: 5.0829\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.058478355407715\n",
            "  Batch 15, Loss: 4.9480\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.311126470565796\n",
            "  Batch 16, Loss: 4.8457\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.256882667541504\n",
            "  Batch 17, Loss: 4.7522\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.1212310791015625\n",
            "  Batch 18, Loss: 4.6616\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.2968807220458984\n",
            "  Batch 19, Loss: 4.5898\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9040629863739014\n",
            "  Batch 20, Loss: 4.5055\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.1576435565948486\n",
            "  Batch 21, Loss: 4.4413\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8018031120300293\n",
            "  Batch 22, Loss: 4.3668\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.1718063354492188\n",
            "  Batch 23, Loss: 4.3148\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9421818256378174\n",
            "  Batch 24, Loss: 4.2576\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8988521099090576\n",
            "  Batch 25, Loss: 4.2033\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.060389280319214\n",
            "  Batch 26, Loss: 4.1593\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9683473110198975\n",
            "  Batch 27, Loss: 4.1152\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0201797485351562\n",
            "  Batch 28, Loss: 4.0761\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.909721612930298\n",
            "  Batch 29, Loss: 4.0359\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9076313972473145\n",
            "  Batch 30, Loss: 3.9983\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9669103622436523\n",
            "  Batch 31, Loss: 3.9650\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8052961826324463\n",
            "  Batch 32, Loss: 3.9288\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.100289821624756\n",
            "  Batch 33, Loss: 3.9037\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9032721519470215\n",
            "  Batch 34, Loss: 3.8742\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.895904541015625\n",
            "  Batch 35, Loss: 3.8463\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6530468463897705\n",
            "  Batch 36, Loss: 3.8131\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.876091241836548\n",
            "  Batch 37, Loss: 3.7878\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9290966987609863\n",
            "  Batch 38, Loss: 3.7652\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.692338705062866\n",
            "  Batch 39, Loss: 3.7377\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.977031946182251\n",
            "  Batch 40, Loss: 3.7187\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7790098190307617\n",
            "  Batch 41, Loss: 3.6958\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.728109836578369\n",
            "  Batch 42, Loss: 3.6727\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6996872425079346\n",
            "  Batch 43, Loss: 3.6501\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9130632877349854\n",
            "  Batch 44, Loss: 3.6334\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6062097549438477\n",
            "  Batch 45, Loss: 3.6105\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.780735731124878\n",
            "  Batch 46, Loss: 3.5925\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8119330406188965\n",
            "  Batch 47, Loss: 3.5759\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.637686252593994\n",
            "  Batch 48, Loss: 3.5563\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8456249237060547\n",
            "  Batch 49, Loss: 3.5418\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8340704441070557\n",
            "  Batch 50, Loss: 3.5277\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.807990074157715\n",
            "  Batch 51, Loss: 3.5136\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.651484966278076\n",
            "  Batch 52, Loss: 3.4970\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9638078212738037\n",
            "  Batch 53, Loss: 3.4869\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4832584857940674\n",
            "  Batch 54, Loss: 3.4683\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.914461374282837\n",
            "  Batch 55, Loss: 3.4583\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6564226150512695\n",
            "  Batch 56, Loss: 3.4440\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7479560375213623\n",
            "  Batch 57, Loss: 3.4317\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7537484169006348\n",
            "  Batch 58, Loss: 3.4201\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7624590396881104\n",
            "  Batch 59, Loss: 3.4089\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.760613203048706\n",
            "  Batch 60, Loss: 3.3981\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.646806478500366\n",
            "  Batch 61, Loss: 3.3858\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.660074472427368\n",
            "  Batch 62, Loss: 3.3741\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.721534252166748\n",
            "  Batch 63, Loss: 3.3637\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.593430757522583\n",
            "  Batch 64, Loss: 3.3517\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6006760597229004\n",
            "  Batch 65, Loss: 3.3401\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.574930429458618\n",
            "  Batch 66, Loss: 3.3285\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.808809995651245\n",
            "  Batch 67, Loss: 3.3208\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.58276104927063\n",
            "  Batch 68, Loss: 3.3099\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.566655158996582\n",
            "  Batch 69, Loss: 3.2992\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.694298028945923\n",
            "  Batch 70, Loss: 3.2905\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7244467735290527\n",
            "  Batch 71, Loss: 3.2825\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7810065746307373\n",
            "  Batch 72, Loss: 3.2756\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5909652709960938\n",
            "  Batch 73, Loss: 3.2662\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6641509532928467\n",
            "  Batch 74, Loss: 3.2581\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.777676820755005\n",
            "  Batch 75, Loss: 3.2517\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.607447385787964\n",
            "  Batch 76, Loss: 3.2432\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4319417476654053\n",
            "  Batch 77, Loss: 3.2326\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7244038581848145\n",
            "  Batch 78, Loss: 3.2261\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.61857533454895\n",
            "  Batch 79, Loss: 3.2184\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.52236008644104\n",
            "  Batch 80, Loss: 3.2097\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.71392822265625\n",
            "  Batch 81, Loss: 3.2036\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.625847578048706\n",
            "  Batch 82, Loss: 3.1966\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7378640174865723\n",
            "  Batch 83, Loss: 3.1910\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6015920639038086\n",
            "  Batch 84, Loss: 3.1840\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.589594841003418\n",
            "  Batch 85, Loss: 3.1770\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.54258131980896\n",
            "  Batch 86, Loss: 3.1697\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.648637056350708\n",
            "  Batch 87, Loss: 3.1637\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5766117572784424\n",
            "  Batch 88, Loss: 3.1570\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.750765085220337\n",
            "  Batch 89, Loss: 3.1524\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.608426809310913\n",
            "  Batch 90, Loss: 3.1464\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.550408363342285\n",
            "  Batch 91, Loss: 3.1398\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.647285223007202\n",
            "  Batch 92, Loss: 3.1345\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4936203956604004\n",
            "  Batch 93, Loss: 3.1276\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6876754760742188\n",
            "  Batch 94, Loss: 3.1229\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4275195598602295\n",
            "  Batch 95, Loss: 3.1156\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5019049644470215\n",
            "  Batch 96, Loss: 3.1092\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4570891857147217\n",
            "  Batch 97, Loss: 3.1025\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6500661373138428\n",
            "  Batch 98, Loss: 3.0979\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.471306085586548\n",
            "  Batch 99, Loss: 3.0915\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4607956409454346\n",
            "  Batch 100, Loss: 3.0852\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7639729976654053\n",
            "  Batch 101, Loss: 3.0820\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6692724227905273\n",
            "  Batch 102, Loss: 3.0780\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.381711483001709\n",
            "  Batch 103, Loss: 3.0712\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.611755609512329\n",
            "  Batch 104, Loss: 3.0668\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6697545051574707\n",
            "  Batch 105, Loss: 3.0630\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4682631492614746\n",
            "  Batch 106, Loss: 3.0574\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3918182849884033\n",
            "  Batch 107, Loss: 3.0512\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5520951747894287\n",
            "  Batch 108, Loss: 3.0466\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.367030382156372\n",
            "  Batch 109, Loss: 3.0404\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5048255920410156\n",
            "  Batch 110, Loss: 3.0355\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.628162384033203\n",
            "  Batch 111, Loss: 3.0318\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.561310291290283\n",
            "  Batch 112, Loss: 3.0276\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.440110445022583\n",
            "  Batch 113, Loss: 3.0224\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3992764949798584\n",
            "  Batch 114, Loss: 3.0169\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3257761001586914\n",
            "  Batch 115, Loss: 3.0109\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4170925617218018\n",
            "  Batch 116, Loss: 3.0058\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.48773455619812\n",
            "  Batch 117, Loss: 3.0014\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7524256706237793\n",
            "  Batch 118, Loss: 2.9993\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.415239095687866\n",
            "  Batch 119, Loss: 2.9944\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.406832218170166\n",
            "  Batch 120, Loss: 2.9895\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3939263820648193\n",
            "  Batch 121, Loss: 2.9846\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3137331008911133\n",
            "  Batch 122, Loss: 2.9791\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.338669776916504\n",
            "  Batch 123, Loss: 2.9738\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.466224193572998\n",
            "  Batch 124, Loss: 2.9698\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8433868885040283\n",
            "  Batch 125, Loss: 2.9687\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6390128135681152\n",
            "  Batch 126, Loss: 2.9661\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.503586530685425\n",
            "  Batch 127, Loss: 2.9625\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4056525230407715\n",
            "  Batch 128, Loss: 2.9581\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.524477958679199\n",
            "  Batch 129, Loss: 2.9548\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.410637378692627\n",
            "  Batch 130, Loss: 2.9506\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.627511739730835\n",
            "  Batch 131, Loss: 2.9481\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5321414470672607\n",
            "  Batch 132, Loss: 2.9450\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6912078857421875\n",
            "  Batch 133, Loss: 2.9431\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.703403949737549\n",
            "  Batch 134, Loss: 2.9413\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3838112354278564\n",
            "  Batch 135, Loss: 2.9371\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.233964443206787\n",
            "  Batch 136, Loss: 2.9320\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6473278999328613\n",
            "  Batch 137, Loss: 2.9299\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5762720108032227\n",
            "  Batch 138, Loss: 2.9273\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7876317501068115\n",
            "  Batch 139, Loss: 2.9263\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.43988037109375\n",
            "  Batch 140, Loss: 2.9229\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.374913454055786\n",
            "  Batch 141, Loss: 2.9190\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.493462085723877\n",
            "  Batch 142, Loss: 2.9160\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4137959480285645\n",
            "  Batch 143, Loss: 2.9125\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4497203826904297\n",
            "  Batch 144, Loss: 2.9092\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4415714740753174\n",
            "  Batch 145, Loss: 2.9060\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.310105800628662\n",
            "  Batch 146, Loss: 2.9019\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5075831413269043\n",
            "  Batch 147, Loss: 2.8993\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3895392417907715\n",
            "  Batch 148, Loss: 2.8958\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.597313404083252\n",
            "  Batch 149, Loss: 2.8938\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.616572141647339\n",
            "  Batch 150, Loss: 2.8920\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5657174587249756\n",
            "  Batch 151, Loss: 2.8898\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6681926250457764\n",
            "  Batch 152, Loss: 2.8883\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5085158348083496\n",
            "  Batch 153, Loss: 2.8859\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.547694444656372\n",
            "  Batch 154, Loss: 2.8837\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2823903560638428\n",
            "  Batch 155, Loss: 2.8798\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.57891845703125\n",
            "  Batch 156, Loss: 2.8779\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7137293815612793\n",
            "  Batch 157, Loss: 2.8768\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2468700408935547\n",
            "  Batch 158, Loss: 2.8728\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.53583025932312\n",
            "  Batch 159, Loss: 2.8707\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3383913040161133\n",
            "  Batch 160, Loss: 2.8674\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.353285551071167\n",
            "  Batch 161, Loss: 2.8642\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5249507427215576\n",
            "  Batch 162, Loss: 2.8621\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3245699405670166\n",
            "  Batch 163, Loss: 2.8588\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2661640644073486\n",
            "  Batch 164, Loss: 2.8552\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5412402153015137\n",
            "  Batch 165, Loss: 2.8533\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.321251153945923\n",
            "  Batch 166, Loss: 2.8501\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3207499980926514\n",
            "  Batch 167, Loss: 2.8469\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.387199878692627\n",
            "  Batch 168, Loss: 2.8442\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4855055809020996\n",
            "  Batch 169, Loss: 2.8420\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.317180633544922\n",
            "  Batch 170, Loss: 2.8390\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5374112129211426\n",
            "  Batch 171, Loss: 2.8372\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2937395572662354\n",
            "  Batch 172, Loss: 2.8340\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3762404918670654\n",
            "  Batch 173, Loss: 2.8314\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3951728343963623\n",
            "  Batch 174, Loss: 2.8289\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3958797454833984\n",
            "  Batch 175, Loss: 2.8264\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.344449758529663\n",
            "  Batch 176, Loss: 2.8237\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4781763553619385\n",
            "  Batch 177, Loss: 2.8217\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4574334621429443\n",
            "  Batch 178, Loss: 2.8197\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2926547527313232\n",
            "  Batch 179, Loss: 2.8167\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4033665657043457\n",
            "  Batch 180, Loss: 2.8144\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5229427814483643\n",
            "  Batch 181, Loss: 2.8128\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2766175270080566\n",
            "  Batch 182, Loss: 2.8099\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.345245122909546\n",
            "  Batch 183, Loss: 2.8073\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4665544033050537\n",
            "  Batch 184, Loss: 2.8055\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1840925216674805\n",
            "  Batch 185, Loss: 2.8021\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3826448917388916\n",
            "  Batch 186, Loss: 2.7999\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4031708240509033\n",
            "  Batch 187, Loss: 2.7977\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4181056022644043\n",
            "  Batch 188, Loss: 2.7957\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.549483060836792\n",
            "  Batch 189, Loss: 2.7944\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2732813358306885\n",
            "  Batch 190, Loss: 2.7917\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.322855234146118\n",
            "  Batch 191, Loss: 2.7892\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.187321662902832\n",
            "  Batch 192, Loss: 2.7861\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3298494815826416\n",
            "  Batch 193, Loss: 2.7837\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3168888092041016\n",
            "  Batch 194, Loss: 2.7813\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2655320167541504\n",
            "  Batch 195, Loss: 2.7787\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.564290761947632\n",
            "  Batch 196, Loss: 2.7776\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.479933500289917\n",
            "  Batch 197, Loss: 2.7761\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.490557909011841\n",
            "  Batch 198, Loss: 2.7746\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.456719160079956\n",
            "  Batch 199, Loss: 2.7730\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.433941602706909\n",
            "  Batch 200, Loss: 2.7713\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4877378940582275\n",
            "  Batch 201, Loss: 2.7699\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.350864887237549\n",
            "  Batch 202, Loss: 2.7678\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.484051465988159\n",
            "  Batch 203, Loss: 2.7665\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5282528400421143\n",
            "  Batch 204, Loss: 2.7653\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1301279067993164\n",
            "  Batch 205, Loss: 2.7622\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4349868297576904\n",
            "  Batch 206, Loss: 2.7606\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1927340030670166\n",
            "  Batch 207, Loss: 2.7579\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.35453724861145\n",
            "  Batch 208, Loss: 2.7559\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.307840347290039\n",
            "  Batch 209, Loss: 2.7538\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2566239833831787\n",
            "  Batch 210, Loss: 2.7514\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4692931175231934\n",
            "  Batch 211, Loss: 2.7501\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.12701153755188\n",
            "  Batch 212, Loss: 2.7471\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3697850704193115\n",
            "  Batch 213, Loss: 2.7454\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3996121883392334\n",
            "  Batch 214, Loss: 2.7437\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2811777591705322\n",
            "  Batch 215, Loss: 2.7416\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2788689136505127\n",
            "  Batch 216, Loss: 2.7394\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3272621631622314\n",
            "  Batch 217, Loss: 2.7375\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3601746559143066\n",
            "  Batch 218, Loss: 2.7358\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.248427629470825\n",
            "  Batch 219, Loss: 2.7336\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.199497699737549\n",
            "  Batch 220, Loss: 2.7312\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.251681327819824\n",
            "  Batch 221, Loss: 2.7290\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2388548851013184\n",
            "  Batch 222, Loss: 2.7268\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3324313163757324\n",
            "  Batch 223, Loss: 2.7250\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.13395357131958\n",
            "  Batch 224, Loss: 2.7224\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.38653564453125\n",
            "  Batch 225, Loss: 2.7209\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1693007946014404\n",
            "  Batch 226, Loss: 2.7184\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.229875326156616\n",
            "  Batch 227, Loss: 2.7163\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2804622650146484\n",
            "  Batch 228, Loss: 2.7144\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2248106002807617\n",
            "  Batch 229, Loss: 2.7122\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3074839115142822\n",
            "  Batch 230, Loss: 2.7105\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3592140674591064\n",
            "  Batch 231, Loss: 2.7090\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2232906818389893\n",
            "  Batch 232, Loss: 2.7069\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6192991733551025\n",
            "  Batch 233, Loss: 2.7065\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4175446033477783\n",
            "  Batch 234, Loss: 2.7053\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4084579944610596\n",
            "  Batch 235, Loss: 2.7040\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1960062980651855\n",
            "  Batch 236, Loss: 2.7018\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.203896999359131\n",
            "  Batch 237, Loss: 2.6997\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0656325817108154\n",
            "  Batch 238, Loss: 2.6971\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1706199645996094\n",
            "  Batch 239, Loss: 2.6949\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2979369163513184\n",
            "  Batch 240, Loss: 2.6932\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1885175704956055\n",
            "  Batch 241, Loss: 2.6911\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.200650453567505\n",
            "  Batch 242, Loss: 2.6891\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.346189498901367\n",
            "  Batch 243, Loss: 2.6877\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.311610698699951\n",
            "  Batch 244, Loss: 2.6861\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.408353328704834\n",
            "  Batch 245, Loss: 2.6850\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.158051013946533\n",
            "  Batch 246, Loss: 2.6829\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2422149181365967\n",
            "  Batch 247, Loss: 2.6811\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.254122495651245\n",
            "  Batch 248, Loss: 2.6794\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2642197608947754\n",
            "  Batch 249, Loss: 2.6777\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.173152446746826\n",
            "  Batch 250, Loss: 2.6757\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.237933397293091\n",
            "  Batch 251, Loss: 2.6739\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3189616203308105\n",
            "  Batch 252, Loss: 2.6725\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3037540912628174\n",
            "  Batch 253, Loss: 2.6711\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2476298809051514\n",
            "  Batch 254, Loss: 2.6694\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0489909648895264\n",
            "  Batch 255, Loss: 2.6670\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.425222396850586\n",
            "  Batch 256, Loss: 2.6660\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2544829845428467\n",
            "  Batch 257, Loss: 2.6644\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1387386322021484\n",
            "  Batch 258, Loss: 2.6624\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1993517875671387\n",
            "  Batch 259, Loss: 2.6606\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.056056499481201\n",
            "  Batch 260, Loss: 2.6583\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1627418994903564\n",
            "  Batch 261, Loss: 2.6564\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.154391288757324\n",
            "  Batch 262, Loss: 2.6545\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1445367336273193\n",
            "  Batch 263, Loss: 2.6525\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.07971453666687\n",
            "  Batch 264, Loss: 2.6504\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0108840465545654\n",
            "  Batch 265, Loss: 2.6479\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1906464099884033\n",
            "  Batch 266, Loss: 2.6462\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2208917140960693\n",
            "  Batch 267, Loss: 2.6446\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2324650287628174\n",
            "  Batch 268, Loss: 2.6431\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4731154441833496\n",
            "  Batch 269, Loss: 2.6425\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.137934446334839\n",
            "  Batch 270, Loss: 2.6406\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.228914260864258\n",
            "  Batch 271, Loss: 2.6391\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0542030334472656\n",
            "  Batch 272, Loss: 2.6369\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2120237350463867\n",
            "  Batch 273, Loss: 2.6354\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2680649757385254\n",
            "  Batch 274, Loss: 2.6340\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4136013984680176\n",
            "  Batch 275, Loss: 2.6332\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.242575168609619\n",
            "  Batch 276, Loss: 2.6318\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1933929920196533\n",
            "  Batch 277, Loss: 2.6302\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1413581371307373\n",
            "  Batch 278, Loss: 2.6285\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.231407403945923\n",
            "  Batch 279, Loss: 2.6270\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.14628005027771\n",
            "  Batch 280, Loss: 2.6253\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.047959327697754\n",
            "  Batch 281, Loss: 2.6233\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.188114643096924\n",
            "  Batch 282, Loss: 2.6217\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0253686904907227\n",
            "  Batch 283, Loss: 2.6196\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9480265378952026\n",
            "  Batch 284, Loss: 2.6173\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1473002433776855\n",
            "  Batch 285, Loss: 2.6156\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0149385929107666\n",
            "  Batch 286, Loss: 2.6135\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1182379722595215\n",
            "  Batch 287, Loss: 2.6118\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.212449550628662\n",
            "  Batch 288, Loss: 2.6104\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0760719776153564\n",
            "  Batch 289, Loss: 2.6085\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9990477561950684\n",
            "  Batch 290, Loss: 2.6064\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.033989191055298\n",
            "  Batch 291, Loss: 2.6045\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.031289577484131\n",
            "  Batch 292, Loss: 2.6025\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9749150276184082\n",
            "  Batch 293, Loss: 2.6004\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0093348026275635\n",
            "  Batch 294, Loss: 2.5984\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.275794744491577\n",
            "  Batch 295, Loss: 2.5973\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.222115993499756\n",
            "  Batch 296, Loss: 2.5960\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9848332405090332\n",
            "  Batch 297, Loss: 2.5939\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.21543025970459\n",
            "  Batch 298, Loss: 2.5927\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1008505821228027\n",
            "  Batch 299, Loss: 2.5910\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.058239221572876\n",
            "  Batch 300, Loss: 2.5892\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.113203525543213\n",
            "  Batch 301, Loss: 2.5877\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.13614559173584\n",
            "  Batch 302, Loss: 2.5862\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1343562602996826\n",
            "  Batch 303, Loss: 2.5847\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.225938320159912\n",
            "  Batch 304, Loss: 2.5835\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.17535400390625\n",
            "  Batch 305, Loss: 2.5822\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.21905255317688\n",
            "  Batch 306, Loss: 2.5810\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0675249099731445\n",
            "  Batch 307, Loss: 2.5793\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2084505558013916\n",
            "  Batch 308, Loss: 2.5781\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9592136144638062\n",
            "  Batch 309, Loss: 2.5761\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0918643474578857\n",
            "  Batch 310, Loss: 2.5745\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.062246799468994\n",
            "  Batch 311, Loss: 2.5729\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0644311904907227\n",
            "  Batch 312, Loss: 2.5713\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.075766086578369\n",
            "  Batch 313, Loss: 2.5697\n",
            "Epoch 1 Loss: 2.5697\n",
            "skipped:  0\n",
            "model finished training\n",
            "model saved\n",
            "lossed saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VlBA8egHHIWK"
      },
      "id": "VlBA8egHHIWK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20K Model"
      ],
      "metadata": {
        "id": "q_WsYvRLI4-U"
      },
      "id": "q_WsYvRLI4-U"
    },
    {
      "cell_type": "code",
      "source": [
        "m2m_data_path_20k = '/content/drive/MyDrive/W266 Final Project/m2m_translated_data/m2m_20k_dataset.csv'\n",
        "\n",
        "m2m_trained_model_20k, m2m_losses_20k = train_pipeline(m2m_data_path_20k, train_model, tokenizer, 'm2m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIGUqIdUHIT6",
        "outputId": "a0a78280-459b-4dc4-85c4-8b12046c5fd5"
      },
      "id": "DIGUqIdUHIT6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20k\n",
            "data loaded\n",
            "on gpu\n",
            "Epoch 1/1\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  6.4046502113342285\n",
            "  Batch 1, Loss: 6.4047\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  9.595268249511719\n",
            "  Batch 2, Loss: 8.0000\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  10.023859024047852\n",
            "  Batch 3, Loss: 8.6746\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  11.616325378417969\n",
            "  Batch 4, Loss: 9.4100\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  9.72398853302002\n",
            "  Batch 5, Loss: 9.4728\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  9.242072105407715\n",
            "  Batch 6, Loss: 9.4344\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  8.703378677368164\n",
            "  Batch 7, Loss: 9.3299\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  8.358681678771973\n",
            "  Batch 8, Loss: 9.2085\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  8.09681224822998\n",
            "  Batch 9, Loss: 9.0850\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  7.6319169998168945\n",
            "  Batch 10, Loss: 8.9397\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  7.0378570556640625\n",
            "  Batch 11, Loss: 8.7668\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  6.9097371101379395\n",
            "  Batch 12, Loss: 8.6120\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  6.380349159240723\n",
            "  Batch 13, Loss: 8.4404\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  6.070838928222656\n",
            "  Batch 14, Loss: 8.2711\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.724551677703857\n",
            "  Batch 15, Loss: 8.1014\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.46461820602417\n",
            "  Batch 16, Loss: 7.9366\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.379406929016113\n",
            "  Batch 17, Loss: 7.7861\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.464937210083008\n",
            "  Batch 18, Loss: 7.6572\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.585623264312744\n",
            "  Batch 19, Loss: 7.5482\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.440770149230957\n",
            "  Batch 20, Loss: 7.4428\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.258016109466553\n",
            "  Batch 21, Loss: 7.3387\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.582275390625\n",
            "  Batch 22, Loss: 7.2589\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.180779933929443\n",
            "  Batch 23, Loss: 7.1686\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.312193870544434\n",
            "  Batch 24, Loss: 7.0912\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.062458515167236\n",
            "  Batch 25, Loss: 7.0101\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.053535461425781\n",
            "  Batch 26, Loss: 6.9348\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.100235462188721\n",
            "  Batch 27, Loss: 6.8669\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.962044715881348\n",
            "  Batch 28, Loss: 6.7988\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.03378438949585\n",
            "  Batch 29, Loss: 6.7380\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.846537113189697\n",
            "  Batch 30, Loss: 6.6749\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.040963172912598\n",
            "  Batch 31, Loss: 6.6222\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.809738636016846\n",
            "  Batch 32, Loss: 6.5656\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.947210311889648\n",
            "  Batch 33, Loss: 6.5165\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.87978458404541\n",
            "  Batch 34, Loss: 6.4684\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.148747444152832\n",
            "  Batch 35, Loss: 6.4307\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.419114112854004\n",
            "  Batch 36, Loss: 6.3748\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.469476699829102\n",
            "  Batch 37, Loss: 6.3233\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.364191055297852\n",
            "  Batch 38, Loss: 6.2718\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.550503730773926\n",
            "  Batch 39, Loss: 6.2276\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.344540596008301\n",
            "  Batch 40, Loss: 6.1805\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.754881381988525\n",
            "  Batch 41, Loss: 6.1458\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.112981796264648\n",
            "  Batch 42, Loss: 6.0974\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.368317127227783\n",
            "  Batch 43, Loss: 6.0572\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.294436454772949\n",
            "  Batch 44, Loss: 6.0171\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.196964740753174\n",
            "  Batch 45, Loss: 5.9767\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.9222609996795654\n",
            "  Batch 46, Loss: 5.9320\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.1183695793151855\n",
            "  Batch 47, Loss: 5.8934\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.0063276290893555\n",
            "  Batch 48, Loss: 5.8541\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.884547233581543\n",
            "  Batch 49, Loss: 5.8139\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.721893548965454\n",
            "  Batch 50, Loss: 5.7721\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.5876269340515137\n",
            "  Batch 51, Loss: 5.7292\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.5744996070861816\n",
            "  Batch 52, Loss: 5.6878\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.575014352798462\n",
            "  Batch 53, Loss: 5.6479\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.5762994289398193\n",
            "  Batch 54, Loss: 5.6096\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.374770402908325\n",
            "  Batch 55, Loss: 5.5689\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0835585594177246\n",
            "  Batch 56, Loss: 5.5245\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.2723848819732666\n",
            "  Batch 57, Loss: 5.4850\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.2584002017974854\n",
            "  Batch 58, Loss: 5.4466\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.2804362773895264\n",
            "  Batch 59, Loss: 5.4099\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.2759287357330322\n",
            "  Batch 60, Loss: 5.3744\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.286748170852661\n",
            "  Batch 61, Loss: 5.3401\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.158910036087036\n",
            "  Batch 62, Loss: 5.3050\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0675454139709473\n",
            "  Batch 63, Loss: 5.2694\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7913978099823\n",
            "  Batch 64, Loss: 5.2307\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.902015447616577\n",
            "  Batch 65, Loss: 5.1949\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.107602596282959\n",
            "  Batch 66, Loss: 5.1633\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.1257972717285156\n",
            "  Batch 67, Loss: 5.1329\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.796649217605591\n",
            "  Batch 68, Loss: 5.0985\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9848766326904297\n",
            "  Batch 69, Loss: 5.0679\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.893019676208496\n",
            "  Batch 70, Loss: 5.0368\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8415517807006836\n",
            "  Batch 71, Loss: 5.0059\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8181967735290527\n",
            "  Batch 72, Loss: 4.9755\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.747781276702881\n",
            "  Batch 73, Loss: 4.9450\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.803148031234741\n",
            "  Batch 74, Loss: 4.9160\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9317572116851807\n",
            "  Batch 75, Loss: 4.8896\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7676117420196533\n",
            "  Batch 76, Loss: 4.8617\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7191386222839355\n",
            "  Batch 77, Loss: 4.8338\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9330995082855225\n",
            "  Batch 78, Loss: 4.8095\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.919584274291992\n",
            "  Batch 79, Loss: 4.7855\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8533811569213867\n",
            "  Batch 80, Loss: 4.7614\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9093618392944336\n",
            "  Batch 81, Loss: 4.7385\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.663649559020996\n",
            "  Batch 82, Loss: 4.7132\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7114672660827637\n",
            "  Batch 83, Loss: 4.6891\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9675962924957275\n",
            "  Batch 84, Loss: 4.6686\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7740890979766846\n",
            "  Batch 85, Loss: 4.6463\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.619624137878418\n",
            "  Batch 86, Loss: 4.6228\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.732740640640259\n",
            "  Batch 87, Loss: 4.6010\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0055408477783203\n",
            "  Batch 88, Loss: 4.5829\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.637375593185425\n",
            "  Batch 89, Loss: 4.5610\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.949126720428467\n",
            "  Batch 90, Loss: 4.5431\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.713710069656372\n",
            "  Batch 91, Loss: 4.5230\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.685685157775879\n",
            "  Batch 92, Loss: 4.5031\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5955588817596436\n",
            "  Batch 93, Loss: 4.4825\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8618392944335938\n",
            "  Batch 94, Loss: 4.4653\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.627894878387451\n",
            "  Batch 95, Loss: 4.4460\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5501949787139893\n",
            "  Batch 96, Loss: 4.4262\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6184611320495605\n",
            "  Batch 97, Loss: 4.4076\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7601587772369385\n",
            "  Batch 98, Loss: 4.3908\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.762831449508667\n",
            "  Batch 99, Loss: 4.3743\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3877503871917725\n",
            "  Batch 100, Loss: 4.3545\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.429100275039673\n",
            "  Batch 101, Loss: 4.3354\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3478822708129883\n",
            "  Batch 102, Loss: 4.3159\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4483189582824707\n",
            "  Batch 103, Loss: 4.2978\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4790499210357666\n",
            "  Batch 104, Loss: 4.2803\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4790525436401367\n",
            "  Batch 105, Loss: 4.2631\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4304535388946533\n",
            "  Batch 106, Loss: 4.2458\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5053281784057617\n",
            "  Batch 107, Loss: 4.2296\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.545116901397705\n",
            "  Batch 108, Loss: 4.2140\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7146353721618652\n",
            "  Batch 109, Loss: 4.2002\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.547638177871704\n",
            "  Batch 110, Loss: 4.1852\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5581490993499756\n",
            "  Batch 111, Loss: 4.1705\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4644997119903564\n",
            "  Batch 112, Loss: 4.1553\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.446730613708496\n",
            "  Batch 113, Loss: 4.1402\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.393815279006958\n",
            "  Batch 114, Loss: 4.1249\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4537293910980225\n",
            "  Batch 115, Loss: 4.1103\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5906989574432373\n",
            "  Batch 116, Loss: 4.0972\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4036154747009277\n",
            "  Batch 117, Loss: 4.0828\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.385556221008301\n",
            "  Batch 118, Loss: 4.0684\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2589480876922607\n",
            "  Batch 119, Loss: 4.0532\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4010727405548096\n",
            "  Batch 120, Loss: 4.0394\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5812270641326904\n",
            "  Batch 121, Loss: 4.0274\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3074023723602295\n",
            "  Batch 122, Loss: 4.0133\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3617660999298096\n",
            "  Batch 123, Loss: 3.9998\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2738678455352783\n",
            "  Batch 124, Loss: 3.9859\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.171182632446289\n",
            "  Batch 125, Loss: 3.9714\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.14273738861084\n",
            "  Batch 126, Loss: 3.9569\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.493009328842163\n",
            "  Batch 127, Loss: 3.9454\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.45112943649292\n",
            "  Batch 128, Loss: 3.9337\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.380891799926758\n",
            "  Batch 129, Loss: 3.9216\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.454085350036621\n",
            "  Batch 130, Loss: 3.9104\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3392703533172607\n",
            "  Batch 131, Loss: 3.8984\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.481536626815796\n",
            "  Batch 132, Loss: 3.8876\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.411668300628662\n",
            "  Batch 133, Loss: 3.8765\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1301944255828857\n",
            "  Batch 134, Loss: 3.8635\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2294058799743652\n",
            "  Batch 135, Loss: 3.8514\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3702144622802734\n",
            "  Batch 136, Loss: 3.8405\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.225623846054077\n",
            "  Batch 137, Loss: 3.8287\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1908347606658936\n",
            "  Batch 138, Loss: 3.8169\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2723467350006104\n",
            "  Batch 139, Loss: 3.8057\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2457075119018555\n",
            "  Batch 140, Loss: 3.7946\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.318247079849243\n",
            "  Batch 141, Loss: 3.7841\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.313025951385498\n",
            "  Batch 142, Loss: 3.7738\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.125070810317993\n",
            "  Batch 143, Loss: 3.7622\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0853567123413086\n",
            "  Batch 144, Loss: 3.7506\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3292202949523926\n",
            "  Batch 145, Loss: 3.7408\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0701406002044678\n",
            "  Batch 146, Loss: 3.7293\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0455236434936523\n",
            "  Batch 147, Loss: 3.7179\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.115816593170166\n",
            "  Batch 148, Loss: 3.7071\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1612389087677\n",
            "  Batch 149, Loss: 3.6967\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.937113642692566\n",
            "  Batch 150, Loss: 3.6850\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0148110389709473\n",
            "  Batch 151, Loss: 3.6739\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9852761030197144\n",
            "  Batch 152, Loss: 3.6628\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9877369403839111\n",
            "  Batch 153, Loss: 3.6518\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.053234815597534\n",
            "  Batch 154, Loss: 3.6415\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3018014430999756\n",
            "  Batch 155, Loss: 3.6328\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.322810411453247\n",
            "  Batch 156, Loss: 3.6244\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3443961143493652\n",
            "  Batch 157, Loss: 3.6163\n",
            "Epoch 1 Loss: 3.6163\n",
            "skipped:  0\n",
            "model finished training\n",
            "model saved\n",
            "lossed saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 50K Model"
      ],
      "metadata": {
        "id": "aTHoNhQkI6kS"
      },
      "id": "aTHoNhQkI6kS"
    },
    {
      "cell_type": "code",
      "source": [
        "m2m_data_path_50k = '/content/drive/MyDrive/W266 Final Project/m2m_translated_data/m2m_50k_dataset.csv'\n",
        "\n",
        "m2m_trained_model_50k, m2m_losses_50k = train_pipeline(m2m_data_path_50k, train_model, tokenizer, 'm2m')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXLCVUA0I-OE",
        "outputId": "e1613fc2-62cb-4020-8514-8e7458282e16"
      },
      "id": "wXLCVUA0I-OE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50k\n",
            "data loaded\n",
            "on gpu\n",
            "Epoch 1/1\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  6.138611316680908\n",
            "  Batch 1, Loss: 6.1386\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  9.720385551452637\n",
            "  Batch 2, Loss: 7.9295\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  7.416428089141846\n",
            "  Batch 3, Loss: 7.7585\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  6.662507057189941\n",
            "  Batch 4, Loss: 7.4845\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.371593952178955\n",
            "  Batch 5, Loss: 7.0619\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.644304275512695\n",
            "  Batch 6, Loss: 6.6590\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.310196399688721\n",
            "  Batch 7, Loss: 6.3234\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.032688617706299\n",
            "  Batch 8, Loss: 6.0371\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.739102363586426\n",
            "  Batch 9, Loss: 5.7818\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.831195116043091\n",
            "  Batch 10, Loss: 5.5867\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.574225664138794\n",
            "  Batch 11, Loss: 5.4037\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.5088138580322266\n",
            "  Batch 12, Loss: 5.2458\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.710557699203491\n",
            "  Batch 13, Loss: 5.1277\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.636587381362915\n",
            "  Batch 14, Loss: 5.0212\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.195281505584717\n",
            "  Batch 15, Loss: 4.8995\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.4333462715148926\n",
            "  Batch 16, Loss: 4.8079\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.314199447631836\n",
            "  Batch 17, Loss: 4.7200\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.368147134780884\n",
            "  Batch 18, Loss: 4.6449\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.2965469360351562\n",
            "  Batch 19, Loss: 4.5739\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.149710178375244\n",
            "  Batch 20, Loss: 4.5027\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.3145546913146973\n",
            "  Batch 21, Loss: 4.4461\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.165741443634033\n",
            "  Batch 22, Loss: 4.3879\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8418288230895996\n",
            "  Batch 23, Loss: 4.3207\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0976195335388184\n",
            "  Batch 24, Loss: 4.2698\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0586066246032715\n",
            "  Batch 25, Loss: 4.2213\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9870917797088623\n",
            "  Batch 26, Loss: 4.1738\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.083493709564209\n",
            "  Batch 27, Loss: 4.1335\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.948648452758789\n",
            "  Batch 28, Loss: 4.0911\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0108141899108887\n",
            "  Batch 29, Loss: 4.0539\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.042248487472534\n",
            "  Batch 30, Loss: 4.0202\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.154135227203369\n",
            "  Batch 31, Loss: 3.9922\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.140336513519287\n",
            "  Batch 32, Loss: 3.9656\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.1613569259643555\n",
            "  Batch 33, Loss: 3.9412\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9775192737579346\n",
            "  Batch 34, Loss: 3.9129\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.097940444946289\n",
            "  Batch 35, Loss: 3.8896\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.093374729156494\n",
            "  Batch 36, Loss: 3.8675\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9498541355133057\n",
            "  Batch 37, Loss: 3.8427\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.259098768234253\n",
            "  Batch 38, Loss: 3.8273\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.920477867126465\n",
            "  Batch 39, Loss: 3.8041\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0084872245788574\n",
            "  Batch 40, Loss: 3.7842\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.874894857406616\n",
            "  Batch 41, Loss: 3.7620\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.091315269470215\n",
            "  Batch 42, Loss: 3.7460\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.1707732677459717\n",
            "  Batch 43, Loss: 3.7327\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0013375282287598\n",
            "  Batch 44, Loss: 3.7160\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.112006664276123\n",
            "  Batch 45, Loss: 3.7026\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.1622724533081055\n",
            "  Batch 46, Loss: 3.6909\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7946648597717285\n",
            "  Batch 47, Loss: 3.6718\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.91473126411438\n",
            "  Batch 48, Loss: 3.6560\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.742232322692871\n",
            "  Batch 49, Loss: 3.6374\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.978182315826416\n",
            "  Batch 50, Loss: 3.6242\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9224300384521484\n",
            "  Batch 51, Loss: 3.6104\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9707469940185547\n",
            "  Batch 52, Loss: 3.5981\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.740675926208496\n",
            "  Batch 53, Loss: 3.5820\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6949243545532227\n",
            "  Batch 54, Loss: 3.5655\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.811668872833252\n",
            "  Batch 55, Loss: 3.5518\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0468392372131348\n",
            "  Batch 56, Loss: 3.5428\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9416799545288086\n",
            "  Batch 57, Loss: 3.5323\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.861131429672241\n",
            "  Batch 58, Loss: 3.5207\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.637599229812622\n",
            "  Batch 59, Loss: 3.5057\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9686148166656494\n",
            "  Batch 60, Loss: 3.4968\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0146424770355225\n",
            "  Batch 61, Loss: 3.4889\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8315494060516357\n",
            "  Batch 62, Loss: 3.4783\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.884377956390381\n",
            "  Batch 63, Loss: 3.4688\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.830625295639038\n",
            "  Batch 64, Loss: 3.4589\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7706313133239746\n",
            "  Batch 65, Loss: 3.4483\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9230639934539795\n",
            "  Batch 66, Loss: 3.4403\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7609987258911133\n",
            "  Batch 67, Loss: 3.4302\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8220632076263428\n",
            "  Batch 68, Loss: 3.4212\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9084131717681885\n",
            "  Batch 69, Loss: 3.4138\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.634901523590088\n",
            "  Batch 70, Loss: 3.4027\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9485414028167725\n",
            "  Batch 71, Loss: 3.3963\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8430962562561035\n",
            "  Batch 72, Loss: 3.3886\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0533947944641113\n",
            "  Batch 73, Loss: 3.3840\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.880016565322876\n",
            "  Batch 74, Loss: 3.3772\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.086989164352417\n",
            "  Batch 75, Loss: 3.3733\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.868910789489746\n",
            "  Batch 76, Loss: 3.3667\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.851055145263672\n",
            "  Batch 77, Loss: 3.3600\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6801793575286865\n",
            "  Batch 78, Loss: 3.3513\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0901641845703125\n",
            "  Batch 79, Loss: 3.3480\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.911266803741455\n",
            "  Batch 80, Loss: 3.3425\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6848108768463135\n",
            "  Batch 81, Loss: 3.3344\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7034363746643066\n",
            "  Batch 82, Loss: 3.3267\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.84425687789917\n",
            "  Batch 83, Loss: 3.3209\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9922404289245605\n",
            "  Batch 84, Loss: 3.3170\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8087575435638428\n",
            "  Batch 85, Loss: 3.3110\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8974344730377197\n",
            "  Batch 86, Loss: 3.3062\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7522995471954346\n",
            "  Batch 87, Loss: 3.2998\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.737117052078247\n",
            "  Batch 88, Loss: 3.2934\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7148337364196777\n",
            "  Batch 89, Loss: 3.2869\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7206897735595703\n",
            "  Batch 90, Loss: 3.2806\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.672670602798462\n",
            "  Batch 91, Loss: 3.2740\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.1081690788269043\n",
            "  Batch 92, Loss: 3.2722\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8095102310180664\n",
            "  Batch 93, Loss: 3.2672\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6875646114349365\n",
            "  Batch 94, Loss: 3.2610\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8167457580566406\n",
            "  Batch 95, Loss: 3.2563\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7095515727996826\n",
            "  Batch 96, Loss: 3.2506\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7834579944610596\n",
            "  Batch 97, Loss: 3.2458\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.836195707321167\n",
            "  Batch 98, Loss: 3.2416\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.625067949295044\n",
            "  Batch 99, Loss: 3.2354\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.957918405532837\n",
            "  Batch 100, Loss: 3.2326\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.563255548477173\n",
            "  Batch 101, Loss: 3.2260\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.830467700958252\n",
            "  Batch 102, Loss: 3.2221\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8906772136688232\n",
            "  Batch 103, Loss: 3.2189\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6305019855499268\n",
            "  Batch 104, Loss: 3.2133\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.696312189102173\n",
            "  Batch 105, Loss: 3.2083\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9013845920562744\n",
            "  Batch 106, Loss: 3.2054\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.835333824157715\n",
            "  Batch 107, Loss: 3.2020\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6345889568328857\n",
            "  Batch 108, Loss: 3.1967\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7856638431549072\n",
            "  Batch 109, Loss: 3.1930\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7620391845703125\n",
            "  Batch 110, Loss: 3.1890\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.964251756668091\n",
            "  Batch 111, Loss: 3.1870\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0573697090148926\n",
            "  Batch 112, Loss: 3.1859\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7136268615722656\n",
            "  Batch 113, Loss: 3.1817\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.075136184692383\n",
            "  Batch 114, Loss: 3.1807\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.884661912918091\n",
            "  Batch 115, Loss: 3.1782\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.780113458633423\n",
            "  Batch 116, Loss: 3.1747\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9077088832855225\n",
            "  Batch 117, Loss: 3.1725\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5700955390930176\n",
            "  Batch 118, Loss: 3.1673\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.773752212524414\n",
            "  Batch 119, Loss: 3.1640\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.825493574142456\n",
            "  Batch 120, Loss: 3.1612\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.822883367538452\n",
            "  Batch 121, Loss: 3.1584\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7845849990844727\n",
            "  Batch 122, Loss: 3.1554\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6979780197143555\n",
            "  Batch 123, Loss: 3.1516\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6991167068481445\n",
            "  Batch 124, Loss: 3.1480\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.790875196456909\n",
            "  Batch 125, Loss: 3.1451\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8044466972351074\n",
            "  Batch 126, Loss: 3.1424\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.788027286529541\n",
            "  Batch 127, Loss: 3.1396\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.688628673553467\n",
            "  Batch 128, Loss: 3.1361\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6876962184906006\n",
            "  Batch 129, Loss: 3.1326\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7090818881988525\n",
            "  Batch 130, Loss: 3.1294\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8174235820770264\n",
            "  Batch 131, Loss: 3.1270\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9928200244903564\n",
            "  Batch 132, Loss: 3.1260\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.718238115310669\n",
            "  Batch 133, Loss: 3.1229\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7151029109954834\n",
            "  Batch 134, Loss: 3.1199\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.534102439880371\n",
            "  Batch 135, Loss: 3.1155\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.860323190689087\n",
            "  Batch 136, Loss: 3.1137\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.960801839828491\n",
            "  Batch 137, Loss: 3.1125\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.663604259490967\n",
            "  Batch 138, Loss: 3.1093\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.576721668243408\n",
            "  Batch 139, Loss: 3.1055\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.869439125061035\n",
            "  Batch 140, Loss: 3.1038\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3715107440948486\n",
            "  Batch 141, Loss: 3.0986\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7546591758728027\n",
            "  Batch 142, Loss: 3.0962\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.622215747833252\n",
            "  Batch 143, Loss: 3.0928\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7353501319885254\n",
            "  Batch 144, Loss: 3.0904\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.810925006866455\n",
            "  Batch 145, Loss: 3.0884\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.635833263397217\n",
            "  Batch 146, Loss: 3.0853\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7640864849090576\n",
            "  Batch 147, Loss: 3.0831\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8197598457336426\n",
            "  Batch 148, Loss: 3.0814\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.65339732170105\n",
            "  Batch 149, Loss: 3.0785\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.87054181098938\n",
            "  Batch 150, Loss: 3.0771\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9000084400177\n",
            "  Batch 151, Loss: 3.0759\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5464305877685547\n",
            "  Batch 152, Loss: 3.0725\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7163472175598145\n",
            "  Batch 153, Loss: 3.0701\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6243903636932373\n",
            "  Batch 154, Loss: 3.0672\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.860626459121704\n",
            "  Batch 155, Loss: 3.0659\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5901808738708496\n",
            "  Batch 156, Loss: 3.0628\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5590527057647705\n",
            "  Batch 157, Loss: 3.0596\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4906742572784424\n",
            "  Batch 158, Loss: 3.0560\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7720794677734375\n",
            "  Batch 159, Loss: 3.0543\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6721208095550537\n",
            "  Batch 160, Loss: 3.0519\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.786510467529297\n",
            "  Batch 161, Loss: 3.0502\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7382748126983643\n",
            "  Batch 162, Loss: 3.0483\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6225202083587646\n",
            "  Batch 163, Loss: 3.0457\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.641479969024658\n",
            "  Batch 164, Loss: 3.0432\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.996220588684082\n",
            "  Batch 165, Loss: 3.0429\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7632627487182617\n",
            "  Batch 166, Loss: 3.0412\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.685342788696289\n",
            "  Batch 167, Loss: 3.0391\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7394285202026367\n",
            "  Batch 168, Loss: 3.0373\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.672461748123169\n",
            "  Batch 169, Loss: 3.0352\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.578453540802002\n",
            "  Batch 170, Loss: 3.0325\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.852538824081421\n",
            "  Batch 171, Loss: 3.0314\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6993110179901123\n",
            "  Batch 172, Loss: 3.0295\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7077207565307617\n",
            "  Batch 173, Loss: 3.0276\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.801348924636841\n",
            "  Batch 174, Loss: 3.0263\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.904231071472168\n",
            "  Batch 175, Loss: 3.0256\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3920834064483643\n",
            "  Batch 176, Loss: 3.0220\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.523738145828247\n",
            "  Batch 177, Loss: 3.0192\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.406883716583252\n",
            "  Batch 178, Loss: 3.0158\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6866612434387207\n",
            "  Batch 179, Loss: 3.0139\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6302361488342285\n",
            "  Batch 180, Loss: 3.0118\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.663787603378296\n",
            "  Batch 181, Loss: 3.0099\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.623823881149292\n",
            "  Batch 182, Loss: 3.0078\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6020615100860596\n",
            "  Batch 183, Loss: 3.0056\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.651294708251953\n",
            "  Batch 184, Loss: 3.0036\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.762003183364868\n",
            "  Batch 185, Loss: 3.0023\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.581895351409912\n",
            "  Batch 186, Loss: 3.0001\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.752300500869751\n",
            "  Batch 187, Loss: 2.9987\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5778071880340576\n",
            "  Batch 188, Loss: 2.9965\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.772310495376587\n",
            "  Batch 189, Loss: 2.9953\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8263206481933594\n",
            "  Batch 190, Loss: 2.9944\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6178641319274902\n",
            "  Batch 191, Loss: 2.9925\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7400903701782227\n",
            "  Batch 192, Loss: 2.9911\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5033931732177734\n",
            "  Batch 193, Loss: 2.9886\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7767493724823\n",
            "  Batch 194, Loss: 2.9875\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6876604557037354\n",
            "  Batch 195, Loss: 2.9860\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.502817153930664\n",
            "  Batch 196, Loss: 2.9835\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6718194484710693\n",
            "  Batch 197, Loss: 2.9819\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6023595333099365\n",
            "  Batch 198, Loss: 2.9800\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7205235958099365\n",
            "  Batch 199, Loss: 2.9787\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5275022983551025\n",
            "  Batch 200, Loss: 2.9765\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5458855628967285\n",
            "  Batch 201, Loss: 2.9743\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6435017585754395\n",
            "  Batch 202, Loss: 2.9727\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.809573173522949\n",
            "  Batch 203, Loss: 2.9719\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5966744422912598\n",
            "  Batch 204, Loss: 2.9700\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7563066482543945\n",
            "  Batch 205, Loss: 2.9690\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.849942207336426\n",
            "  Batch 206, Loss: 2.9684\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6379356384277344\n",
            "  Batch 207, Loss: 2.9668\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5797653198242188\n",
            "  Batch 208, Loss: 2.9650\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9466969966888428\n",
            "  Batch 209, Loss: 2.9649\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.549898624420166\n",
            "  Batch 210, Loss: 2.9629\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5731937885284424\n",
            "  Batch 211, Loss: 2.9610\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5318565368652344\n",
            "  Batch 212, Loss: 2.9590\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6011624336242676\n",
            "  Batch 213, Loss: 2.9573\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6750781536102295\n",
            "  Batch 214, Loss: 2.9560\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5091495513916016\n",
            "  Batch 215, Loss: 2.9539\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.808687448501587\n",
            "  Batch 216, Loss: 2.9533\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.658212423324585\n",
            "  Batch 217, Loss: 2.9519\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.554783344268799\n",
            "  Batch 218, Loss: 2.9501\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.63454532623291\n",
            "  Batch 219, Loss: 2.9487\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7665209770202637\n",
            "  Batch 220, Loss: 2.9478\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4926059246063232\n",
            "  Batch 221, Loss: 2.9458\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6490590572357178\n",
            "  Batch 222, Loss: 2.9444\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.929945468902588\n",
            "  Batch 223, Loss: 2.9444\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7051098346710205\n",
            "  Batch 224, Loss: 2.9433\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6498162746429443\n",
            "  Batch 225, Loss: 2.9420\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.586576461791992\n",
            "  Batch 226, Loss: 2.9404\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.531280517578125\n",
            "  Batch 227, Loss: 2.9386\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6981189250946045\n",
            "  Batch 228, Loss: 2.9376\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4513230323791504\n",
            "  Batch 229, Loss: 2.9354\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.664334535598755\n",
            "  Batch 230, Loss: 2.9343\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0272867679595947\n",
            "  Batch 231, Loss: 2.9347\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.447645425796509\n",
            "  Batch 232, Loss: 2.9326\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.563544750213623\n",
            "  Batch 233, Loss: 2.9310\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6270530223846436\n",
            "  Batch 234, Loss: 2.9297\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4598870277404785\n",
            "  Batch 235, Loss: 2.9277\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6460790634155273\n",
            "  Batch 236, Loss: 2.9265\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.456634759902954\n",
            "  Batch 237, Loss: 2.9245\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6579184532165527\n",
            "  Batch 238, Loss: 2.9234\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8292415142059326\n",
            "  Batch 239, Loss: 2.9230\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6257481575012207\n",
            "  Batch 240, Loss: 2.9218\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6672580242156982\n",
            "  Batch 241, Loss: 2.9207\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6931166648864746\n",
            "  Batch 242, Loss: 2.9198\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7322957515716553\n",
            "  Batch 243, Loss: 2.9190\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7722413539886475\n",
            "  Batch 244, Loss: 2.9184\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.603109836578369\n",
            "  Batch 245, Loss: 2.9171\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6492819786071777\n",
            "  Batch 246, Loss: 2.9160\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7285971641540527\n",
            "  Batch 247, Loss: 2.9152\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5756888389587402\n",
            "  Batch 248, Loss: 2.9139\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5600576400756836\n",
            "  Batch 249, Loss: 2.9125\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.457278251647949\n",
            "  Batch 250, Loss: 2.9106\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.661858081817627\n",
            "  Batch 251, Loss: 2.9096\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5794315338134766\n",
            "  Batch 252, Loss: 2.9083\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3804233074188232\n",
            "  Batch 253, Loss: 2.9062\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.650052547454834\n",
            "  Batch 254, Loss: 2.9052\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6193883419036865\n",
            "  Batch 255, Loss: 2.9041\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4908785820007324\n",
            "  Batch 256, Loss: 2.9025\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.539170980453491\n",
            "  Batch 257, Loss: 2.9011\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.690986394882202\n",
            "  Batch 258, Loss: 2.9003\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5044074058532715\n",
            "  Batch 259, Loss: 2.8987\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3988864421844482\n",
            "  Batch 260, Loss: 2.8968\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.645565986633301\n",
            "  Batch 261, Loss: 2.8959\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5604400634765625\n",
            "  Batch 262, Loss: 2.8946\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6437978744506836\n",
            "  Batch 263, Loss: 2.8936\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5632567405700684\n",
            "  Batch 264, Loss: 2.8924\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.694728374481201\n",
            "  Batch 265, Loss: 2.8916\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.603830337524414\n",
            "  Batch 266, Loss: 2.8905\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.528449296951294\n",
            "  Batch 267, Loss: 2.8892\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.597745656967163\n",
            "  Batch 268, Loss: 2.8881\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5865731239318848\n",
            "  Batch 269, Loss: 2.8870\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.911064863204956\n",
            "  Batch 270, Loss: 2.8871\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.543736219406128\n",
            "  Batch 271, Loss: 2.8858\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5610246658325195\n",
            "  Batch 272, Loss: 2.8846\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5369884967803955\n",
            "  Batch 273, Loss: 2.8833\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5448246002197266\n",
            "  Batch 274, Loss: 2.8821\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3522491455078125\n",
            "  Batch 275, Loss: 2.8802\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6180355548858643\n",
            "  Batch 276, Loss: 2.8792\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.410085916519165\n",
            "  Batch 277, Loss: 2.8775\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5464141368865967\n",
            "  Batch 278, Loss: 2.8763\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.800835609436035\n",
            "  Batch 279, Loss: 2.8761\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7482030391693115\n",
            "  Batch 280, Loss: 2.8756\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.434593439102173\n",
            "  Batch 281, Loss: 2.8740\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4341511726379395\n",
            "  Batch 282, Loss: 2.8725\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4929933547973633\n",
            "  Batch 283, Loss: 2.8711\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.407663106918335\n",
            "  Batch 284, Loss: 2.8695\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6863787174224854\n",
            "  Batch 285, Loss: 2.8689\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.34033203125\n",
            "  Batch 286, Loss: 2.8670\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4947681427001953\n",
            "  Batch 287, Loss: 2.8657\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.883171558380127\n",
            "  Batch 288, Loss: 2.8658\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.505603551864624\n",
            "  Batch 289, Loss: 2.8645\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4392032623291016\n",
            "  Batch 290, Loss: 2.8631\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.62555193901062\n",
            "  Batch 291, Loss: 2.8623\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7106051445007324\n",
            "  Batch 292, Loss: 2.8617\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7344298362731934\n",
            "  Batch 293, Loss: 2.8613\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.587056875228882\n",
            "  Batch 294, Loss: 2.8604\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.650848388671875\n",
            "  Batch 295, Loss: 2.8597\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.693857431411743\n",
            "  Batch 296, Loss: 2.8591\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3748281002044678\n",
            "  Batch 297, Loss: 2.8575\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5462820529937744\n",
            "  Batch 298, Loss: 2.8564\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.625162363052368\n",
            "  Batch 299, Loss: 2.8557\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.472309112548828\n",
            "  Batch 300, Loss: 2.8544\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.543069362640381\n",
            "  Batch 301, Loss: 2.8533\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.358781337738037\n",
            "  Batch 302, Loss: 2.8517\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.74360990524292\n",
            "  Batch 303, Loss: 2.8513\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.487761974334717\n",
            "  Batch 304, Loss: 2.8501\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.554776430130005\n",
            "  Batch 305, Loss: 2.8492\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.601090431213379\n",
            "  Batch 306, Loss: 2.8484\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3571226596832275\n",
            "  Batch 307, Loss: 2.8468\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6536898612976074\n",
            "  Batch 308, Loss: 2.8461\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5617356300354004\n",
            "  Batch 309, Loss: 2.8452\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5335772037506104\n",
            "  Batch 310, Loss: 2.8442\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6432321071624756\n",
            "  Batch 311, Loss: 2.8436\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5650556087493896\n",
            "  Batch 312, Loss: 2.8427\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5726399421691895\n",
            "  Batch 313, Loss: 2.8418\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.475569725036621\n",
            "  Batch 314, Loss: 2.8406\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.558356285095215\n",
            "  Batch 315, Loss: 2.8398\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.362598419189453\n",
            "  Batch 316, Loss: 2.8382\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6827375888824463\n",
            "  Batch 317, Loss: 2.8378\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4648571014404297\n",
            "  Batch 318, Loss: 2.8366\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6599018573760986\n",
            "  Batch 319, Loss: 2.8360\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.478671073913574\n",
            "  Batch 320, Loss: 2.8349\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2482786178588867\n",
            "  Batch 321, Loss: 2.8331\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5003538131713867\n",
            "  Batch 322, Loss: 2.8320\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.488743543624878\n",
            "  Batch 323, Loss: 2.8310\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6671931743621826\n",
            "  Batch 324, Loss: 2.8305\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.673625946044922\n",
            "  Batch 325, Loss: 2.8300\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4552602767944336\n",
            "  Batch 326, Loss: 2.8288\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.530193567276001\n",
            "  Batch 327, Loss: 2.8279\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4119622707366943\n",
            "  Batch 328, Loss: 2.8267\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.544459104537964\n",
            "  Batch 329, Loss: 2.8258\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.579167366027832\n",
            "  Batch 330, Loss: 2.8251\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5425808429718018\n",
            "  Batch 331, Loss: 2.8242\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4309661388397217\n",
            "  Batch 332, Loss: 2.8230\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.598156690597534\n",
            "  Batch 333, Loss: 2.8223\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2032389640808105\n",
            "  Batch 334, Loss: 2.8205\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4425458908081055\n",
            "  Batch 335, Loss: 2.8194\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.506035566329956\n",
            "  Batch 336, Loss: 2.8184\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.496159076690674\n",
            "  Batch 337, Loss: 2.8175\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.548161268234253\n",
            "  Batch 338, Loss: 2.8167\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5618155002593994\n",
            "  Batch 339, Loss: 2.8159\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.46657395362854\n",
            "  Batch 340, Loss: 2.8149\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.412719964981079\n",
            "  Batch 341, Loss: 2.8137\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.48656964302063\n",
            "  Batch 342, Loss: 2.8128\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3757994174957275\n",
            "  Batch 343, Loss: 2.8115\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.390334129333496\n",
            "  Batch 344, Loss: 2.8103\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5949745178222656\n",
            "  Batch 345, Loss: 2.8096\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3811285495758057\n",
            "  Batch 346, Loss: 2.8084\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3790578842163086\n",
            "  Batch 347, Loss: 2.8072\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4657809734344482\n",
            "  Batch 348, Loss: 2.8062\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4428069591522217\n",
            "  Batch 349, Loss: 2.8051\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.571826219558716\n",
            "  Batch 350, Loss: 2.8045\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6785271167755127\n",
            "  Batch 351, Loss: 2.8041\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1697051525115967\n",
            "  Batch 352, Loss: 2.8023\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.429152727127075\n",
            "  Batch 353, Loss: 2.8013\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6652376651763916\n",
            "  Batch 354, Loss: 2.8009\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6085875034332275\n",
            "  Batch 355, Loss: 2.8003\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4176788330078125\n",
            "  Batch 356, Loss: 2.7993\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5267772674560547\n",
            "  Batch 357, Loss: 2.7985\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.365853786468506\n",
            "  Batch 358, Loss: 2.7973\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7194559574127197\n",
            "  Batch 359, Loss: 2.7971\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4322733879089355\n",
            "  Batch 360, Loss: 2.7961\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3037188053131104\n",
            "  Batch 361, Loss: 2.7947\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4931886196136475\n",
            "  Batch 362, Loss: 2.7939\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.20806884765625\n",
            "  Batch 363, Loss: 2.7922\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.38857102394104\n",
            "  Batch 364, Loss: 2.7911\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3981308937072754\n",
            "  Batch 365, Loss: 2.7901\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.455339193344116\n",
            "  Batch 366, Loss: 2.7891\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4640331268310547\n",
            "  Batch 367, Loss: 2.7883\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5937438011169434\n",
            "  Batch 368, Loss: 2.7877\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4229485988616943\n",
            "  Batch 369, Loss: 2.7867\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.467881679534912\n",
            "  Batch 370, Loss: 2.7859\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4340732097625732\n",
            "  Batch 371, Loss: 2.7849\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.510669231414795\n",
            "  Batch 372, Loss: 2.7842\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.726578950881958\n",
            "  Batch 373, Loss: 2.7840\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.650636911392212\n",
            "  Batch 374, Loss: 2.7837\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.204606533050537\n",
            "  Batch 375, Loss: 2.7821\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5351195335388184\n",
            "  Batch 376, Loss: 2.7815\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2053492069244385\n",
            "  Batch 377, Loss: 2.7800\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5716705322265625\n",
            "  Batch 378, Loss: 2.7794\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1212096214294434\n",
            "  Batch 379, Loss: 2.7777\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4431328773498535\n",
            "  Batch 380, Loss: 2.7768\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.213660478591919\n",
            "  Batch 381, Loss: 2.7753\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2383317947387695\n",
            "  Batch 382, Loss: 2.7739\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.378035545349121\n",
            "  Batch 383, Loss: 2.7729\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.403059482574463\n",
            "  Batch 384, Loss: 2.7719\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3431310653686523\n",
            "  Batch 385, Loss: 2.7708\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.514878273010254\n",
            "  Batch 386, Loss: 2.7701\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6182034015655518\n",
            "  Batch 387, Loss: 2.7697\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3719265460968018\n",
            "  Batch 388, Loss: 2.7687\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4455184936523438\n",
            "  Batch 389, Loss: 2.7679\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3309996128082275\n",
            "  Batch 390, Loss: 2.7668\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7402894496917725\n",
            "  Batch 391, Loss: 2.7667\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7071642875671387\n",
            "  Batch 392, Loss: 2.7665\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.42390513420105\n",
            "  Batch 393, Loss: 2.7657\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.137979507446289\n",
            "  Batch 394, Loss: 2.7641\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.389698028564453\n",
            "  Batch 395, Loss: 2.7631\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3292548656463623\n",
            "  Batch 396, Loss: 2.7620\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.47585391998291\n",
            "  Batch 397, Loss: 2.7613\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5912983417510986\n",
            "  Batch 398, Loss: 2.7609\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.42464280128479\n",
            "  Batch 399, Loss: 2.7600\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4451639652252197\n",
            "  Batch 400, Loss: 2.7593\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2890660762786865\n",
            "  Batch 401, Loss: 2.7581\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.590705394744873\n",
            "  Batch 402, Loss: 2.7577\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3428609371185303\n",
            "  Batch 403, Loss: 2.7566\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.292172908782959\n",
            "  Batch 404, Loss: 2.7555\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.16399884223938\n",
            "  Batch 405, Loss: 2.7540\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.31253719329834\n",
            "  Batch 406, Loss: 2.7529\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4975345134735107\n",
            "  Batch 407, Loss: 2.7523\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3369040489196777\n",
            "  Batch 408, Loss: 2.7513\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3587822914123535\n",
            "  Batch 409, Loss: 2.7503\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3853859901428223\n",
            "  Batch 410, Loss: 2.7494\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2291531562805176\n",
            "  Batch 411, Loss: 2.7482\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5396132469177246\n",
            "  Batch 412, Loss: 2.7477\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3687002658843994\n",
            "  Batch 413, Loss: 2.7468\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.462982654571533\n",
            "  Batch 414, Loss: 2.7461\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3229246139526367\n",
            "  Batch 415, Loss: 2.7451\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.443244218826294\n",
            "  Batch 416, Loss: 2.7443\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.565368175506592\n",
            "  Batch 417, Loss: 2.7439\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.340449094772339\n",
            "  Batch 418, Loss: 2.7429\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2948737144470215\n",
            "  Batch 419, Loss: 2.7419\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4452643394470215\n",
            "  Batch 420, Loss: 2.7412\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3502299785614014\n",
            "  Batch 421, Loss: 2.7402\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.326852560043335\n",
            "  Batch 422, Loss: 2.7392\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.306112289428711\n",
            "  Batch 423, Loss: 2.7382\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.392803192138672\n",
            "  Batch 424, Loss: 2.7374\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3794851303100586\n",
            "  Batch 425, Loss: 2.7366\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3735358715057373\n",
            "  Batch 426, Loss: 2.7357\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.395392656326294\n",
            "  Batch 427, Loss: 2.7349\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2487504482269287\n",
            "  Batch 428, Loss: 2.7338\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.528646230697632\n",
            "  Batch 429, Loss: 2.7333\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.329298734664917\n",
            "  Batch 430, Loss: 2.7324\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.29248046875\n",
            "  Batch 431, Loss: 2.7313\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.280095100402832\n",
            "  Batch 432, Loss: 2.7303\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.465782403945923\n",
            "  Batch 433, Loss: 2.7297\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.27428936958313\n",
            "  Batch 434, Loss: 2.7286\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3466341495513916\n",
            "  Batch 435, Loss: 2.7278\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.315793037414551\n",
            "  Batch 436, Loss: 2.7268\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.587280511856079\n",
            "  Batch 437, Loss: 2.7265\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3156745433807373\n",
            "  Batch 438, Loss: 2.7256\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2556705474853516\n",
            "  Batch 439, Loss: 2.7245\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4302432537078857\n",
            "  Batch 440, Loss: 2.7238\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3827645778656006\n",
            "  Batch 441, Loss: 2.7230\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.298222541809082\n",
            "  Batch 442, Loss: 2.7221\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2397944927215576\n",
            "  Batch 443, Loss: 2.7210\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4776902198791504\n",
            "  Batch 444, Loss: 2.7204\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.265995979309082\n",
            "  Batch 445, Loss: 2.7194\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.45478892326355\n",
            "  Batch 446, Loss: 2.7188\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2563047409057617\n",
            "  Batch 447, Loss: 2.7178\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.33996844291687\n",
            "  Batch 448, Loss: 2.7170\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4383726119995117\n",
            "  Batch 449, Loss: 2.7163\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2290966510772705\n",
            "  Batch 450, Loss: 2.7153\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.527909755706787\n",
            "  Batch 451, Loss: 2.7148\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2900547981262207\n",
            "  Batch 452, Loss: 2.7139\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4584286212921143\n",
            "  Batch 453, Loss: 2.7133\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4075491428375244\n",
            "  Batch 454, Loss: 2.7127\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3318164348602295\n",
            "  Batch 455, Loss: 2.7118\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1825027465820312\n",
            "  Batch 456, Loss: 2.7107\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4224069118499756\n",
            "  Batch 457, Loss: 2.7100\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3661632537841797\n",
            "  Batch 458, Loss: 2.7093\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3185343742370605\n",
            "  Batch 459, Loss: 2.7084\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.308741569519043\n",
            "  Batch 460, Loss: 2.7076\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2944657802581787\n",
            "  Batch 461, Loss: 2.7067\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.199187994003296\n",
            "  Batch 462, Loss: 2.7056\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.448763608932495\n",
            "  Batch 463, Loss: 2.7050\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.438293695449829\n",
            "  Batch 464, Loss: 2.7044\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4125044345855713\n",
            "  Batch 465, Loss: 2.7038\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.450580596923828\n",
            "  Batch 466, Loss: 2.7033\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.382861375808716\n",
            "  Batch 467, Loss: 2.7026\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.44785213470459\n",
            "  Batch 468, Loss: 2.7020\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2415850162506104\n",
            "  Batch 469, Loss: 2.7011\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.327805519104004\n",
            "  Batch 470, Loss: 2.7003\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.460083246231079\n",
            "  Batch 471, Loss: 2.6997\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.463294267654419\n",
            "  Batch 472, Loss: 2.6992\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.464909315109253\n",
            "  Batch 473, Loss: 2.6988\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2235233783721924\n",
            "  Batch 474, Loss: 2.6977\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5252816677093506\n",
            "  Batch 475, Loss: 2.6974\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4444706439971924\n",
            "  Batch 476, Loss: 2.6969\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1883621215820312\n",
            "  Batch 477, Loss: 2.6958\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5741748809814453\n",
            "  Batch 478, Loss: 2.6955\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.332857608795166\n",
            "  Batch 479, Loss: 2.6948\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.415936231613159\n",
            "  Batch 480, Loss: 2.6942\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.405162811279297\n",
            "  Batch 481, Loss: 2.6936\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.548522710800171\n",
            "  Batch 482, Loss: 2.6933\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.485999822616577\n",
            "  Batch 483, Loss: 2.6929\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4385876655578613\n",
            "  Batch 484, Loss: 2.6923\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.130812883377075\n",
            "  Batch 485, Loss: 2.6912\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4059901237487793\n",
            "  Batch 486, Loss: 2.6906\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.443741798400879\n",
            "  Batch 487, Loss: 2.6901\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3530502319335938\n",
            "  Batch 488, Loss: 2.6894\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.391357660293579\n",
            "  Batch 489, Loss: 2.6888\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.462200164794922\n",
            "  Batch 490, Loss: 2.6883\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.201406240463257\n",
            "  Batch 491, Loss: 2.6873\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.389068126678467\n",
            "  Batch 492, Loss: 2.6867\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.299790382385254\n",
            "  Batch 493, Loss: 2.6859\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5037472248077393\n",
            "  Batch 494, Loss: 2.6856\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2796127796173096\n",
            "  Batch 495, Loss: 2.6848\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5278141498565674\n",
            "  Batch 496, Loss: 2.6844\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2869110107421875\n",
            "  Batch 497, Loss: 2.6836\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.477482795715332\n",
            "  Batch 498, Loss: 2.6832\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3452839851379395\n",
            "  Batch 499, Loss: 2.6825\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3451781272888184\n",
            "  Batch 500, Loss: 2.6819\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4099245071411133\n",
            "  Batch 501, Loss: 2.6813\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.134212017059326\n",
            "  Batch 502, Loss: 2.6802\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.415823221206665\n",
            "  Batch 503, Loss: 2.6797\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.307738780975342\n",
            "  Batch 504, Loss: 2.6790\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4137487411499023\n",
            "  Batch 505, Loss: 2.6784\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.280207395553589\n",
            "  Batch 506, Loss: 2.6777\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3569459915161133\n",
            "  Batch 507, Loss: 2.6770\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.384453773498535\n",
            "  Batch 508, Loss: 2.6765\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.346719741821289\n",
            "  Batch 509, Loss: 2.6758\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.346377372741699\n",
            "  Batch 510, Loss: 2.6752\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1734442710876465\n",
            "  Batch 511, Loss: 2.6742\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3717715740203857\n",
            "  Batch 512, Loss: 2.6736\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4543700218200684\n",
            "  Batch 513, Loss: 2.6732\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3569207191467285\n",
            "  Batch 514, Loss: 2.6725\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.673205614089966\n",
            "  Batch 515, Loss: 2.6725\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.258671522140503\n",
            "  Batch 516, Loss: 2.6717\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5062460899353027\n",
            "  Batch 517, Loss: 2.6714\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.625701904296875\n",
            "  Batch 518, Loss: 2.6713\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.221294641494751\n",
            "  Batch 519, Loss: 2.6705\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2543325424194336\n",
            "  Batch 520, Loss: 2.6697\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3547959327697754\n",
            "  Batch 521, Loss: 2.6691\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4623260498046875\n",
            "  Batch 522, Loss: 2.6687\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6098995208740234\n",
            "  Batch 523, Loss: 2.6686\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.386186122894287\n",
            "  Batch 524, Loss: 2.6680\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2369487285614014\n",
            "  Batch 525, Loss: 2.6672\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4992599487304688\n",
            "  Batch 526, Loss: 2.6669\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.41218900680542\n",
            "  Batch 527, Loss: 2.6664\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.561145305633545\n",
            "  Batch 528, Loss: 2.6662\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.211016893386841\n",
            "  Batch 529, Loss: 2.6653\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.544962167739868\n",
            "  Batch 530, Loss: 2.6651\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4035496711730957\n",
            "  Batch 531, Loss: 2.6646\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4013402462005615\n",
            "  Batch 532, Loss: 2.6641\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.315856695175171\n",
            "  Batch 533, Loss: 2.6635\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5467469692230225\n",
            "  Batch 534, Loss: 2.6632\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.359072208404541\n",
            "  Batch 535, Loss: 2.6627\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.27187180519104\n",
            "  Batch 536, Loss: 2.6619\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4505536556243896\n",
            "  Batch 537, Loss: 2.6616\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.399030923843384\n",
            "  Batch 538, Loss: 2.6611\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.544597864151001\n",
            "  Batch 539, Loss: 2.6609\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.308983325958252\n",
            "  Batch 540, Loss: 2.6602\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3814568519592285\n",
            "  Batch 541, Loss: 2.6597\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.354414939880371\n",
            "  Batch 542, Loss: 2.6591\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.342132806777954\n",
            "  Batch 543, Loss: 2.6585\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.12933611869812\n",
            "  Batch 544, Loss: 2.6576\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2425477504730225\n",
            "  Batch 545, Loss: 2.6568\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2477598190307617\n",
            "  Batch 546, Loss: 2.6561\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2676093578338623\n",
            "  Batch 547, Loss: 2.6553\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2180442810058594\n",
            "  Batch 548, Loss: 2.6545\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4291436672210693\n",
            "  Batch 549, Loss: 2.6541\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2960546016693115\n",
            "  Batch 550, Loss: 2.6535\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2740094661712646\n",
            "  Batch 551, Loss: 2.6528\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.238032102584839\n",
            "  Batch 552, Loss: 2.6520\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0787343978881836\n",
            "  Batch 553, Loss: 2.6510\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4440269470214844\n",
            "  Batch 554, Loss: 2.6506\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.218660354614258\n",
            "  Batch 555, Loss: 2.6499\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.124467611312866\n",
            "  Batch 556, Loss: 2.6489\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.436302900314331\n",
            "  Batch 557, Loss: 2.6485\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.331174373626709\n",
            "  Batch 558, Loss: 2.6480\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2772910594940186\n",
            "  Batch 559, Loss: 2.6473\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4656782150268555\n",
            "  Batch 560, Loss: 2.6470\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2402613162994385\n",
            "  Batch 561, Loss: 2.6462\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2529759407043457\n",
            "  Batch 562, Loss: 2.6455\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3624935150146484\n",
            "  Batch 563, Loss: 2.6450\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.252316474914551\n",
            "  Batch 564, Loss: 2.6443\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3024556636810303\n",
            "  Batch 565, Loss: 2.6437\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2339584827423096\n",
            "  Batch 566, Loss: 2.6430\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.435574531555176\n",
            "  Batch 567, Loss: 2.6427\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3608133792877197\n",
            "  Batch 568, Loss: 2.6422\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1460916996002197\n",
            "  Batch 569, Loss: 2.6413\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.097795248031616\n",
            "  Batch 570, Loss: 2.6403\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1766176223754883\n",
            "  Batch 571, Loss: 2.6395\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2802653312683105\n",
            "  Batch 572, Loss: 2.6389\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.232590675354004\n",
            "  Batch 573, Loss: 2.6382\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3298683166503906\n",
            "  Batch 574, Loss: 2.6376\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3841681480407715\n",
            "  Batch 575, Loss: 2.6372\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2692861557006836\n",
            "  Batch 576, Loss: 2.6366\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3022544384002686\n",
            "  Batch 577, Loss: 2.6360\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4582371711730957\n",
            "  Batch 578, Loss: 2.6357\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.349825382232666\n",
            "  Batch 579, Loss: 2.6352\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6945111751556396\n",
            "  Batch 580, Loss: 2.6353\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3482768535614014\n",
            "  Batch 581, Loss: 2.6348\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5002810955047607\n",
            "  Batch 582, Loss: 2.6346\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2690300941467285\n",
            "  Batch 583, Loss: 2.6339\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4093739986419678\n",
            "  Batch 584, Loss: 2.6336\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.433123826980591\n",
            "  Batch 585, Loss: 2.6332\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4379870891571045\n",
            "  Batch 586, Loss: 2.6329\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3793489933013916\n",
            "  Batch 587, Loss: 2.6324\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.388550281524658\n",
            "  Batch 588, Loss: 2.6320\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3597984313964844\n",
            "  Batch 589, Loss: 2.6316\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.453209161758423\n",
            "  Batch 590, Loss: 2.6313\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5668411254882812\n",
            "  Batch 591, Loss: 2.6312\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4422767162323\n",
            "  Batch 592, Loss: 2.6308\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4525845050811768\n",
            "  Batch 593, Loss: 2.6305\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2666139602661133\n",
            "  Batch 594, Loss: 2.6299\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.428103446960449\n",
            "  Batch 595, Loss: 2.6296\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.387035608291626\n",
            "  Batch 596, Loss: 2.6292\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0898022651672363\n",
            "  Batch 597, Loss: 2.6283\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.375256299972534\n",
            "  Batch 598, Loss: 2.6279\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2978484630584717\n",
            "  Batch 599, Loss: 2.6273\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3059744834899902\n",
            "  Batch 600, Loss: 2.6268\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2643282413482666\n",
            "  Batch 601, Loss: 2.6262\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.373133897781372\n",
            "  Batch 602, Loss: 2.6257\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2875022888183594\n",
            "  Batch 603, Loss: 2.6252\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.435143232345581\n",
            "  Batch 604, Loss: 2.6249\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4250316619873047\n",
            "  Batch 605, Loss: 2.6245\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3595876693725586\n",
            "  Batch 606, Loss: 2.6241\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4308922290802\n",
            "  Batch 607, Loss: 2.6238\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.326960802078247\n",
            "  Batch 608, Loss: 2.6233\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3172447681427\n",
            "  Batch 609, Loss: 2.6228\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2952444553375244\n",
            "  Batch 610, Loss: 2.6223\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.371208429336548\n",
            "  Batch 611, Loss: 2.6218\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3274831771850586\n",
            "  Batch 612, Loss: 2.6214\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.250429391860962\n",
            "  Batch 613, Loss: 2.6208\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4196059703826904\n",
            "  Batch 614, Loss: 2.6204\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.476247787475586\n",
            "  Batch 615, Loss: 2.6202\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2130625247955322\n",
            "  Batch 616, Loss: 2.6195\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.377894163131714\n",
            "  Batch 617, Loss: 2.6191\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3335516452789307\n",
            "  Batch 618, Loss: 2.6187\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.443230628967285\n",
            "  Batch 619, Loss: 2.6184\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.37839412689209\n",
            "  Batch 620, Loss: 2.6180\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.485166311264038\n",
            "  Batch 621, Loss: 2.6178\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.243088960647583\n",
            "  Batch 622, Loss: 2.6172\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3203163146972656\n",
            "  Batch 623, Loss: 2.6167\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.325270652770996\n",
            "  Batch 624, Loss: 2.6162\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.378005027770996\n",
            "  Batch 625, Loss: 2.6159\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.458242654800415\n",
            "  Batch 626, Loss: 2.6156\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.333872079849243\n",
            "  Batch 627, Loss: 2.6152\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.396138906478882\n",
            "  Batch 628, Loss: 2.6148\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4651379585266113\n",
            "  Batch 629, Loss: 2.6146\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4120185375213623\n",
            "  Batch 630, Loss: 2.6143\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4452154636383057\n",
            "  Batch 631, Loss: 2.6140\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4708430767059326\n",
            "  Batch 632, Loss: 2.6138\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2071213722229004\n",
            "  Batch 633, Loss: 2.6131\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3860044479370117\n",
            "  Batch 634, Loss: 2.6128\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.349170207977295\n",
            "  Batch 635, Loss: 2.6123\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.404789686203003\n",
            "  Batch 636, Loss: 2.6120\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.268855571746826\n",
            "  Batch 637, Loss: 2.6115\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2321043014526367\n",
            "  Batch 638, Loss: 2.6109\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4075679779052734\n",
            "  Batch 639, Loss: 2.6106\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4172282218933105\n",
            "  Batch 640, Loss: 2.6103\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.200775623321533\n",
            "  Batch 641, Loss: 2.6096\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.390035390853882\n",
            "  Batch 642, Loss: 2.6093\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.230156660079956\n",
            "  Batch 643, Loss: 2.6087\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3568880558013916\n",
            "  Batch 644, Loss: 2.6083\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.066927194595337\n",
            "  Batch 645, Loss: 2.6075\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.37298321723938\n",
            "  Batch 646, Loss: 2.6071\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4363372325897217\n",
            "  Batch 647, Loss: 2.6068\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3630802631378174\n",
            "  Batch 648, Loss: 2.6065\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2966814041137695\n",
            "  Batch 649, Loss: 2.6060\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3118531703948975\n",
            "  Batch 650, Loss: 2.6055\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.252103328704834\n",
            "  Batch 651, Loss: 2.6050\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.347661256790161\n",
            "  Batch 652, Loss: 2.6046\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1801888942718506\n",
            "  Batch 653, Loss: 2.6039\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.281318426132202\n",
            "  Batch 654, Loss: 2.6035\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.303738832473755\n",
            "  Batch 655, Loss: 2.6030\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.201183319091797\n",
            "  Batch 656, Loss: 2.6024\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2992801666259766\n",
            "  Batch 657, Loss: 2.6019\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3142318725585938\n",
            "  Batch 658, Loss: 2.6015\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3603668212890625\n",
            "  Batch 659, Loss: 2.6011\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.396240472793579\n",
            "  Batch 660, Loss: 2.6008\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.461878538131714\n",
            "  Batch 661, Loss: 2.6006\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3223228454589844\n",
            "  Batch 662, Loss: 2.6002\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2443597316741943\n",
            "  Batch 663, Loss: 2.5996\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.206977128982544\n",
            "  Batch 664, Loss: 2.5990\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2399485111236572\n",
            "  Batch 665, Loss: 2.5985\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2656562328338623\n",
            "  Batch 666, Loss: 2.5980\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3086860179901123\n",
            "  Batch 667, Loss: 2.5976\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.23911714553833\n",
            "  Batch 668, Loss: 2.5970\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2100324630737305\n",
            "  Batch 669, Loss: 2.5965\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1719775199890137\n",
            "  Batch 670, Loss: 2.5958\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2814931869506836\n",
            "  Batch 671, Loss: 2.5954\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2320241928100586\n",
            "  Batch 672, Loss: 2.5948\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2010550498962402\n",
            "  Batch 673, Loss: 2.5942\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.259894371032715\n",
            "  Batch 674, Loss: 2.5937\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0955238342285156\n",
            "  Batch 675, Loss: 2.5930\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2143518924713135\n",
            "  Batch 676, Loss: 2.5924\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3296751976013184\n",
            "  Batch 677, Loss: 2.5921\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3405561447143555\n",
            "  Batch 678, Loss: 2.5917\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.480794668197632\n",
            "  Batch 679, Loss: 2.5915\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2408461570739746\n",
            "  Batch 680, Loss: 2.5910\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.249922513961792\n",
            "  Batch 681, Loss: 2.5905\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1837003231048584\n",
            "  Batch 682, Loss: 2.5899\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.233839988708496\n",
            "  Batch 683, Loss: 2.5894\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.045974016189575\n",
            "  Batch 684, Loss: 2.5886\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.152132749557495\n",
            "  Batch 685, Loss: 2.5880\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.279008150100708\n",
            "  Batch 686, Loss: 2.5875\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0333244800567627\n",
            "  Batch 687, Loss: 2.5867\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3002469539642334\n",
            "  Batch 688, Loss: 2.5863\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2009966373443604\n",
            "  Batch 689, Loss: 2.5857\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.339425802230835\n",
            "  Batch 690, Loss: 2.5854\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.358333110809326\n",
            "  Batch 691, Loss: 2.5850\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2277984619140625\n",
            "  Batch 692, Loss: 2.5845\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1249887943267822\n",
            "  Batch 693, Loss: 2.5839\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.101889133453369\n",
            "  Batch 694, Loss: 2.5832\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.140808343887329\n",
            "  Batch 695, Loss: 2.5825\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.305790424346924\n",
            "  Batch 696, Loss: 2.5821\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.309382200241089\n",
            "  Batch 697, Loss: 2.5817\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1821959018707275\n",
            "  Batch 698, Loss: 2.5812\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.124077320098877\n",
            "  Batch 699, Loss: 2.5805\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3022406101226807\n",
            "  Batch 700, Loss: 2.5801\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.193756103515625\n",
            "  Batch 701, Loss: 2.5796\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2010817527770996\n",
            "  Batch 702, Loss: 2.5790\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2944819927215576\n",
            "  Batch 703, Loss: 2.5786\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2673001289367676\n",
            "  Batch 704, Loss: 2.5782\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3638529777526855\n",
            "  Batch 705, Loss: 2.5779\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2253873348236084\n",
            "  Batch 706, Loss: 2.5774\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1862807273864746\n",
            "  Batch 707, Loss: 2.5768\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.337318181991577\n",
            "  Batch 708, Loss: 2.5765\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.346928596496582\n",
            "  Batch 709, Loss: 2.5762\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2724270820617676\n",
            "  Batch 710, Loss: 2.5757\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3313732147216797\n",
            "  Batch 711, Loss: 2.5754\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.486945152282715\n",
            "  Batch 712, Loss: 2.5753\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0490305423736572\n",
            "  Batch 713, Loss: 2.5745\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.091024160385132\n",
            "  Batch 714, Loss: 2.5738\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2749459743499756\n",
            "  Batch 715, Loss: 2.5734\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2288291454315186\n",
            "  Batch 716, Loss: 2.5729\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.26676344871521\n",
            "  Batch 717, Loss: 2.5725\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1511478424072266\n",
            "  Batch 718, Loss: 2.5719\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.310289144515991\n",
            "  Batch 719, Loss: 2.5716\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.349078416824341\n",
            "  Batch 720, Loss: 2.5713\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1521759033203125\n",
            "  Batch 721, Loss: 2.5707\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2380030155181885\n",
            "  Batch 722, Loss: 2.5702\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3081278800964355\n",
            "  Batch 723, Loss: 2.5699\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3882055282592773\n",
            "  Batch 724, Loss: 2.5696\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.355377197265625\n",
            "  Batch 725, Loss: 2.5693\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.27233624458313\n",
            "  Batch 726, Loss: 2.5689\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1725687980651855\n",
            "  Batch 727, Loss: 2.5684\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.12162709236145\n",
            "  Batch 728, Loss: 2.5677\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.401217460632324\n",
            "  Batch 729, Loss: 2.5675\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1565802097320557\n",
            "  Batch 730, Loss: 2.5669\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.241680860519409\n",
            "  Batch 731, Loss: 2.5665\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.149336099624634\n",
            "  Batch 732, Loss: 2.5659\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.416741132736206\n",
            "  Batch 733, Loss: 2.5657\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9379948377609253\n",
            "  Batch 734, Loss: 2.5649\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.207345962524414\n",
            "  Batch 735, Loss: 2.5644\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1848807334899902\n",
            "  Batch 736, Loss: 2.5639\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1765851974487305\n",
            "  Batch 737, Loss: 2.5633\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.265500783920288\n",
            "  Batch 738, Loss: 2.5629\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.085096836090088\n",
            "  Batch 739, Loss: 2.5623\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3305115699768066\n",
            "  Batch 740, Loss: 2.5620\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1034739017486572\n",
            "  Batch 741, Loss: 2.5614\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1914560794830322\n",
            "  Batch 742, Loss: 2.5609\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.041381597518921\n",
            "  Batch 743, Loss: 2.5602\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.102620840072632\n",
            "  Batch 744, Loss: 2.5596\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.250330686569214\n",
            "  Batch 745, Loss: 2.5591\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.234052896499634\n",
            "  Batch 746, Loss: 2.5587\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9830631017684937\n",
            "  Batch 747, Loss: 2.5579\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.122633695602417\n",
            "  Batch 748, Loss: 2.5573\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.16861629486084\n",
            "  Batch 749, Loss: 2.5568\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.096102714538574\n",
            "  Batch 750, Loss: 2.5562\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0362541675567627\n",
            "  Batch 751, Loss: 2.5555\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1670279502868652\n",
            "  Batch 752, Loss: 2.5550\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9942458868026733\n",
            "  Batch 753, Loss: 2.5543\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0383710861206055\n",
            "  Batch 754, Loss: 2.5536\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.103985071182251\n",
            "  Batch 755, Loss: 2.5530\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4812586307525635\n",
            "  Batch 756, Loss: 2.5529\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.284010410308838\n",
            "  Batch 757, Loss: 2.5525\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2215609550476074\n",
            "  Batch 758, Loss: 2.5521\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.215669870376587\n",
            "  Batch 759, Loss: 2.5517\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.294665575027466\n",
            "  Batch 760, Loss: 2.5513\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3197154998779297\n",
            "  Batch 761, Loss: 2.5510\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.234513998031616\n",
            "  Batch 762, Loss: 2.5506\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.231107473373413\n",
            "  Batch 763, Loss: 2.5502\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1881115436553955\n",
            "  Batch 764, Loss: 2.5497\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1574015617370605\n",
            "  Batch 765, Loss: 2.5492\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1748368740081787\n",
            "  Batch 766, Loss: 2.5487\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.416029930114746\n",
            "  Batch 767, Loss: 2.5485\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0082333087921143\n",
            "  Batch 768, Loss: 2.5478\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.0378222465515137\n",
            "  Batch 769, Loss: 2.5472\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.151386022567749\n",
            "  Batch 770, Loss: 2.5466\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1485507488250732\n",
            "  Batch 771, Loss: 2.5461\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.135377883911133\n",
            "  Batch 772, Loss: 2.5456\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.268603563308716\n",
            "  Batch 773, Loss: 2.5452\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9579399824142456\n",
            "  Batch 774, Loss: 2.5445\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9953173398971558\n",
            "  Batch 775, Loss: 2.5438\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.164367198944092\n",
            "  Batch 776, Loss: 2.5433\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.9840987920761108\n",
            "  Batch 777, Loss: 2.5426\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2427892684936523\n",
            "  Batch 778, Loss: 2.5422\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.346080780029297\n",
            "  Batch 779, Loss: 2.5419\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3206381797790527\n",
            "  Batch 780, Loss: 2.5416\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1300086975097656\n",
            "  Batch 781, Loss: 2.5411\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.1075541973114014\n",
            "  Batch 782, Loss: 2.5406\n",
            "Epoch 1 Loss: 2.5406\n",
            "skipped:  0\n",
            "model finished training\n",
            "model saved\n",
            "lossed saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 100K Model"
      ],
      "metadata": {
        "id": "t7V-OE-pI6iH"
      },
      "id": "t7V-OE-pI6iH"
    },
    {
      "cell_type": "code",
      "source": [
        "m2m_data_path_100k = '/content/drive/MyDrive/W266 Final Project/m2m_translated_data/m2m_100k_dataset.csv'\n",
        "\n",
        "m2m_trained_model_100k, m2m_losses_100k = train_pipeline(m2m_data_path_100k, train_model, tokenizer, 'm2m')"
      ],
      "metadata": {
        "id": "ZSjLnJGSI-nT"
      },
      "id": "ZSjLnJGSI-nT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 200K Model"
      ],
      "metadata": {
        "id": "iyCB41SFI-5L"
      },
      "id": "iyCB41SFI-5L"
    },
    {
      "cell_type": "code",
      "source": [
        "m2m_data_path_200k = '/content/drive/MyDrive/W266 Final Project/m2m_translated_data/m2m_200k_dataset.csv'\n",
        "\n",
        "m2m_trained_model_200k, m2m_losses_200k = train_pipeline(m2m_data_path_200k, train_model, tokenizer, 'm2m')"
      ],
      "metadata": {
        "id": "GSq4KAz_I_HC"
      },
      "id": "GSq4KAz_I_HC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ChatGPT Models"
      ],
      "metadata": {
        "id": "8-EqyYQQm6-O"
      },
      "id": "8-EqyYQQm6-O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10K Model"
      ],
      "metadata": {
        "id": "6yqoIN5EnB6d"
      },
      "id": "6yqoIN5EnB6d"
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_data_path_10k = '/content/drive/MyDrive/W266 Final Project/gpt_translated_data/gpt_10k_dataset.csv'\n",
        "gpt_trained_model_10k, gpt_losses_10k = train_pipeline(gpt_data_path_10k, train_model, tokenizer, 'gpt')"
      ],
      "metadata": {
        "id": "J7JHBwurnB6d"
      },
      "execution_count": null,
      "outputs": [],
      "id": "J7JHBwurnB6d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20K Model"
      ],
      "metadata": {
        "id": "FegXXXZ6nB6e"
      },
      "id": "FegXXXZ6nB6e"
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_data_path_20k = '/content/drive/MyDrive/W266 Final Project/gpt_translated_data/gpt_20k_dataset.csv'\n",
        "gpt_trained_model_20k, gpt_losses_20k = train_pipeline(gpt_data_path_20k, train_model, tokenizer, 'gpt')"
      ],
      "metadata": {
        "id": "_W6V2f2AnB6e"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_W6V2f2AnB6e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjXI6YRunB6e"
      },
      "source": [
        "## 50K Model"
      ],
      "id": "xjXI6YRunB6e"
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_data_path_50k = '/content/drive/MyDrive/W266 Final Project/gpt_translated_data/gpt_50k_dataset.csv'\n",
        "gpt_trained_model_50k, gpt_losses_50k = train_pipeline(gpt_data_path_50k, train_model, tokenizer, 'gpt')"
      ],
      "metadata": {
        "id": "DO_ppCA7nB6e"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DO_ppCA7nB6e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 100k model"
      ],
      "metadata": {
        "id": "DmrBpL3GnB6e"
      },
      "id": "DmrBpL3GnB6e"
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_data_path_100k = '/content/drive/MyDrive/W266 Final Project/gpt_translated_data/gpt_100k_dataset.csv'\n",
        "gpt_trained_model_100k, gpt_losses_100k = train_pipeline(gpt_data_path_100k, train_model, tokenizer, 'gpt')"
      ],
      "metadata": {
        "id": "AocRdRgInB6e"
      },
      "execution_count": null,
      "outputs": [],
      "id": "AocRdRgInB6e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2nd Half 50K Model"
      ],
      "metadata": {
        "id": "DZeucRgZZVLZ"
      },
      "id": "DZeucRgZZVLZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# train model using last 25K rows of gpt_100k_dataset to create a 50K dataset, 50K model uses first 25K rows\n",
        "gpt_data_path_last_50k = '/content/drive/MyDrive/W266 Final Project/gpt_translated_data/gpt_last50k_dataset.csv'\n",
        "gpt_trained_model_last_50k, gpt_losses_last_50k = train_pipeline(gpt_data_path_last_50k, train_model, tokenizer, 'gpt_last_50k')"
      ],
      "metadata": {
        "id": "Ne23o3_ZniTV"
      },
      "id": "Ne23o3_ZniTV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reworded ChatGPT Models"
      ],
      "metadata": {
        "id": "3I9Y_ek9nlRq"
      },
      "id": "3I9Y_ek9nlRq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10K Model"
      ],
      "metadata": {
        "id": "Qpa3tgnOnlRq"
      },
      "id": "Qpa3tgnOnlRq"
    },
    {
      "cell_type": "code",
      "source": [
        "rw_data_path_10k = '/content/drive/MyDrive/W266 Final Project/gt_reworded_data/reword_10k_dataset.csv'\n",
        "rw_trained_model_10k, rw_losses_10k, rw_train_epoch_losses_10k, rw_val_epoch_losses_10k = train_pipeline(rw_data_path_10k, train_model, tokenizer, 'rw')"
      ],
      "metadata": {
        "id": "Of0O2uCJnlRr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c8cd8a4c-f559-4881-893a-dabca3fbbcaa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10k\n",
            "data loaded\n",
            "on gpu\n",
            "Epoch 1/4\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.083862781524658\n",
            "  Batch 1, Loss: 5.0839\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  6.814765930175781\n",
            "  Batch 2, Loss: 5.9493\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.431977272033691\n",
            "  Batch 3, Loss: 5.7769\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  5.125519752502441\n",
            "  Batch 4, Loss: 5.6140\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  4.319746017456055\n",
            "  Batch 5, Loss: 5.3552\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.959989070892334\n",
            "  Batch 6, Loss: 5.1226\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.8030011653900146\n",
            "  Batch 7, Loss: 4.9341\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.6175811290740967\n",
            "  Batch 8, Loss: 4.7696\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.8699872493743896\n",
            "  Batch 9, Loss: 4.6696\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.9175660610198975\n",
            "  Batch 10, Loss: 4.5944\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.6599810123443604\n",
            "  Batch 11, Loss: 4.5095\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.600327968597412\n",
            "  Batch 12, Loss: 4.4337\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.788266181945801\n",
            "  Batch 13, Loss: 4.3840\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.6545796394348145\n",
            "  Batch 14, Loss: 4.3319\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.7946388721466064\n",
            "  Batch 15, Loss: 4.2961\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.6745197772979736\n",
            "  Batch 16, Loss: 4.2573\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.5147128105163574\n",
            "  Batch 17, Loss: 4.2136\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.5454094409942627\n",
            "  Batch 18, Loss: 4.1765\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.459735870361328\n",
            "  Batch 19, Loss: 4.1387\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.606971263885498\n",
            "  Batch 20, Loss: 4.1122\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.418887138366699\n",
            "  Batch 21, Loss: 4.0791\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.4344286918640137\n",
            "  Batch 22, Loss: 4.0498\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.6293694972991943\n",
            "  Batch 23, Loss: 4.0316\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.36592698097229\n",
            "  Batch 24, Loss: 4.0038\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.3130013942718506\n",
            "  Batch 25, Loss: 3.9762\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.2719552516937256\n",
            "  Batch 26, Loss: 3.9491\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.2546441555023193\n",
            "  Batch 27, Loss: 3.9234\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.3700525760650635\n",
            "  Batch 28, Loss: 3.9036\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.228398084640503\n",
            "  Batch 29, Loss: 3.8803\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.3784244060516357\n",
            "  Batch 30, Loss: 3.8636\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.387021541595459\n",
            "  Batch 31, Loss: 3.8482\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.451908826828003\n",
            "  Batch 32, Loss: 3.8358\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.204591989517212\n",
            "  Batch 33, Loss: 3.8167\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.1807303428649902\n",
            "  Batch 34, Loss: 3.7980\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.2483229637145996\n",
            "  Batch 35, Loss: 3.7823\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.2240118980407715\n",
            "  Batch 36, Loss: 3.7668\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.2043704986572266\n",
            "  Batch 37, Loss: 3.7516\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0956456661224365\n",
            "  Batch 38, Loss: 3.7343\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.07125186920166\n",
            "  Batch 39, Loss: 3.7173\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0389726161956787\n",
            "  Batch 40, Loss: 3.7004\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.252908706665039\n",
            "  Batch 41, Loss: 3.6895\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0876331329345703\n",
            "  Batch 42, Loss: 3.6751\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9610111713409424\n",
            "  Batch 43, Loss: 3.6585\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.052475690841675\n",
            "  Batch 44, Loss: 3.6448\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.1107800006866455\n",
            "  Batch 45, Loss: 3.6329\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9435813426971436\n",
            "  Batch 46, Loss: 3.6179\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.997824192047119\n",
            "  Batch 47, Loss: 3.6047\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.019961357116699\n",
            "  Batch 48, Loss: 3.5925\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8400280475616455\n",
            "  Batch 49, Loss: 3.5772\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.3490817546844482\n",
            "  Batch 50, Loss: 3.5726\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.012640953063965\n",
            "  Batch 51, Loss: 3.5616\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.047390937805176\n",
            "  Batch 52, Loss: 3.5517\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9223828315734863\n",
            "  Batch 53, Loss: 3.5399\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.022047281265259\n",
            "  Batch 54, Loss: 3.5303\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0148630142211914\n",
            "  Batch 55, Loss: 3.5209\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0992703437805176\n",
            "  Batch 56, Loss: 3.5134\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9115631580352783\n",
            "  Batch 57, Loss: 3.5028\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.3174822330474854\n",
            "  Batch 58, Loss: 3.4996\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9229328632354736\n",
            "  Batch 59, Loss: 3.4898\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0860490798950195\n",
            "  Batch 60, Loss: 3.4831\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.0382297039031982\n",
            "  Batch 61, Loss: 3.4758\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8423023223876953\n",
            "  Batch 62, Loss: 3.4656\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.114367961883545\n",
            "  Batch 63, Loss: 3.4600\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.1826131343841553\n",
            "  Batch 64, Loss: 3.4557\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.898484468460083\n",
            "  Batch 65, Loss: 3.4471\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8490352630615234\n",
            "  Batch 66, Loss: 3.4381\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8345346450805664\n",
            "  Batch 67, Loss: 3.4291\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8150508403778076\n",
            "  Batch 68, Loss: 3.4200\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.03340744972229\n",
            "  Batch 69, Loss: 3.4144\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.77131724357605\n",
            "  Batch 70, Loss: 3.4052\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8584249019622803\n",
            "  Batch 71, Loss: 3.3975\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6633400917053223\n",
            "  Batch 72, Loss: 3.3873\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.9151113033294678\n",
            "  Batch 73, Loss: 3.3809\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  3.053201913833618\n",
            "  Batch 74, Loss: 3.3764\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.938581943511963\n",
            "  Batch 75, Loss: 3.3706\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8109524250030518\n",
            "  Batch 76, Loss: 3.3632\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7949941158294678\n",
            "  Batch 77, Loss: 3.3559\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6682331562042236\n",
            "  Batch 78, Loss: 3.3470\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8574347496032715\n",
            "  Batch 79, Loss: 3.3408\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.810234546661377\n",
            "  Batch 80, Loss: 3.3342\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.765045166015625\n",
            "  Batch 81, Loss: 3.3272\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8520143032073975\n",
            "  Batch 82, Loss: 3.3214\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.721104145050049\n",
            "  Batch 83, Loss: 3.3142\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.8470423221588135\n",
            "  Batch 84, Loss: 3.3086\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.870290517807007\n",
            "  Batch 85, Loss: 3.3034\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.875028133392334\n",
            "  Batch 86, Loss: 3.2985\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6879353523254395\n",
            "  Batch 87, Loss: 3.2914\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.909571886062622\n",
            "  Batch 88, Loss: 3.2871\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7918283939361572\n",
            "  Batch 89, Loss: 3.2815\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.650038957595825\n",
            "  Batch 90, Loss: 3.2745\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.884577512741089\n",
            "  Batch 91, Loss: 3.2702\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6678433418273926\n",
            "  Batch 92, Loss: 3.2637\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.662578582763672\n",
            "  Batch 93, Loss: 3.2572\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7542214393615723\n",
            "  Batch 94, Loss: 3.2519\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7425174713134766\n",
            "  Batch 95, Loss: 3.2465\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6445302963256836\n",
            "  Batch 96, Loss: 3.2402\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5084567070007324\n",
            "  Batch 97, Loss: 3.2327\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7491226196289062\n",
            "  Batch 98, Loss: 3.2278\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6182339191436768\n",
            "  Batch 99, Loss: 3.2216\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.708635091781616\n",
            "  Batch 100, Loss: 3.2165\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6667003631591797\n",
            "  Batch 101, Loss: 3.2110\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.775339365005493\n",
            "  Batch 102, Loss: 3.2068\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6930348873138428\n",
            "  Batch 103, Loss: 3.2018\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.67134165763855\n",
            "  Batch 104, Loss: 3.1967\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.655595302581787\n",
            "  Batch 105, Loss: 3.1915\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5973992347717285\n",
            "  Batch 106, Loss: 3.1859\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.495879650115967\n",
            "  Batch 107, Loss: 3.1795\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.836838960647583\n",
            "  Batch 108, Loss: 3.1763\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.529947519302368\n",
            "  Batch 109, Loss: 3.1704\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6597607135772705\n",
            "  Batch 110, Loss: 3.1657\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.473292589187622\n",
            "  Batch 111, Loss: 3.1595\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.7132232189178467\n",
            "  Batch 112, Loss: 3.1555\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6636321544647217\n",
            "  Batch 113, Loss: 3.1511\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6351590156555176\n",
            "  Batch 114, Loss: 3.1466\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.616213321685791\n",
            "  Batch 115, Loss: 3.1420\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5989978313446045\n",
            "  Batch 116, Loss: 3.1373\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.473529577255249\n",
            "  Batch 117, Loss: 3.1317\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4273622035980225\n",
            "  Batch 118, Loss: 3.1257\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.666508674621582\n",
            "  Batch 119, Loss: 3.1218\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.552753210067749\n",
            "  Batch 120, Loss: 3.1171\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.6030735969543457\n",
            "  Batch 121, Loss: 3.1128\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.380747079849243\n",
            "  Batch 122, Loss: 3.1068\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4851057529449463\n",
            "  Batch 123, Loss: 3.1018\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5800952911376953\n",
            "  Batch 124, Loss: 3.0976\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.481612205505371\n",
            "  Batch 125, Loss: 3.0926\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.515357494354248\n",
            "  Batch 126, Loss: 3.0881\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4949073791503906\n",
            "  Batch 127, Loss: 3.0834\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.482135534286499\n",
            "  Batch 128, Loss: 3.0787\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5269663333892822\n",
            "  Batch 129, Loss: 3.0744\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.44730281829834\n",
            "  Batch 130, Loss: 3.0696\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3842058181762695\n",
            "  Batch 131, Loss: 3.0644\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2874763011932373\n",
            "  Batch 132, Loss: 3.0585\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4936089515686035\n",
            "  Batch 133, Loss: 3.0542\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5905606746673584\n",
            "  Batch 134, Loss: 3.0508\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.630359411239624\n",
            "  Batch 135, Loss: 3.0477\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3939952850341797\n",
            "  Batch 136, Loss: 3.0428\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4149646759033203\n",
            "  Batch 137, Loss: 3.0383\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4285523891448975\n",
            "  Batch 138, Loss: 3.0338\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4179575443267822\n",
            "  Batch 139, Loss: 3.0294\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3881781101226807\n",
            "  Batch 140, Loss: 3.0248\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.362100839614868\n",
            "  Batch 141, Loss: 3.0201\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3902506828308105\n",
            "  Batch 142, Loss: 3.0157\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.5423572063446045\n",
            "  Batch 143, Loss: 3.0124\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3771533966064453\n",
            "  Batch 144, Loss: 3.0080\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.388138771057129\n",
            "  Batch 145, Loss: 3.0037\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.153040647506714\n",
            "  Batch 146, Loss: 2.9979\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4716269969940186\n",
            "  Batch 147, Loss: 2.9943\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4872982501983643\n",
            "  Batch 148, Loss: 2.9909\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3167357444763184\n",
            "  Batch 149, Loss: 2.9863\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4540579319000244\n",
            "  Batch 150, Loss: 2.9828\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3654351234436035\n",
            "  Batch 151, Loss: 2.9787\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3962252140045166\n",
            "  Batch 152, Loss: 2.9749\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.3480186462402344\n",
            "  Batch 153, Loss: 2.9708\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.269401788711548\n",
            "  Batch 154, Loss: 2.9662\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.289130687713623\n",
            "  Batch 155, Loss: 2.9619\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.4592628479003906\n",
            "  Batch 156, Loss: 2.9586\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  2.2852022647857666\n",
            "  Batch 157, Loss: 2.9543\n",
            "length of train_dataloader:  157\n",
            "Epoch 1 Loss: 2.9543\n",
            "Validation Loss after Epoch 1: 4.4824\n",
            "Epoch 2/4\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6715399026870728\n",
            "  Batch 1, Loss: 1.6715\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6536561250686646\n",
            "  Batch 2, Loss: 1.6626\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.678127646446228\n",
            "  Batch 3, Loss: 1.6678\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.7471020221710205\n",
            "  Batch 4, Loss: 1.6876\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5832515954971313\n",
            "  Batch 5, Loss: 1.6667\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5776594877243042\n",
            "  Batch 6, Loss: 1.6519\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.662761926651001\n",
            "  Batch 7, Loss: 1.6534\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.7544842958450317\n",
            "  Batch 8, Loss: 1.6661\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.633750319480896\n",
            "  Batch 9, Loss: 1.6625\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.504928469657898\n",
            "  Batch 10, Loss: 1.6467\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.569025993347168\n",
            "  Batch 11, Loss: 1.6397\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6447776556015015\n",
            "  Batch 12, Loss: 1.6401\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.644951581954956\n",
            "  Batch 13, Loss: 1.6405\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6920336484909058\n",
            "  Batch 14, Loss: 1.6441\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.769261360168457\n",
            "  Batch 15, Loss: 1.6525\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.7706999778747559\n",
            "  Batch 16, Loss: 1.6599\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6599785089492798\n",
            "  Batch 17, Loss: 1.6599\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5435066223144531\n",
            "  Batch 18, Loss: 1.6534\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6485052108764648\n",
            "  Batch 19, Loss: 1.6532\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5888148546218872\n",
            "  Batch 20, Loss: 1.6499\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.705604076385498\n",
            "  Batch 21, Loss: 1.6526\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4469364881515503\n",
            "  Batch 22, Loss: 1.6432\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.7007864713668823\n",
            "  Batch 23, Loss: 1.6457\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.593935251235962\n",
            "  Batch 24, Loss: 1.6436\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6078441143035889\n",
            "  Batch 25, Loss: 1.6422\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6892940998077393\n",
            "  Batch 26, Loss: 1.6440\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6323788166046143\n",
            "  Batch 27, Loss: 1.6435\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6431776285171509\n",
            "  Batch 28, Loss: 1.6435\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6925228834152222\n",
            "  Batch 29, Loss: 1.6452\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.7033458948135376\n",
            "  Batch 30, Loss: 1.6472\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.693806767463684\n",
            "  Batch 31, Loss: 1.6487\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4686137437820435\n",
            "  Batch 32, Loss: 1.6430\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6128045320510864\n",
            "  Batch 33, Loss: 1.6421\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.7244162559509277\n",
            "  Batch 34, Loss: 1.6445\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5113180875778198\n",
            "  Batch 35, Loss: 1.6407\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.634516954421997\n",
            "  Batch 36, Loss: 1.6406\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6019515991210938\n",
            "  Batch 37, Loss: 1.6395\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6124075651168823\n",
            "  Batch 38, Loss: 1.6388\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6471006870269775\n",
            "  Batch 39, Loss: 1.6390\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.651283860206604\n",
            "  Batch 40, Loss: 1.6393\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6724995374679565\n",
            "  Batch 41, Loss: 1.6401\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.582419514656067\n",
            "  Batch 42, Loss: 1.6388\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.7053101062774658\n",
            "  Batch 43, Loss: 1.6403\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6666812896728516\n",
            "  Batch 44, Loss: 1.6409\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.550612449645996\n",
            "  Batch 45, Loss: 1.6389\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5761079788208008\n",
            "  Batch 46, Loss: 1.6375\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6125410795211792\n",
            "  Batch 47, Loss: 1.6370\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.465376377105713\n",
            "  Batch 48, Loss: 1.6334\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6235344409942627\n",
            "  Batch 49, Loss: 1.6332\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5963877439498901\n",
            "  Batch 50, Loss: 1.6325\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5159968137741089\n",
            "  Batch 51, Loss: 1.6302\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4830447435379028\n",
            "  Batch 52, Loss: 1.6274\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5925716161727905\n",
            "  Batch 53, Loss: 1.6267\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5705971717834473\n",
            "  Batch 54, Loss: 1.6257\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4625524282455444\n",
            "  Batch 55, Loss: 1.6227\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4764509201049805\n",
            "  Batch 56, Loss: 1.6201\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.653252124786377\n",
            "  Batch 57, Loss: 1.6207\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6219912767410278\n",
            "  Batch 58, Loss: 1.6207\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.7601852416992188\n",
            "  Batch 59, Loss: 1.6231\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.618130087852478\n",
            "  Batch 60, Loss: 1.6230\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5920692682266235\n",
            "  Batch 61, Loss: 1.6225\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5910983085632324\n",
            "  Batch 62, Loss: 1.6220\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5773612260818481\n",
            "  Batch 63, Loss: 1.6213\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6981492042541504\n",
            "  Batch 64, Loss: 1.6225\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6338649988174438\n",
            "  Batch 65, Loss: 1.6226\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3561304807662964\n",
            "  Batch 66, Loss: 1.6186\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5296341180801392\n",
            "  Batch 67, Loss: 1.6173\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5420111417770386\n",
            "  Batch 68, Loss: 1.6162\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6098158359527588\n",
            "  Batch 69, Loss: 1.6161\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5050994157791138\n",
            "  Batch 70, Loss: 1.6145\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4521746635437012\n",
            "  Batch 71, Loss: 1.6122\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6488896608352661\n",
            "  Batch 72, Loss: 1.6127\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5461848974227905\n",
            "  Batch 73, Loss: 1.6118\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.542249321937561\n",
            "  Batch 74, Loss: 1.6109\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.470055103302002\n",
            "  Batch 75, Loss: 1.6090\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5327361822128296\n",
            "  Batch 76, Loss: 1.6080\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6204302310943604\n",
            "  Batch 77, Loss: 1.6081\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.47142493724823\n",
            "  Batch 78, Loss: 1.6064\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6001453399658203\n",
            "  Batch 79, Loss: 1.6063\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6643496751785278\n",
            "  Batch 80, Loss: 1.6070\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.500496745109558\n",
            "  Batch 81, Loss: 1.6057\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4351348876953125\n",
            "  Batch 82, Loss: 1.6036\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.696738600730896\n",
            "  Batch 83, Loss: 1.6048\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6221961975097656\n",
            "  Batch 84, Loss: 1.6050\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3794716596603394\n",
            "  Batch 85, Loss: 1.6023\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5179171562194824\n",
            "  Batch 86, Loss: 1.6013\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4038680791854858\n",
            "  Batch 87, Loss: 1.5991\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.6116901636123657\n",
            "  Batch 88, Loss: 1.5992\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4687566757202148\n",
            "  Batch 89, Loss: 1.5977\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4972373247146606\n",
            "  Batch 90, Loss: 1.5966\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4139395952224731\n",
            "  Batch 91, Loss: 1.5946\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.463889241218567\n",
            "  Batch 92, Loss: 1.5932\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3636170625686646\n",
            "  Batch 93, Loss: 1.5907\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4115495681762695\n",
            "  Batch 94, Loss: 1.5888\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.357090950012207\n",
            "  Batch 95, Loss: 1.5864\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.478820562362671\n",
            "  Batch 96, Loss: 1.5853\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3238344192504883\n",
            "  Batch 97, Loss: 1.5826\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3417476415634155\n",
            "  Batch 98, Loss: 1.5801\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4881174564361572\n",
            "  Batch 99, Loss: 1.5792\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4824481010437012\n",
            "  Batch 100, Loss: 1.5782\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4900704622268677\n",
            "  Batch 101, Loss: 1.5773\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.492142915725708\n",
            "  Batch 102, Loss: 1.5765\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3996354341506958\n",
            "  Batch 103, Loss: 1.5748\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3329441547393799\n",
            "  Batch 104, Loss: 1.5725\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.468624472618103\n",
            "  Batch 105, Loss: 1.5715\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3850568532943726\n",
            "  Batch 106, Loss: 1.5697\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3206487894058228\n",
            "  Batch 107, Loss: 1.5674\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.481701135635376\n",
            "  Batch 108, Loss: 1.5666\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.5025290250778198\n",
            "  Batch 109, Loss: 1.5660\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.266834020614624\n",
            "  Batch 110, Loss: 1.5633\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3450223207473755\n",
            "  Batch 111, Loss: 1.5613\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2673674821853638\n",
            "  Batch 112, Loss: 1.5587\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.492571473121643\n",
            "  Batch 113, Loss: 1.5581\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3992313146591187\n",
            "  Batch 114, Loss: 1.5567\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.204582929611206\n",
            "  Batch 115, Loss: 1.5537\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.337333083152771\n",
            "  Batch 116, Loss: 1.5518\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4287385940551758\n",
            "  Batch 117, Loss: 1.5507\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4169044494628906\n",
            "  Batch 118, Loss: 1.5496\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.241239070892334\n",
            "  Batch 119, Loss: 1.5470\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4437437057495117\n",
            "  Batch 120, Loss: 1.5462\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.322252631187439\n",
            "  Batch 121, Loss: 1.5443\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2448241710662842\n",
            "  Batch 122, Loss: 1.5418\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3681477308273315\n",
            "  Batch 123, Loss: 1.5404\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2804151773452759\n",
            "  Batch 124, Loss: 1.5383\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3302866220474243\n",
            "  Batch 125, Loss: 1.5367\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.4068567752838135\n",
            "  Batch 126, Loss: 1.5356\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3066867589950562\n",
            "  Batch 127, Loss: 1.5338\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3863353729248047\n",
            "  Batch 128, Loss: 1.5327\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.1362212896347046\n",
            "  Batch 129, Loss: 1.5296\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3518425226211548\n",
            "  Batch 130, Loss: 1.5282\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.1903475522994995\n",
            "  Batch 131, Loss: 1.5257\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.1897929906845093\n",
            "  Batch 132, Loss: 1.5231\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2317956686019897\n",
            "  Batch 133, Loss: 1.5209\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2157803773880005\n",
            "  Batch 134, Loss: 1.5187\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2585935592651367\n",
            "  Batch 135, Loss: 1.5167\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2794102430343628\n",
            "  Batch 136, Loss: 1.5150\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.1554628610610962\n",
            "  Batch 137, Loss: 1.5124\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2697981595993042\n",
            "  Batch 138, Loss: 1.5106\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.254127860069275\n",
            "  Batch 139, Loss: 1.5088\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.270369291305542\n",
            "  Batch 140, Loss: 1.5071\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.301638126373291\n",
            "  Batch 141, Loss: 1.5056\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.425226092338562\n",
            "  Batch 142, Loss: 1.5050\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2521926164627075\n",
            "  Batch 143, Loss: 1.5033\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2291860580444336\n",
            "  Batch 144, Loss: 1.5014\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2098362445831299\n",
            "  Batch 145, Loss: 1.4994\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2100579738616943\n",
            "  Batch 146, Loss: 1.4974\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.292837381362915\n",
            "  Batch 147, Loss: 1.4960\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.257684350013733\n",
            "  Batch 148, Loss: 1.4944\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2481193542480469\n",
            "  Batch 149, Loss: 1.4927\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.137200117111206\n",
            "  Batch 150, Loss: 1.4903\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.3085356950759888\n",
            "  Batch 151, Loss: 1.4891\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.157091736793518\n",
            "  Batch 152, Loss: 1.4870\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.2860732078552246\n",
            "  Batch 153, Loss: 1.4856\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.1847890615463257\n",
            "  Batch 154, Loss: 1.4837\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.205910563468933\n",
            "  Batch 155, Loss: 1.4819\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.328370451927185\n",
            "  Batch 156, Loss: 1.4809\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  1.0236035585403442\n",
            "  Batch 157, Loss: 1.4780\n",
            "length of train_dataloader:  157\n",
            "Epoch 2 Loss: 1.4780\n",
            "Validation Loss after Epoch 2: 5.4799\n",
            "Epoch 3/4\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8229204416275024\n",
            "  Batch 1, Loss: 0.8229\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8149386644363403\n",
            "  Batch 2, Loss: 0.8189\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7515788078308105\n",
            "  Batch 3, Loss: 0.7965\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.747412919998169\n",
            "  Batch 4, Loss: 0.7842\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8136471509933472\n",
            "  Batch 5, Loss: 0.7901\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8583065271377563\n",
            "  Batch 6, Loss: 0.8015\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7256449460983276\n",
            "  Batch 7, Loss: 0.7906\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8565167188644409\n",
            "  Batch 8, Loss: 0.7989\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7782812118530273\n",
            "  Batch 9, Loss: 0.7966\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7561471462249756\n",
            "  Batch 10, Loss: 0.7925\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8007050156593323\n",
            "  Batch 11, Loss: 0.7933\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8588919043540955\n",
            "  Batch 12, Loss: 0.7987\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8493534326553345\n",
            "  Batch 13, Loss: 0.8026\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7764245271682739\n",
            "  Batch 14, Loss: 0.8008\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8443729281425476\n",
            "  Batch 15, Loss: 0.8037\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8685175180435181\n",
            "  Batch 16, Loss: 0.8077\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8524625897407532\n",
            "  Batch 17, Loss: 0.8104\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8173279762268066\n",
            "  Batch 18, Loss: 0.8107\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8392125964164734\n",
            "  Batch 19, Loss: 0.8122\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8418362736701965\n",
            "  Batch 20, Loss: 0.8137\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8025791049003601\n",
            "  Batch 21, Loss: 0.8132\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7615985870361328\n",
            "  Batch 22, Loss: 0.8108\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8034985661506653\n",
            "  Batch 23, Loss: 0.8105\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8402204513549805\n",
            "  Batch 24, Loss: 0.8118\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7987211346626282\n",
            "  Batch 25, Loss: 0.8112\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8105310201644897\n",
            "  Batch 26, Loss: 0.8112\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8144364953041077\n",
            "  Batch 27, Loss: 0.8113\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8292112946510315\n",
            "  Batch 28, Loss: 0.8120\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8355697393417358\n",
            "  Batch 29, Loss: 0.8128\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8005017638206482\n",
            "  Batch 30, Loss: 0.8124\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7650713324546814\n",
            "  Batch 31, Loss: 0.8109\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8563253283500671\n",
            "  Batch 32, Loss: 0.8123\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7494856119155884\n",
            "  Batch 33, Loss: 0.8104\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.9039323329925537\n",
            "  Batch 34, Loss: 0.8131\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.9036787748336792\n",
            "  Batch 35, Loss: 0.8157\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7711966633796692\n",
            "  Batch 36, Loss: 0.8145\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8608921766281128\n",
            "  Batch 37, Loss: 0.8157\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8408370018005371\n",
            "  Batch 38, Loss: 0.8164\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7634807229042053\n",
            "  Batch 39, Loss: 0.8150\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7725178599357605\n",
            "  Batch 40, Loss: 0.8140\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8501061797142029\n",
            "  Batch 41, Loss: 0.8149\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7761594653129578\n",
            "  Batch 42, Loss: 0.8139\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8556363582611084\n",
            "  Batch 43, Loss: 0.8149\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8031226992607117\n",
            "  Batch 44, Loss: 0.8146\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.921677827835083\n",
            "  Batch 45, Loss: 0.8170\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8461307287216187\n",
            "  Batch 46, Loss: 0.8176\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7971248626708984\n",
            "  Batch 47, Loss: 0.8172\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8312970399856567\n",
            "  Batch 48, Loss: 0.8175\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7675219774246216\n",
            "  Batch 49, Loss: 0.8165\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7814690470695496\n",
            "  Batch 50, Loss: 0.8158\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7875537872314453\n",
            "  Batch 51, Loss: 0.8152\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7605177760124207\n",
            "  Batch 52, Loss: 0.8142\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8833816647529602\n",
            "  Batch 53, Loss: 0.8155\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8398315906524658\n",
            "  Batch 54, Loss: 0.8159\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7845568656921387\n",
            "  Batch 55, Loss: 0.8154\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8338329195976257\n",
            "  Batch 56, Loss: 0.8157\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8513333797454834\n",
            "  Batch 57, Loss: 0.8163\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7769965529441833\n",
            "  Batch 58, Loss: 0.8156\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8809403777122498\n",
            "  Batch 59, Loss: 0.8167\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7795584797859192\n",
            "  Batch 60, Loss: 0.8161\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8385883569717407\n",
            "  Batch 61, Loss: 0.8165\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8808020353317261\n",
            "  Batch 62, Loss: 0.8175\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7553091049194336\n",
            "  Batch 63, Loss: 0.8165\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8391711115837097\n",
            "  Batch 64, Loss: 0.8169\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7510337829589844\n",
            "  Batch 65, Loss: 0.8159\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7826570868492126\n",
            "  Batch 66, Loss: 0.8154\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7429081201553345\n",
            "  Batch 67, Loss: 0.8143\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7834094762802124\n",
            "  Batch 68, Loss: 0.8138\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.803656280040741\n",
            "  Batch 69, Loss: 0.8137\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8798209428787231\n",
            "  Batch 70, Loss: 0.8146\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7833319902420044\n",
            "  Batch 71, Loss: 0.8142\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8495838642120361\n",
            "  Batch 72, Loss: 0.8147\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7326273918151855\n",
            "  Batch 73, Loss: 0.8136\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7334543466567993\n",
            "  Batch 74, Loss: 0.8125\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7917426228523254\n",
            "  Batch 75, Loss: 0.8122\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7722570896148682\n",
            "  Batch 76, Loss: 0.8117\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8316702842712402\n",
            "  Batch 77, Loss: 0.8119\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7850306034088135\n",
            "  Batch 78, Loss: 0.8116\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7878811955451965\n",
            "  Batch 79, Loss: 0.8113\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7856054902076721\n",
            "  Batch 80, Loss: 0.8110\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7746550440788269\n",
            "  Batch 81, Loss: 0.8105\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.814194917678833\n",
            "  Batch 82, Loss: 0.8106\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8366405963897705\n",
            "  Batch 83, Loss: 0.8109\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8610277771949768\n",
            "  Batch 84, Loss: 0.8115\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8168797492980957\n",
            "  Batch 85, Loss: 0.8115\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.817665696144104\n",
            "  Batch 86, Loss: 0.8116\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8222389221191406\n",
            "  Batch 87, Loss: 0.8117\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7780517339706421\n",
            "  Batch 88, Loss: 0.8114\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7755441665649414\n",
            "  Batch 89, Loss: 0.8110\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7677000164985657\n",
            "  Batch 90, Loss: 0.8105\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8052582740783691\n",
            "  Batch 91, Loss: 0.8104\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7973761558532715\n",
            "  Batch 92, Loss: 0.8103\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7914280295372009\n",
            "  Batch 93, Loss: 0.8101\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.749281644821167\n",
            "  Batch 94, Loss: 0.8094\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7793253064155579\n",
            "  Batch 95, Loss: 0.8091\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7780923247337341\n",
            "  Batch 96, Loss: 0.8088\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7479531764984131\n",
            "  Batch 97, Loss: 0.8082\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.732258141040802\n",
            "  Batch 98, Loss: 0.8074\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7793712615966797\n",
            "  Batch 99, Loss: 0.8071\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7100329995155334\n",
            "  Batch 100, Loss: 0.8061\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7391413450241089\n",
            "  Batch 101, Loss: 0.8055\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7297237515449524\n",
            "  Batch 102, Loss: 0.8047\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7956774234771729\n",
            "  Batch 103, Loss: 0.8046\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8235845565795898\n",
            "  Batch 104, Loss: 0.8048\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8230981826782227\n",
            "  Batch 105, Loss: 0.8050\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7072393298149109\n",
            "  Batch 106, Loss: 0.8041\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7484557032585144\n",
            "  Batch 107, Loss: 0.8036\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.8207083344459534\n",
            "  Batch 108, Loss: 0.8037\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7468489408493042\n",
            "  Batch 109, Loss: 0.8032\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6629235148429871\n",
            "  Batch 110, Loss: 0.8019\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7585424780845642\n",
            "  Batch 111, Loss: 0.8015\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7529316544532776\n",
            "  Batch 112, Loss: 0.8011\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7142896056175232\n",
            "  Batch 113, Loss: 0.8003\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7868270874023438\n",
            "  Batch 114, Loss: 0.8002\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7249348163604736\n",
            "  Batch 115, Loss: 0.7995\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.709881603717804\n",
            "  Batch 116, Loss: 0.7988\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7786827087402344\n",
            "  Batch 117, Loss: 0.7986\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7691749334335327\n",
            "  Batch 118, Loss: 0.7984\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7498717904090881\n",
            "  Batch 119, Loss: 0.7979\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7845187187194824\n",
            "  Batch 120, Loss: 0.7978\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7450170516967773\n",
            "  Batch 121, Loss: 0.7974\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.736393928527832\n",
            "  Batch 122, Loss: 0.7969\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.747234582901001\n",
            "  Batch 123, Loss: 0.7965\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7149626612663269\n",
            "  Batch 124, Loss: 0.7958\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7044053077697754\n",
            "  Batch 125, Loss: 0.7951\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6985087394714355\n",
            "  Batch 126, Loss: 0.7943\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7621364593505859\n",
            "  Batch 127, Loss: 0.7941\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7685970664024353\n",
            "  Batch 128, Loss: 0.7939\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7447777390480042\n",
            "  Batch 129, Loss: 0.7935\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.720872163772583\n",
            "  Batch 130, Loss: 0.7929\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7142891883850098\n",
            "  Batch 131, Loss: 0.7923\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7124336957931519\n",
            "  Batch 132, Loss: 0.7917\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7655258178710938\n",
            "  Batch 133, Loss: 0.7915\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7099677324295044\n",
            "  Batch 134, Loss: 0.7909\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6836336851119995\n",
            "  Batch 135, Loss: 0.7901\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6211721301078796\n",
            "  Batch 136, Loss: 0.7889\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7529745697975159\n",
            "  Batch 137, Loss: 0.7886\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6176252365112305\n",
            "  Batch 138, Loss: 0.7874\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7082937359809875\n",
            "  Batch 139, Loss: 0.7868\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7310298681259155\n",
            "  Batch 140, Loss: 0.7864\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6464763283729553\n",
            "  Batch 141, Loss: 0.7854\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.731322169303894\n",
            "  Batch 142, Loss: 0.7851\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6560447812080383\n",
            "  Batch 143, Loss: 0.7842\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6551142334938049\n",
            "  Batch 144, Loss: 0.7833\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.674952507019043\n",
            "  Batch 145, Loss: 0.7825\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6725611090660095\n",
            "  Batch 146, Loss: 0.7818\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6557932496070862\n",
            "  Batch 147, Loss: 0.7809\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6374021172523499\n",
            "  Batch 148, Loss: 0.7799\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6702015399932861\n",
            "  Batch 149, Loss: 0.7792\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.7233404517173767\n",
            "  Batch 150, Loss: 0.7788\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6755924820899963\n",
            "  Batch 151, Loss: 0.7781\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6416592001914978\n",
            "  Batch 152, Loss: 0.7772\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6527377367019653\n",
            "  Batch 153, Loss: 0.7764\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.709768533706665\n",
            "  Batch 154, Loss: 0.7760\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6917836666107178\n",
            "  Batch 155, Loss: 0.7754\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6875483989715576\n",
            "  Batch 156, Loss: 0.7749\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6633214354515076\n",
            "  Batch 157, Loss: 0.7742\n",
            "length of train_dataloader:  157\n",
            "Epoch 3 Loss: 0.7742\n",
            "Validation Loss after Epoch 3: 6.3891\n",
            "Epoch 4/4\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5456538200378418\n",
            "  Batch 1, Loss: 0.5457\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.49106982350349426\n",
            "  Batch 2, Loss: 0.5184\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5239270925521851\n",
            "  Batch 3, Loss: 0.5202\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5028920769691467\n",
            "  Batch 4, Loss: 0.5159\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5313774943351746\n",
            "  Batch 5, Loss: 0.5190\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5833057761192322\n",
            "  Batch 6, Loss: 0.5297\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5202323794364929\n",
            "  Batch 7, Loss: 0.5284\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.532757043838501\n",
            "  Batch 8, Loss: 0.5289\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5317696928977966\n",
            "  Batch 9, Loss: 0.5292\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5739251375198364\n",
            "  Batch 10, Loss: 0.5337\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5700145959854126\n",
            "  Batch 11, Loss: 0.5370\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5972703695297241\n",
            "  Batch 12, Loss: 0.5420\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5280898809432983\n",
            "  Batch 13, Loss: 0.5409\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5125056505203247\n",
            "  Batch 14, Loss: 0.5389\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5387411713600159\n",
            "  Batch 15, Loss: 0.5389\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5360327363014221\n",
            "  Batch 16, Loss: 0.5387\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5413005352020264\n",
            "  Batch 17, Loss: 0.5389\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5310022234916687\n",
            "  Batch 18, Loss: 0.5384\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5434989333152771\n",
            "  Batch 19, Loss: 0.5387\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5392792224884033\n",
            "  Batch 20, Loss: 0.5387\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5352699756622314\n",
            "  Batch 21, Loss: 0.5386\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5864260196685791\n",
            "  Batch 22, Loss: 0.5407\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.4917667508125305\n",
            "  Batch 23, Loss: 0.5386\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5139307379722595\n",
            "  Batch 24, Loss: 0.5376\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5271698236465454\n",
            "  Batch 25, Loss: 0.5372\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5539412498474121\n",
            "  Batch 26, Loss: 0.5378\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5432133078575134\n",
            "  Batch 27, Loss: 0.5380\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5415229201316833\n",
            "  Batch 28, Loss: 0.5381\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5314274430274963\n",
            "  Batch 29, Loss: 0.5379\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.4863888919353485\n",
            "  Batch 30, Loss: 0.5362\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5317020416259766\n",
            "  Batch 31, Loss: 0.5360\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.571829080581665\n",
            "  Batch 32, Loss: 0.5372\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.4832293689250946\n",
            "  Batch 33, Loss: 0.5355\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.530343234539032\n",
            "  Batch 34, Loss: 0.5354\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5183809995651245\n",
            "  Batch 35, Loss: 0.5349\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5447518825531006\n",
            "  Batch 36, Loss: 0.5352\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5781571865081787\n",
            "  Batch 37, Loss: 0.5363\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5223904252052307\n",
            "  Batch 38, Loss: 0.5360\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5123269557952881\n",
            "  Batch 39, Loss: 0.5354\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5402852296829224\n",
            "  Batch 40, Loss: 0.5355\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.47171053290367126\n",
            "  Batch 41, Loss: 0.5339\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5288164019584656\n",
            "  Batch 42, Loss: 0.5338\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.591074526309967\n",
            "  Batch 43, Loss: 0.5351\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5437710881233215\n",
            "  Batch 44, Loss: 0.5353\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5206805467605591\n",
            "  Batch 45, Loss: 0.5350\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.512907862663269\n",
            "  Batch 46, Loss: 0.5345\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5884589552879333\n",
            "  Batch 47, Loss: 0.5357\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5780454277992249\n",
            "  Batch 48, Loss: 0.5366\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5021939277648926\n",
            "  Batch 49, Loss: 0.5359\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5589878559112549\n",
            "  Batch 50, Loss: 0.5363\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5949761271476746\n",
            "  Batch 51, Loss: 0.5375\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5110738277435303\n",
            "  Batch 52, Loss: 0.5370\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5572331547737122\n",
            "  Batch 53, Loss: 0.5373\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.49724096059799194\n",
            "  Batch 54, Loss: 0.5366\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5713522434234619\n",
            "  Batch 55, Loss: 0.5372\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5353965759277344\n",
            "  Batch 56, Loss: 0.5372\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5543148517608643\n",
            "  Batch 57, Loss: 0.5375\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.49127164483070374\n",
            "  Batch 58, Loss: 0.5367\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5559558868408203\n",
            "  Batch 59, Loss: 0.5370\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.529381275177002\n",
            "  Batch 60, Loss: 0.5369\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5809823870658875\n",
            "  Batch 61, Loss: 0.5376\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.500673770904541\n",
            "  Batch 62, Loss: 0.5370\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5494643449783325\n",
            "  Batch 63, Loss: 0.5372\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5084075331687927\n",
            "  Batch 64, Loss: 0.5368\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5828396081924438\n",
            "  Batch 65, Loss: 0.5375\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5568748712539673\n",
            "  Batch 66, Loss: 0.5378\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5324813723564148\n",
            "  Batch 67, Loss: 0.5377\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5351880192756653\n",
            "  Batch 68, Loss: 0.5377\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5498393774032593\n",
            "  Batch 69, Loss: 0.5378\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5683605074882507\n",
            "  Batch 70, Loss: 0.5383\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5131494402885437\n",
            "  Batch 71, Loss: 0.5379\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.557985246181488\n",
            "  Batch 72, Loss: 0.5382\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5358522534370422\n",
            "  Batch 73, Loss: 0.5382\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5154043436050415\n",
            "  Batch 74, Loss: 0.5379\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.4932533800601959\n",
            "  Batch 75, Loss: 0.5373\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5167624354362488\n",
            "  Batch 76, Loss: 0.5370\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5648550987243652\n",
            "  Batch 77, Loss: 0.5374\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5510732531547546\n",
            "  Batch 78, Loss: 0.5375\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.535527229309082\n",
            "  Batch 79, Loss: 0.5375\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.48517030477523804\n",
            "  Batch 80, Loss: 0.5369\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5695986151695251\n",
            "  Batch 81, Loss: 0.5373\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5074215531349182\n",
            "  Batch 82, Loss: 0.5369\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5404374599456787\n",
            "  Batch 83, Loss: 0.5369\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5029051303863525\n",
            "  Batch 84, Loss: 0.5365\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5514411330223083\n",
            "  Batch 85, Loss: 0.5367\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5668478012084961\n",
            "  Batch 86, Loss: 0.5371\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5268669724464417\n",
            "  Batch 87, Loss: 0.5369\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5230649709701538\n",
            "  Batch 88, Loss: 0.5368\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5058809518814087\n",
            "  Batch 89, Loss: 0.5364\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5345876812934875\n",
            "  Batch 90, Loss: 0.5364\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.47775864601135254\n",
            "  Batch 91, Loss: 0.5358\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5243959426879883\n",
            "  Batch 92, Loss: 0.5356\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5154522657394409\n",
            "  Batch 93, Loss: 0.5354\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5521359443664551\n",
            "  Batch 94, Loss: 0.5356\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5269960761070251\n",
            "  Batch 95, Loss: 0.5355\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5337074398994446\n",
            "  Batch 96, Loss: 0.5355\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.4885123670101166\n",
            "  Batch 97, Loss: 0.5350\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5386477708816528\n",
            "  Batch 98, Loss: 0.5350\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.552498459815979\n",
            "  Batch 99, Loss: 0.5352\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.530262291431427\n",
            "  Batch 100, Loss: 0.5352\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5206382274627686\n",
            "  Batch 101, Loss: 0.5350\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5517678260803223\n",
            "  Batch 102, Loss: 0.5352\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5263672471046448\n",
            "  Batch 103, Loss: 0.5351\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5143659114837646\n",
            "  Batch 104, Loss: 0.5349\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.525133490562439\n",
            "  Batch 105, Loss: 0.5348\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5153545141220093\n",
            "  Batch 106, Loss: 0.5346\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5007532238960266\n",
            "  Batch 107, Loss: 0.5343\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5926820039749146\n",
            "  Batch 108, Loss: 0.5349\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5548682808876038\n",
            "  Batch 109, Loss: 0.5350\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5195722579956055\n",
            "  Batch 110, Loss: 0.5349\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5141330361366272\n",
            "  Batch 111, Loss: 0.5347\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5051005482673645\n",
            "  Batch 112, Loss: 0.5344\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5024042129516602\n",
            "  Batch 113, Loss: 0.5342\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5174010992050171\n",
            "  Batch 114, Loss: 0.5340\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5386146903038025\n",
            "  Batch 115, Loss: 0.5341\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5290172696113586\n",
            "  Batch 116, Loss: 0.5340\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5818399786949158\n",
            "  Batch 117, Loss: 0.5344\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5091580748558044\n",
            "  Batch 118, Loss: 0.5342\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.4926697015762329\n",
            "  Batch 119, Loss: 0.5339\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5340954065322876\n",
            "  Batch 120, Loss: 0.5339\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5095370411872864\n",
            "  Batch 121, Loss: 0.5337\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.48073485493659973\n",
            "  Batch 122, Loss: 0.5332\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.4483622610569\n",
            "  Batch 123, Loss: 0.5325\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5101799368858337\n",
            "  Batch 124, Loss: 0.5324\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5323356986045837\n",
            "  Batch 125, Loss: 0.5324\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.4794769287109375\n",
            "  Batch 126, Loss: 0.5319\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5105370283126831\n",
            "  Batch 127, Loss: 0.5318\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5211859941482544\n",
            "  Batch 128, Loss: 0.5317\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.456235408782959\n",
            "  Batch 129, Loss: 0.5311\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5187085270881653\n",
            "  Batch 130, Loss: 0.5310\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.4633927345275879\n",
            "  Batch 131, Loss: 0.5305\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.4981076121330261\n",
            "  Batch 132, Loss: 0.5302\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5925171971321106\n",
            "  Batch 133, Loss: 0.5307\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.49647223949432373\n",
            "  Batch 134, Loss: 0.5305\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5195233821868896\n",
            "  Batch 135, Loss: 0.5304\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5221370458602905\n",
            "  Batch 136, Loss: 0.5303\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.48912033438682556\n",
            "  Batch 137, Loss: 0.5300\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5251027345657349\n",
            "  Batch 138, Loss: 0.5300\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5259218811988831\n",
            "  Batch 139, Loss: 0.5299\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5553316473960876\n",
            "  Batch 140, Loss: 0.5301\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5568031668663025\n",
            "  Batch 141, Loss: 0.5303\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.47349581122398376\n",
            "  Batch 142, Loss: 0.5299\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5207674503326416\n",
            "  Batch 143, Loss: 0.5299\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.46911874413490295\n",
            "  Batch 144, Loss: 0.5294\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.4885978400707245\n",
            "  Batch 145, Loss: 0.5292\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.48760986328125\n",
            "  Batch 146, Loss: 0.5289\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5009971857070923\n",
            "  Batch 147, Loss: 0.5287\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.517220139503479\n",
            "  Batch 148, Loss: 0.5286\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5334610342979431\n",
            "  Batch 149, Loss: 0.5286\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.48192358016967773\n",
            "  Batch 150, Loss: 0.5283\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.50554358959198\n",
            "  Batch 151, Loss: 0.5282\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5741910338401794\n",
            "  Batch 152, Loss: 0.5285\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5221204161643982\n",
            "  Batch 153, Loss: 0.5284\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.4996322989463806\n",
            "  Batch 154, Loss: 0.5282\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5388725399971008\n",
            "  Batch 155, Loss: 0.5283\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.5572187304496765\n",
            "  Batch 156, Loss: 0.5285\n",
            "here\n",
            "after forward pass\n",
            "after backward pass\n",
            "after step update\n",
            "loss:  0.6156337857246399\n",
            "  Batch 157, Loss: 0.5291\n",
            "length of train_dataloader:  157\n",
            "Epoch 4 Loss: 0.5291\n",
            "Validation Loss after Epoch 4: 6.9818\n",
            "model finished training\n",
            "model saved\n",
            "lossed saved\n"
          ]
        }
      ],
      "id": "Of0O2uCJnlRr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20K Model"
      ],
      "metadata": {
        "id": "Uy79HcoBnlRr"
      },
      "id": "Uy79HcoBnlRr"
    },
    {
      "cell_type": "code",
      "source": [
        "# 20k is 50k, 50k is 20k\n",
        "rw_data_path_20k = '/content/drive/MyDrive/W266 Final Project/gt_reworded_data/reword_50k_dataset.csv'\n",
        "rw_trained_model_20k, rw_losses_20k, rw_train_epoch_losses_20k, rw_val_epoch_losses_20k = train_pipeline(rw_data_path_20k, train_model, tokenizer, 'rw')"
      ],
      "metadata": {
        "id": "ExFNrMNwnlRr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ExFNrMNwnlRr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuBk-B4tnlRr"
      },
      "source": [
        "## 50K Model"
      ],
      "id": "YuBk-B4tnlRr"
    },
    {
      "cell_type": "code",
      "source": [
        "rw_data_path_50k = '/content/drive/MyDrive/W266 Final Project/gt_reworded_data/reword_20k_dataset.csv'\n",
        "rw_trained_model_50k, rw_losses_50k, rw_train_epoch_losses_50k, rw_val_epoch_losses_50k = train_pipeline(rw_data_path_50k, train_model, tokenizer, 'rw')"
      ],
      "metadata": {
        "id": "eNB9_8T6nlRr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "eNB9_8T6nlRr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 100k model"
      ],
      "metadata": {
        "id": "iuQwrNT1nlRr"
      },
      "id": "iuQwrNT1nlRr"
    },
    {
      "cell_type": "code",
      "source": [
        "rw_data_path_100k = '/content/drive/MyDrive/W266 Final Project/gt_reworded_data/reword_100k_dataset.csv'\n",
        "rw_trained_model_100k, rw_losses_100k, rw_train_epoch_losses_100k, rw_val_epoch_losses_100k = train_pipeline(rw_data_path_100k, train_model, tokenizer, 'rw')"
      ],
      "metadata": {
        "id": "4ei2vHIgnlRr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "4ei2vHIgnlRr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# mBART Models"
      ],
      "metadata": {
        "id": "lCTMquHGLDay"
      },
      "id": "lCTMquHGLDay"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10K Model"
      ],
      "metadata": {
        "id": "8HL96NkzLJzV"
      },
      "id": "8HL96NkzLJzV"
    },
    {
      "cell_type": "code",
      "source": [
        "mbart_data_path_10k = '/content/drive/MyDrive/W266 Final Project/mBART_translated_data/mBART_10k_dataset.csv'\n",
        "mbart_trained_model_10k, mbart_losses_10k = train_pipeline(mbart_data_path_10k, train_model, tokenizer, 'mbart')"
      ],
      "metadata": {
        "id": "EI3gHsOILJzV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "EI3gHsOILJzV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20K Model"
      ],
      "metadata": {
        "id": "EnUPi1eALJzV"
      },
      "id": "EnUPi1eALJzV"
    },
    {
      "cell_type": "code",
      "source": [
        "mbart_data_path_20k = '/content/drive/MyDrive/W266 Final Project/mBART_translated_data/mBART_20k_dataset.csv'\n",
        "mbart_trained_model_20k, mbart_losses_20k = train_pipeline(mbart_data_path_20k, train_model, tokenizer, 'mbart')"
      ],
      "metadata": {
        "id": "viQIFwgqLJzV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "viQIFwgqLJzV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 50K Model"
      ],
      "metadata": {
        "id": "hvJts89bLJzV"
      },
      "id": "hvJts89bLJzV"
    },
    {
      "cell_type": "code",
      "source": [
        "mbart_data_path_50k = '/content/drive/MyDrive/W266 Final Project/mBART_translated_data/mBART_50k_dataset.csv'\n",
        "mbart_trained_model_50k, mbart_losses_50k = train_pipeline(mbart_data_path_50k, train_model, tokenizer, 'mbart')"
      ],
      "metadata": {
        "id": "NjsSzKw0LJzW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "NjsSzKw0LJzW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 100K Model"
      ],
      "metadata": {
        "id": "TD-aV_IlLJzW"
      },
      "id": "TD-aV_IlLJzW"
    },
    {
      "cell_type": "code",
      "source": [
        "mbart_data_path_100k = '/content/drive/MyDrive/W266 Final Project/mBART_translated_data/mBART_100k_dataset.csv'\n",
        "mbart_trained_model_100k, mbart_losses_100k = train_pipeline(mbart_data_path_100k, train_model, tokenizer, 'mbart')"
      ],
      "metadata": {
        "id": "DYpnNONrLJzW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "DYpnNONrLJzW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 200K Model"
      ],
      "metadata": {
        "id": "HA96vBq0LJzW"
      },
      "id": "HA96vBq0LJzW"
    },
    {
      "cell_type": "code",
      "source": [
        "mbart_data_path_200k = '/content/drive/MyDrive/W266 Final Project/mBART_translated_data/mBART_200k_dataset.csv'\n",
        "# mbart_trained_model_200k, mbart_losses_200k = train_pipeline(mbart_data_path_200k, train_model, tokenizer, 'mbart')"
      ],
      "metadata": {
        "id": "Ehiz0bk9LJzW"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Ehiz0bk9LJzW"
    },
    {
      "cell_type": "code",
      "source": [
        "# 2nd time training with train/val epoch loss\n",
        "mbart_trained_model_200k, batch_losses_200k, train_epoch_losses_200k, val_epoch_losses_200k = train_pipeline(mbart_data_path_200k, train_model, tokenizer, 'mbart_train_val_epoch')"
      ],
      "metadata": {
        "id": "Ev53hRqxPcru"
      },
      "id": "Ev53hRqxPcru",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2nd 10K dataset"
      ],
      "metadata": {
        "id": "YAsYzDv0cNqc"
      },
      "id": "YAsYzDv0cNqc"
    },
    {
      "cell_type": "code",
      "source": [
        "# train model using last 5K rows only to create 2nd half 10K dataset\n",
        "# 10K dataset uses first 5K rows to create 10K dataset\n",
        "mbart_data_path_last_10k = '/content/drive/MyDrive/W266 Final Project/mBART_translated_data/mBART_last10k_dataset.csv'\n",
        "mbart_trained_model_last_10k, mbart_losses_last_10k = train_pipeline(mbart_data_path_last_10k, train_model, tokenizer, 'mbart_last_10k')\n"
      ],
      "metadata": {
        "id": "nI0tZdLrtrUu"
      },
      "id": "nI0tZdLrtrUu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2nd 100K Dataset"
      ],
      "metadata": {
        "id": "l4IgsgmgcIz5"
      },
      "id": "l4IgsgmgcIz5"
    },
    {
      "cell_type": "code",
      "source": [
        "# then train model using last 50K rows to create the 2nd half 100K dataset\n",
        "# 100K model uses first 50K rows\n",
        "mbart_data_path_last_100k = '/content/drive/MyDrive/W266 Final Project/mBART_translated_data/mBART_last100k_dataset.csv'\n",
        "mbart_trained_model_last_100k, mbart_losses_last_100k = train_pipeline(mbart_data_path_last_100k, train_model, tokenizer, 'mbart_last_100k')"
      ],
      "metadata": {
        "id": "wGMElfzjcJKS"
      },
      "id": "wGMElfzjcJKS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uo172R3vdxds"
      },
      "id": "Uo172R3vdxds",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualizations"
      ],
      "metadata": {
        "id": "vV3_DXkPqVm4"
      },
      "id": "vV3_DXkPqVm4"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "mbart_losses = pd.read_csv('/content/drive/MyDrive/W266 Final Project/mbart_train_val_epoch_200k_train_val_losses.csv')\n",
        "mbart_losses.columns = ['index', 'training_loss', 'validation_loss']"
      ],
      "metadata": {
        "id": "Zb4puTvH-3EN"
      },
      "id": "Zb4puTvH-3EN",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot train/val loss curves for Google Translate and mBART models\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# training and validation loss values\n",
        "train_loss = mbart_losses['training_loss'].values\n",
        "val_loss = mbart_losses['validation_loss'].values\n",
        "\n",
        "# create a figure and axis\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# set x and y axis labels and title\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Training and Validation Loss')\n",
        "\n",
        "# plot training and validation loss curves\n",
        "ax.plot(range(1, len(train_loss)+1), train_loss, label='Training Loss')\n",
        "ax.plot(range(1, len(val_loss)+1), val_loss, label='Validation Loss')\n",
        "\n",
        "# add a legend to the plot\n",
        "ax.legend()\n",
        "\n",
        "# display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6XpM9HuRqXUV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "ba646499-8160-4e15-dd87-5c15f04bb733"
      },
      "id": "6XpM9HuRqXUV",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "google_translate_losses = pd.read_csv('/content/drive/MyDrive/W266 Final Project/google_translate_train_val_epoch_200k_train_val_losses.csv')\n",
        "google_translate_losses.columns = ['index', 'training_loss', 'validation_loss']\n",
        "\n",
        "# training and validation loss values\n",
        "train_loss = google_translate_losses['training_loss'].values\n",
        "val_loss = google_translate_losses['validation_loss'].values\n",
        "\n",
        "# create a figure and axis\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "# set x and y axis labels and title\n",
        "ax.set_xlabel('Epochs')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Training and Validation Loss')\n",
        "\n",
        "# plot training and validation loss curves\n",
        "ax.plot(range(1, len(train_loss)+1), train_loss, label='Training Loss')\n",
        "ax.plot(range(1, len(val_loss)+1), val_loss, label='Validation Loss')\n",
        "\n",
        "# add a legend to the plot\n",
        "ax.legend()\n",
        "\n",
        "# display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xidOZcqZqXO-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "538f96d9-815c-439f-eda3-ebb932689324"
      },
      "id": "xidOZcqZqXO-",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2-gsSMniqWrQ"
      },
      "id": "2-gsSMniqWrQ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0LjnvJI2zzhi",
        "3pefbjInKMyu",
        "fEVRe2Nom2eF",
        "QkPfs2CFHGxx",
        "dUU3SDCFIrJD",
        "q_WsYvRLI4-U",
        "aTHoNhQkI6kS",
        "8-EqyYQQm6-O",
        "3I9Y_ek9nlRq"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}